{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 — Evaluate Product Categorization Accuracy\n",
    "\n",
    "**Objective:** Evaluate LLM product classification results against the Product Catalog ground truth,\n",
    "compute accuracy metrics, analyze failures, and support cross-version comparison.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| Per-Level Accuracy | L1 through L5 independently (case-insensitive, null-safe) |\n",
    "| Exact Match | All 5 levels correct (null-to-null counts as match) |\n",
    "| Partial Match | L1+L2 correct, L1+L2+L3 correct |\n",
    "| Account-Weighted | Weighted by raw account count |\n",
    "| Per-Layer | Obvious (Layer 1) vs Ambiguous (Layer 2) vs Unknown (Layer 3) |\n",
    "| Failure Analysis | Categorize mismatches by root cause |\n",
    "| Cost Summary | Total tokens and estimated cost for the run |\n",
    "| Cross-Version | Compare accuracy across prompt versions |\n",
    "\n",
    "### Layer Evaluation Rules\n",
    "\n",
    "- **Layer 1 (Obvious):** Standard per-level comparison against single GT mapping.\n",
    "- **Layer 2 (Ambiguous):** Match against ANY valid GT mapping for that product_code.\n",
    "- **Layer 3 (Unknown):** Flagged as NEEDS MANUAL REVIEW — no accuracy calculation.\n",
    "\n",
    "**Runs on:** Databricks Runtime 15.4 LTS or above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Configuration ─────────────────────────────────────────────────\n",
    "CATALOG_NAME    = \"ciq-bp_dummy-dev\"\n",
    "SCHEMA_NAME     = \"default\"\n",
    "PROMPT_VERSION  = \"v1.0\"  # Set to None to evaluate the latest run\n",
    "\n",
    "RESULTS_TABLE = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.product_classification_results\"\n",
    "GT_TABLE      = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.product_ground_truth_normalized\"\n",
    "EVAL_TABLE    = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.product_evaluation_results\"\n",
    "\n",
    "TARGET_ACCURACY = 0.80\n",
    "\n",
    "print(f\"Results table:  {RESULTS_TABLE}\")\n",
    "print(f\"GT table:       {GT_TABLE}\")\n",
    "print(f\"Eval table:     {EVAL_TABLE}\")\n",
    "print(f\"Prompt version: {PROMPT_VERSION or '(latest)'}\")\n",
    "print(f\"Target:         >= {TARGET_ACCURACY:.0%} account-weighted exact match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1 — Validate upstream tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    for table, label in [\n",
    "        (RESULTS_TABLE, \"product_classification_results\"),\n",
    "        (GT_TABLE, \"product_ground_truth_normalized\"),\n",
    "    ]:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {table}\").collect()[0][\"cnt\"]\n",
    "        print(f\"  OK  {label}: {count} rows\")\n",
    "    print(\"\\nUpstream tables validated.\")\n",
    "except NameError:\n",
    "    raise SystemExit(\"Spark session not found — run this notebook in Databricks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2 — Load results and ground truth from Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROMPT_VERSION:\n",
    "    df_results = (\n",
    "        spark.sql(f\"SELECT * FROM {RESULTS_TABLE} WHERE prompt_version = '{PROMPT_VERSION}'\")\n",
    "        .toPandas()\n",
    "    )\n",
    "    print(f\"Loaded results for prompt_version='{PROMPT_VERSION}': {len(df_results)} rows\")\n",
    "else:\n",
    "    latest_version = (\n",
    "        spark.sql(f\"SELECT prompt_version FROM {RESULTS_TABLE} ORDER BY run_timestamp DESC LIMIT 1\")\n",
    "        .collect()[0][\"prompt_version\"]\n",
    "    )\n",
    "    df_results = (\n",
    "        spark.sql(f\"SELECT * FROM {RESULTS_TABLE} WHERE prompt_version = '{latest_version}'\")\n",
    "        .toPandas()\n",
    "    )\n",
    "    print(f\"Loaded latest results (prompt_version='{latest_version}'): {len(df_results)} rows\")\n",
    "\n",
    "df_results[\"product_code\"] = df_results[\"product_code\"].astype(str).str.strip()\n",
    "\n",
    "df_gt = spark.table(GT_TABLE).toPandas()\n",
    "df_gt[\"product_code\"] = df_gt[\"product_code\"].astype(str).str.strip()\n",
    "\n",
    "print(f\"Ground truth: {len(df_gt)} rows, {df_gt['product_code'].nunique()} unique codes\")\n",
    "print(f\"\\nResults columns: {list(df_results.columns)}\")\n",
    "print(f\"GT columns: {list(df_gt.columns)}\")\n",
    "print(f\"\\nLayer distribution in results:\")\n",
    "for layer in sorted(df_results[\"layer\"].unique()):\n",
    "    n = len(df_results[df_results[\"layer\"] == layer])\n",
    "    print(f\"  Layer {layer}: {n} codes\")\n",
    "print(f\"\\nSource file distribution in results:\")\n",
    "print(df_results[\"source_file\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3 — Comparison helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_NULL = \"__null__\"\n",
    "\n",
    "\n",
    "def _canon(val):\n",
    "    \"\"\"Canonicalize a value for comparison: lowercase, strip, null-safe.\"\"\"\n",
    "    if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "        return _NULL\n",
    "    s = str(val).strip().lower()\n",
    "    if s in (\"\", \"none\", \"nan\", \"null\", \"n/a\"):\n",
    "        return _NULL\n",
    "    return s\n",
    "\n",
    "\n",
    "def levels_match(row, llm_col, gt_col):\n",
    "    return _canon(row[llm_col]) == _canon(row[gt_col])\n",
    "\n",
    "\n",
    "def add_match_columns(df):\n",
    "    \"\"\"Add per-level and aggregate match booleans for 5-level product taxonomy.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"match_L1\"] = df.apply(levels_match, axis=1, llm_col=\"line_of_business\", gt_col=\"gt_L1_line_of_business\")\n",
    "    df[\"match_L2\"] = df.apply(levels_match, axis=1, llm_col=\"product_type\", gt_col=\"gt_L2_type\")\n",
    "    df[\"match_L3\"] = df.apply(levels_match, axis=1, llm_col=\"product_category\", gt_col=\"gt_L3_category\")\n",
    "    df[\"match_L4\"] = df.apply(levels_match, axis=1, llm_col=\"product_subcategory\", gt_col=\"gt_L4_subcategory\")\n",
    "    df[\"match_L5\"] = df.apply(levels_match, axis=1, llm_col=\"product_special\", gt_col=\"gt_L5_special\")\n",
    "\n",
    "    df[\"exact_match\"]          = df[[\"match_L1\", \"match_L2\", \"match_L3\", \"match_L4\", \"match_L5\"]].all(axis=1)\n",
    "    df[\"partial_match_L1L2\"]   = df[[\"match_L1\", \"match_L2\"]].all(axis=1)\n",
    "    df[\"partial_match_L1L2L3\"] = df[[\"match_L1\", \"match_L2\", \"match_L3\"]].all(axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "LAYER_NAMES = {1: \"Obvious\", 2: \"Ambiguous\", 3: \"Unknown\"}\n",
    "\n",
    "print(\"Comparison helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4 — Prepare GT lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_level_cols = [\n",
    "    \"gt_L1_line_of_business\", \"gt_L2_type\", \"gt_L3_category\",\n",
    "    \"gt_L4_subcategory\", \"gt_L5_special\",\n",
    "]\n",
    "\n",
    "gt_mapping_counts = (\n",
    "    df_gt\n",
    "    .groupby(\"product_code\")\n",
    "    .apply(lambda g: g[gt_level_cols].drop_duplicates().shape[0])\n",
    "    .reset_index(name=\"n_mappings\")\n",
    ")\n",
    "\n",
    "multi_codes  = set(gt_mapping_counts.loc[gt_mapping_counts[\"n_mappings\"] > 1, \"product_code\"])\n",
    "single_codes = set(gt_mapping_counts.loc[gt_mapping_counts[\"n_mappings\"] == 1, \"product_code\"])\n",
    "all_gt_codes = set(df_gt[\"product_code\"].unique())\n",
    "\n",
    "gt_all_cols = [\"product_code\", \"product_name\"] + gt_level_cols + [\"product_domain\"]\n",
    "df_gt_single = (\n",
    "    df_gt[df_gt[\"product_code\"].isin(single_codes)]\n",
    "    .drop_duplicates(subset=\"product_code\", keep=\"first\")[gt_all_cols]\n",
    ")\n",
    "\n",
    "print(f\"GT codes with 1 mapping (Layer 1):  {len(single_codes)}\")\n",
    "print(f\"GT codes with 2+ mappings (Layer 2): {len(multi_codes)}\")\n",
    "if multi_codes:\n",
    "    print(f\"Multi-mapping codes: {sorted(multi_codes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5 — Layer 1 evaluation (Obvious codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l1 = pd.merge(\n",
    "    df_results[df_results[\"layer\"] == 1],\n",
    "    df_gt_single,\n",
    "    on=\"product_code\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_gt\"),\n",
    ")\n",
    "\n",
    "df_l1 = add_match_columns(df_l1)\n",
    "n_l1 = len(df_l1)\n",
    "accts_l1 = df_l1[\"account_count\"].sum()\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(f\"LAYER 1 — OBVIOUS CODES  (n = {n_l1}, accounts = {accts_l1:,})\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"  L1 (Line of Business):    {df_l1['match_L1'].mean():.1%}\")\n",
    "print(f\"  L2 (Product Type):        {df_l1['match_L2'].mean():.1%}\")\n",
    "print(f\"  L3 (Category):            {df_l1['match_L3'].mean():.1%}\")\n",
    "print(f\"  L4 (Sub-category):        {df_l1['match_L4'].mean():.1%}\")\n",
    "print(f\"  L5 (Special):             {df_l1['match_L5'].mean():.1%}\")\n",
    "print(f\"  ─────────────────────────────\")\n",
    "print(f\"  Partial (L1+L2):          {df_l1['partial_match_L1L2'].mean():.1%}\")\n",
    "print(f\"  Partial (L1+L2+L3):       {df_l1['partial_match_L1L2L3'].mean():.1%}\")\n",
    "print(f\"  Exact Match (all 5):      {df_l1['exact_match'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 65)\n",
    "print(f\"ACCOUNT-WEIGHTED ACCURACY  (total: {accts_l1:,} accounts)\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "for col, label in [\n",
    "    (\"match_L1\",              \"L1 (Line of Business)\"),\n",
    "    (\"match_L2\",              \"L2 (Product Type)\"),\n",
    "    (\"match_L3\",              \"L3 (Category)\"),\n",
    "    (\"match_L4\",              \"L4 (Sub-category)\"),\n",
    "    (\"match_L5\",              \"L5 (Special)\"),\n",
    "    (\"partial_match_L1L2\",    \"Partial (L1+L2)\"),\n",
    "    (\"partial_match_L1L2L3\",  \"Partial (L1+L2+L3)\"),\n",
    "    (\"exact_match\",           \"Exact (all 5)\"),\n",
    "]:\n",
    "    w = (df_l1[col] * df_l1[\"account_count\"]).sum() / max(accts_l1, 1)\n",
    "    print(f\"  {label:<25} {w:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6 — Layer 2 evaluation (Ambiguous codes)\n",
    "\n",
    "A prediction is correct if it matches **any** of the valid GT mappings for that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l2_src = df_results[df_results[\"layer\"] == 2].copy()\n",
    "\n",
    "llm_cols = [\"line_of_business\", \"product_type\", \"product_category\", \"product_subcategory\", \"product_special\"]\n",
    "\n",
    "l2_rows = []\n",
    "for _, row in df_l2_src.iterrows():\n",
    "    code = row[\"product_code\"]\n",
    "    gt_maps = (\n",
    "        df_gt[df_gt[\"product_code\"] == code][gt_level_cols]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    llm = tuple(_canon(row[c]) for c in llm_cols)\n",
    "\n",
    "    best_levels = 0\n",
    "    matched_any = False\n",
    "    for _, g in gt_maps.iterrows():\n",
    "        gt = tuple(_canon(g[c]) for c in gt_level_cols)\n",
    "        n_match = sum(a == b for a, b in zip(llm, gt))\n",
    "        best_levels = max(best_levels, n_match)\n",
    "        if llm == gt:\n",
    "            matched_any = True\n",
    "\n",
    "    l2_rows.append({\n",
    "        \"product_code\":        code,\n",
    "        \"product_name\":        row[\"product_name\"],\n",
    "        \"account_count\":       row[\"account_count\"],\n",
    "        \"llm_path\":            f\"{row['line_of_business']} > {row['product_type']} > {row['product_category']}\",\n",
    "        \"exact_match_any\":     matched_any,\n",
    "        \"best_levels_matched\": best_levels,\n",
    "        \"n_valid_mappings\":    len(gt_maps),\n",
    "        \"confidence\":          row[\"confidence\"],\n",
    "    })\n",
    "\n",
    "df_l2 = pd.DataFrame(l2_rows) if l2_rows else pd.DataFrame()\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(f\"LAYER 2 — AMBIGUOUS CODES  (n = {len(df_l2)})\")\n",
    "print(\"=\" * 65)\n",
    "if len(df_l2) > 0:\n",
    "    print(f\"  Exact match (any valid mapping):  {df_l2['exact_match_any'].mean():.1%}\")\n",
    "    print(f\"  Avg best levels matched:          {df_l2['best_levels_matched'].mean():.1f} / 5\")\n",
    "    print()\n",
    "    print(df_l2.to_string(index=False))\n",
    "else:\n",
    "    print(\"  No ambiguous codes in results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7 — Layer 3 (Unknown codes — manual review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l3 = df_results[df_results[\"layer\"] == 3].copy()\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(f\"LAYER 3 — UNKNOWN CODES  (n = {len(df_l3)}, no ground truth)\")\n",
    "print(\"=\" * 65)\n",
    "print(\"These codes are absent from the Product Catalog GT and need manual review.\\n\")\n",
    "\n",
    "for _, r in df_l3.sort_values(\"account_count\", ascending=False).iterrows():\n",
    "    print(\n",
    "        f\"  code={str(r['product_code']):>3}\"\n",
    "        f\" | accts={r['account_count']:>6,}\"\n",
    "        f\" | {r['source_file']:<7}\"\n",
    "        f\" | conf={r['confidence']:.2f}\"\n",
    "    )\n",
    "    print(f\"    desc: {r['product_name']}\")\n",
    "    print(\n",
    "        f\"    LLM:  {r['line_of_business']} > {r['product_type']}\"\n",
    "        f\" > {r['product_category']} > {r['product_subcategory']} > {r['product_special']}\"\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8 — Failure analysis (Layer 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = df_l1[~df_l1[\"exact_match\"]].copy()\n",
    "\n",
    "\n",
    "def failure_type(row):\n",
    "    if not row[\"match_L1\"]:\n",
    "        return \"WRONG LOB (L1)\"\n",
    "    if not row[\"match_L2\"]:\n",
    "        return \"WRONG TYPE (L2)\"\n",
    "    if not row[\"match_L3\"]:\n",
    "        return \"WRONG CATEGORY (L3)\"\n",
    "    if not row[\"match_L4\"]:\n",
    "        return \"WRONG SUBCATEGORY (L4)\"\n",
    "    if not row[\"match_L5\"]:\n",
    "        return \"WRONG SPECIAL (L5)\"\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "\n",
    "failures[\"failure_type\"] = failures.apply(failure_type, axis=1)\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(f\"FAILURE ANALYSIS — {len(failures)} mismatches out of {len(df_l1)} obvious codes\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "if len(failures) > 0:\n",
    "    ft = (\n",
    "        failures.groupby(\"failure_type\")\n",
    "        .agg(\n",
    "            count=(\"product_code\", \"count\"),\n",
    "            accounts=(\"account_count\", \"sum\"),\n",
    "            examples=(\"product_code\", lambda x: \", \".join(x.head(4))),\n",
    "        )\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "    print(\"\\nFailure type distribution:\")\n",
    "    print(ft.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo failures — all Layer 1 codes matched exactly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(failures) > 0:\n",
    "    print(\"=\" * 65)\n",
    "    print(\"DETAILED FAILURES (sorted by account count, highest impact first)\")\n",
    "    print(\"=\" * 65)\n",
    "\n",
    "    for _, r in failures.sort_values(\"account_count\", ascending=False).iterrows():\n",
    "        print(\n",
    "            f\"\\n  code={str(r['product_code']):>3}\"\n",
    "            f\" | accts={r['account_count']:>6,}\"\n",
    "            f\" | conf={r['confidence']:.2f}\"\n",
    "            f\" | {r['failure_type']}\"\n",
    "        )\n",
    "        print(f\"    name: {str(r['product_name'])[:55]}\")\n",
    "\n",
    "        for lvl, llm_col, gt_col in [\n",
    "            (\"L1\", \"line_of_business\", \"gt_L1_line_of_business\"),\n",
    "            (\"L2\", \"product_type\", \"gt_L2_type\"),\n",
    "            (\"L3\", \"product_category\", \"gt_L3_category\"),\n",
    "            (\"L4\", \"product_subcategory\", \"gt_L4_subcategory\"),\n",
    "            (\"L5\", \"product_special\", \"gt_L5_special\"),\n",
    "        ]:\n",
    "            llm_val = str(r[llm_col])\n",
    "            gt_val  = str(r[gt_col])\n",
    "            ok      = \"Y\" if r[f\"match_{lvl}\"] else \"X\"\n",
    "            if not r[f\"match_{lvl}\"]:\n",
    "                print(f'    {lvl}: [{ok}] LLM=\"{llm_val}\" vs GT=\"{gt_val}\"')\n",
    "            else:\n",
    "                print(f'    {lvl}: [{ok}] \"{llm_val}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9 — Cost summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 65)\n",
    "print(\"COST SUMMARY\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "total_tokens_in  = df_results[\"tokens_in\"].sum()\n",
    "total_tokens_out = df_results[\"tokens_out\"].sum()\n",
    "total_cost       = df_results[\"estimated_cost\"].sum()\n",
    "model_name       = df_results[\"model_name\"].iloc[0] if len(df_results) > 0 else \"N/A\"\n",
    "\n",
    "print(f\"  Model:            {model_name}\")\n",
    "print(f\"  Total tokens in:  {total_tokens_in:,}\")\n",
    "print(f\"  Total tokens out: {total_tokens_out:,}\")\n",
    "print(f\"  Total tokens:     {total_tokens_in + total_tokens_out:,}\")\n",
    "print(f\"  Estimated cost:   ${total_cost:.4f}\")\n",
    "print(f\"  Codes classified: {len(df_results)}\")\n",
    "print(f\"  Cost per code:    ${total_cost / max(len(df_results), 1):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10 — Cross-version comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_versions = (\n",
    "    spark.sql(f\"SELECT DISTINCT prompt_version FROM {RESULTS_TABLE} ORDER BY prompt_version\")\n",
    "    .toPandas()[\"prompt_version\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(f\"Prompt versions in results table: {all_versions}\")\n",
    "\n",
    "if len(all_versions) > 1:\n",
    "    version_metrics = []\n",
    "\n",
    "    for version in all_versions:\n",
    "        v_results = (\n",
    "            spark.sql(f\"SELECT * FROM {RESULTS_TABLE} WHERE prompt_version = '{version}'\")\n",
    "            .toPandas()\n",
    "        )\n",
    "        v_results[\"product_code\"] = v_results[\"product_code\"].astype(str).str.strip()\n",
    "\n",
    "        v_l1 = pd.merge(\n",
    "            v_results[v_results[\"layer\"] == 1],\n",
    "            df_gt_single,\n",
    "            on=\"product_code\",\n",
    "            how=\"left\",\n",
    "            suffixes=(\"\", \"_gt\"),\n",
    "        )\n",
    "        v_l1 = add_match_columns(v_l1)\n",
    "        v_accts = v_l1[\"account_count\"].sum()\n",
    "\n",
    "        vw_exact   = (v_l1[\"exact_match\"] * v_l1[\"account_count\"]).sum() / max(v_accts, 1)\n",
    "        vw_partial = (v_l1[\"partial_match_L1L2\"] * v_l1[\"account_count\"]).sum() / max(v_accts, 1)\n",
    "        v_cost     = v_results[\"estimated_cost\"].sum()\n",
    "\n",
    "        version_metrics.append({\n",
    "            \"prompt_version\":       version,\n",
    "            \"codes\":                len(v_results),\n",
    "            \"l1_codes\":             len(v_l1),\n",
    "            \"exact_match\":          f\"{v_l1['exact_match'].mean():.1%}\",\n",
    "            \"partial_L1L2\":         f\"{v_l1['partial_match_L1L2'].mean():.1%}\",\n",
    "            \"vol_weighted_exact\":   f\"{vw_exact:.1%}\",\n",
    "            \"vol_weighted_partial\": f\"{vw_partial:.1%}\",\n",
    "            \"L1_accuracy\":          f\"{v_l1['match_L1'].mean():.1%}\",\n",
    "            \"L2_accuracy\":          f\"{v_l1['match_L2'].mean():.1%}\",\n",
    "            \"L3_accuracy\":          f\"{v_l1['match_L3'].mean():.1%}\",\n",
    "            \"L4_accuracy\":          f\"{v_l1['match_L4'].mean():.1%}\",\n",
    "            \"L5_accuracy\":          f\"{v_l1['match_L5'].mean():.1%}\",\n",
    "            \"estimated_cost\":       f\"${v_cost:.4f}\",\n",
    "        })\n",
    "\n",
    "    df_comparison = pd.DataFrame(version_metrics)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 65)\n",
    "    print(\"CROSS-VERSION COMPARISON\")\n",
    "    print(\"=\" * 65)\n",
    "    print(df_comparison.to_string(index=False))\n",
    "\n",
    "    if len(version_metrics) >= 2:\n",
    "        prev = version_metrics[-2]\n",
    "        curr = version_metrics[-1]\n",
    "        print(f\"\\n  Comparing {prev['prompt_version']} -> {curr['prompt_version']}:\")\n",
    "        print(f\"    Acct-weighted exact: {prev['vol_weighted_exact']} -> {curr['vol_weighted_exact']}\")\n",
    "        print(f\"    Acct-weighted partial: {prev['vol_weighted_partial']} -> {curr['vol_weighted_partial']}\")\n",
    "        print(f\"    Cost: {prev['estimated_cost']} -> {curr['estimated_cost']}\")\n",
    "else:\n",
    "    print(\"Only one prompt version found — cross-version comparison will be available after additional runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11 — Summary report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accts_total = df_results[\"account_count\"].sum()\n",
    "vw_exact    = (df_l1[\"exact_match\"] * df_l1[\"account_count\"]).sum() / max(accts_l1, 1)\n",
    "vw_partial  = (df_l1[\"partial_match_L1L2\"] * df_l1[\"account_count\"]).sum() / max(accts_l1, 1)\n",
    "amb_match   = df_l2[\"exact_match_any\"].mean() if len(df_l2) > 0 else 0\n",
    "target_met  = vw_exact >= TARGET_ACCURACY\n",
    "status      = \"MET\" if target_met else \"BELOW TARGET\"\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"  PRODUCT CATEGORIZATION — ACCURACY SUMMARY\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"\")\n",
    "print(f\"  Prompt version:                  {df_results['prompt_version'].iloc[0]}\")\n",
    "print(f\"  Model:                           {model_name}\")\n",
    "print(f\"  Total codes evaluated:           {len(df_results)}\")\n",
    "print(f\"  Total accounts:                  {accts_total:,}\")\n",
    "print(f\"\")\n",
    "print(f\"  LAYER 1 — Obvious (single-mapping)\")\n",
    "print(f\"    Codes: {len(df_l1)}    Accounts: {accts_l1:,}\")\n",
    "print(f\"    L1 (LoB):             {df_l1['match_L1'].mean():.1%}\")\n",
    "print(f\"    L2 (Type):            {df_l1['match_L2'].mean():.1%}\")\n",
    "print(f\"    L3 (Category):        {df_l1['match_L3'].mean():.1%}\")\n",
    "print(f\"    L4 (Sub-category):    {df_l1['match_L4'].mean():.1%}\")\n",
    "print(f\"    L5 (Special):         {df_l1['match_L5'].mean():.1%}\")\n",
    "print(f\"    Exact Match (all 5):  {df_l1['exact_match'].mean():.1%}  (acct-wt: {vw_exact:.1%})\")\n",
    "print(f\"    Partial (L1+L2):      {df_l1['partial_match_L1L2'].mean():.1%}  (acct-wt: {vw_partial:.1%})\")\n",
    "print(f\"\")\n",
    "print(f\"  LAYER 2 — Ambiguous: {len(df_l2)} codes\")\n",
    "print(f\"    Match (any valid):    {amb_match:.1%}\")\n",
    "print(f\"\")\n",
    "print(f\"  LAYER 3 — Unknown: {len(df_l3)} codes (NEEDS MANUAL REVIEW)\")\n",
    "print(f\"\")\n",
    "if len(failures) > 0:\n",
    "    print(f\"  KEY FAILURES:\")\n",
    "    for ft_name in failures[\"failure_type\"].value_counts().index:\n",
    "        ft_count = len(failures[failures[\"failure_type\"] == ft_name])\n",
    "        print(f\"    {ft_name}: {ft_count} codes\")\n",
    "    print(f\"\")\n",
    "print(f\"  COST: ${total_cost:.4f} ({len(df_results)} codes)\")\n",
    "print(f\"\")\n",
    "print(f\"  TARGET: >= {TARGET_ACCURACY:.0%} acct-weighted exact match -> {status}\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 12 — Save evaluation results to Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1: full match details\n",
    "eval_l1 = df_l1[[\n",
    "    \"product_code\", \"layer\", \"prompt_version\", \"model_name\",\n",
    "    \"product_name\", \"account_count\", \"source_file\",\n",
    "    \"line_of_business\", \"product_type\", \"product_category\",\n",
    "    \"product_subcategory\", \"product_special\",\n",
    "    \"gt_L1_line_of_business\", \"gt_L2_type\", \"gt_L3_category\",\n",
    "    \"gt_L4_subcategory\", \"gt_L5_special\",\n",
    "    \"match_L1\", \"match_L2\", \"match_L3\", \"match_L4\", \"match_L5\",\n",
    "    \"exact_match\", \"partial_match_L1L2\", \"partial_match_L1L2L3\",\n",
    "    \"confidence\",\n",
    "]].copy()\n",
    "eval_l1[\"failure_type\"] = None\n",
    "if len(failures) > 0:\n",
    "    eval_l1.loc[\n",
    "        eval_l1[\"product_code\"].isin(failures[\"product_code\"]),\n",
    "        \"failure_type\",\n",
    "    ] = failures.set_index(\"product_code\")[\"failure_type\"]\n",
    "eval_l1[\"review_status\"] = \"EVALUATED\"\n",
    "\n",
    "# Layer 2\n",
    "eval_l2_records = []\n",
    "for _, row in df_l2_src.iterrows():\n",
    "    code = row[\"product_code\"]\n",
    "    l2_match = df_l2[df_l2[\"product_code\"] == code]\n",
    "    is_match = bool(l2_match[\"exact_match_any\"].iloc[0]) if len(l2_match) > 0 else False\n",
    "    eval_l2_records.append({\n",
    "        \"product_code\":           code,\n",
    "        \"layer\":                  row[\"layer\"],\n",
    "        \"prompt_version\":         row[\"prompt_version\"],\n",
    "        \"model_name\":             row[\"model_name\"],\n",
    "        \"product_name\":           row[\"product_name\"],\n",
    "        \"account_count\":          row[\"account_count\"],\n",
    "        \"source_file\":            row[\"source_file\"],\n",
    "        \"line_of_business\":       row[\"line_of_business\"],\n",
    "        \"product_type\":           row[\"product_type\"],\n",
    "        \"product_category\":       row[\"product_category\"],\n",
    "        \"product_subcategory\":    row[\"product_subcategory\"],\n",
    "        \"product_special\":        row[\"product_special\"],\n",
    "        \"gt_L1_line_of_business\": None,\n",
    "        \"gt_L2_type\":             None,\n",
    "        \"gt_L3_category\":         None,\n",
    "        \"gt_L4_subcategory\":      None,\n",
    "        \"gt_L5_special\":          None,\n",
    "        \"match_L1\":               None,\n",
    "        \"match_L2\":               None,\n",
    "        \"match_L3\":               None,\n",
    "        \"match_L4\":               None,\n",
    "        \"match_L5\":               None,\n",
    "        \"exact_match\":            is_match,\n",
    "        \"partial_match_L1L2\":     None,\n",
    "        \"partial_match_L1L2L3\":   None,\n",
    "        \"confidence\":             row[\"confidence\"],\n",
    "        \"failure_type\":           None if is_match else \"AMBIGUOUS_NO_MATCH\",\n",
    "        \"review_status\":          \"EVALUATED\",\n",
    "    })\n",
    "eval_l2 = pd.DataFrame(eval_l2_records) if eval_l2_records else pd.DataFrame()\n",
    "\n",
    "# Layer 3\n",
    "eval_l3_records = []\n",
    "for _, row in df_l3.iterrows():\n",
    "    eval_l3_records.append({\n",
    "        \"product_code\":           row[\"product_code\"],\n",
    "        \"layer\":                  row[\"layer\"],\n",
    "        \"prompt_version\":         row[\"prompt_version\"],\n",
    "        \"model_name\":             row[\"model_name\"],\n",
    "        \"product_name\":           row[\"product_name\"],\n",
    "        \"account_count\":          row[\"account_count\"],\n",
    "        \"source_file\":            row[\"source_file\"],\n",
    "        \"line_of_business\":       row[\"line_of_business\"],\n",
    "        \"product_type\":           row[\"product_type\"],\n",
    "        \"product_category\":       row[\"product_category\"],\n",
    "        \"product_subcategory\":    row[\"product_subcategory\"],\n",
    "        \"product_special\":        row[\"product_special\"],\n",
    "        \"gt_L1_line_of_business\": None,\n",
    "        \"gt_L2_type\":             None,\n",
    "        \"gt_L3_category\":         None,\n",
    "        \"gt_L4_subcategory\":      None,\n",
    "        \"gt_L5_special\":          None,\n",
    "        \"match_L1\":               None,\n",
    "        \"match_L2\":               None,\n",
    "        \"match_L3\":               None,\n",
    "        \"match_L4\":               None,\n",
    "        \"match_L5\":               None,\n",
    "        \"exact_match\":            None,\n",
    "        \"partial_match_L1L2\":     None,\n",
    "        \"partial_match_L1L2L3\":   None,\n",
    "        \"confidence\":             row[\"confidence\"],\n",
    "        \"failure_type\":           None,\n",
    "        \"review_status\":          \"NEEDS MANUAL REVIEW\",\n",
    "    })\n",
    "eval_l3_df = pd.DataFrame(eval_l3_records) if eval_l3_records else pd.DataFrame()\n",
    "\n",
    "# Combine all layers\n",
    "eval_parts = [df for df in [eval_l1, eval_l2, eval_l3_df] if len(df) > 0]\n",
    "df_eval = pd.concat(eval_parts, ignore_index=True) if eval_parts else pd.DataFrame()\n",
    "\n",
    "print(f\"Evaluation results: {len(df_eval)} rows\")\n",
    "print(f\"  Layer 1: {len(eval_l1)}\")\n",
    "print(f\"  Layer 2: {len(eval_l2)}\")\n",
    "print(f\"  Layer 3: {len(eval_l3_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sdf_eval = spark.createDataFrame(df_eval)\n",
    "    sdf_eval.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(EVAL_TABLE)\n",
    "    print(f\"Saved {len(df_eval)} rows to {EVAL_TABLE}\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC write.\")\n",
    "    print(f\"DataFrame ready with {len(df_eval)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {EVAL_TABLE}\").collect()[0][\"cnt\"]\n",
    "    print(f\"  OK  {EVAL_TABLE}: {count} rows\")\n",
    "\n",
    "    eval_cols = [f.name for f in spark.table(EVAL_TABLE).schema.fields]\n",
    "    required = [\n",
    "        \"product_code\", \"layer\", \"prompt_version\", \"model_name\",\n",
    "        \"exact_match\", \"failure_type\", \"review_status\",\n",
    "        \"line_of_business\", \"product_type\",\n",
    "        \"gt_L1_line_of_business\", \"gt_L2_type\",\n",
    "    ]\n",
    "    missing = [c for c in required if c not in eval_cols]\n",
    "    assert not missing, f\"Missing columns: {missing}\"\n",
    "    print(f\"  OK  All required columns present\")\n",
    "\n",
    "    status_counts = spark.sql(\n",
    "        f\"SELECT review_status, COUNT(*) as cnt FROM {EVAL_TABLE} GROUP BY review_status\"\n",
    "    ).toPandas()\n",
    "    print(f\"\\n  Review status:\")\n",
    "    for _, row in status_counts.iterrows():\n",
    "        print(f\"    {row['review_status']}: {row['cnt']} codes\")\n",
    "\n",
    "    print(\"\\nAll validations passed.\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC validation.\")\n",
    "    print(f\"Local DataFrame ready: {len(df_eval)} rows, columns: {list(df_eval.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}