{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 — Prepare Product Data\n",
    "\n",
    "**Objective:** Build the foundational Unity Catalog tables for the product categorization pipeline.\n",
    "\n",
    "| Section | Output Table | Description |\n",
    "|---------|-------------|-------------|\n",
    "| **A** | `product_ground_truth_normalized` | Parsed hierarchical GT from Deposits + Loans product catalogs |\n",
    "| **B** | `product_code_catalog` | Unique ACTYPE codes from raw Deposit/Loan/CD data with descriptions and account counts |\n",
    "| **C** | Layer assignment | Adds `layer` column to the catalog (Obvious / Ambiguous / Unknown) |\n",
    "\n",
    "**Runs on:** Databricks Runtime 15.4 LTS or above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Configuration ─────────────────────────────────────────────────\n",
    "CATALOG_NAME = \"ciq-bp_dummy-dev\"\n",
    "SCHEMA_NAME  = \"default\"\n",
    "\n",
    "DEPOSITS_GT_PATH = \"../../data/bank-plus-data/source-of-truth/products/Product catalog(Deposits).csv\"\n",
    "LOANS_GT_PATH    = \"../../data/bank-plus-data/source-of-truth/products/Product catalog(Loans).csv\"\n",
    "DDA_TYPES_PATH   = \"../../data/bank-plus-data/source-of-truth/products/Product catalog(DDA Types).csv\"\n",
    "LOAN_TYPES_PATH  = \"../../data/bank-plus-data/source-of-truth/products/Product catalog(Loan Types).csv\"\n",
    "\n",
    "RAW_DEPOSIT_PATH = \"../../data/bank-plus-data/raw/CheckingIQ_Deposit_ALL_*.csv\"\n",
    "RAW_LOAN_PATH    = \"../../data/bank-plus-data/raw/CheckingIQ_Loan_13Month_All_*.csv\"\n",
    "RAW_CD_PATH      = \"../../data/bank-plus-data/raw/CheckingIQ_CD_All_*.csv\"\n",
    "\n",
    "GT_TABLE      = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.product_ground_truth_normalized\"\n",
    "CATALOG_TABLE = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.product_code_catalog\"\n",
    "\n",
    "print(f\"Catalog:        {CATALOG_NAME}\")\n",
    "print(f\"Schema:         {SCHEMA_NAME}\")\n",
    "print(f\"GT table:       {GT_TABLE}\")\n",
    "print(f\"Catalog table:  {CATALOG_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Validation: check that input files exist ─────────────────────\n",
    "import os, glob as globmod\n",
    "\n",
    "for path, label in [\n",
    "    (DEPOSITS_GT_PATH, \"Deposits GT\"),\n",
    "    (LOANS_GT_PATH,    \"Loans GT\"),\n",
    "    (DDA_TYPES_PATH,   \"DDA Types reference\"),\n",
    "    (LOAN_TYPES_PATH,  \"Loan Types reference\"),\n",
    "]:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"  OK  {label}: {path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Missing {label}: {path}\")\n",
    "\n",
    "for pattern, label in [\n",
    "    (RAW_DEPOSIT_PATH, \"Raw Deposits\"),\n",
    "    (RAW_LOAN_PATH,    \"Raw Loans\"),\n",
    "    (RAW_CD_PATH,      \"Raw CDs\"),\n",
    "]:\n",
    "    matches = globmod.glob(pattern)\n",
    "    if matches:\n",
    "        print(f\"  OK  {label}: {len(matches)} file(s)\")\n",
    "    else:\n",
    "        print(f\"  WARN  {label}: no files matching {pattern} (will use Databricks paths)\")\n",
    "\n",
    "print(\"\\nInput file check complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section A — Ground Truth Normalization\n",
    "\n",
    "Parse the hierarchical Product Catalog CSVs (Deposits + Loans) into flat ground truth rows.\n",
    "\n",
    "The CSV files use **indentation-based hierarchy**: columns 1-5 map to taxonomy Levels 1-5,\n",
    "and rows that only populate a level column (without a product code) are hierarchy headers\n",
    "that set context for subsequent code rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_hierarchical_product_gt(path, product_domain, skip_unmapped_rows=0):\n",
    "    \"\"\"\n",
    "    Parse a hierarchical product catalog CSV into flat ground truth rows.\n",
    "\n",
    "    The CSV has this structure:\n",
    "      - Optional unmapped codes at the top (loan file only)\n",
    "      - A blank row + header rows (StrategyCorps level configuration, Level 1/2/3...)\n",
    "      - Hierarchy rows: a value in columns 0-4 sets that level for subsequent rows\n",
    "      - Code rows: columns 5 (product_code) and 6 (product_name) populated\n",
    "\n",
    "    Forward-fills L1-L4 from hierarchy headers to code rows.\n",
    "    \"\"\"\n",
    "    df_raw = pd.read_csv(path, header=None, encoding=\"latin-1\")\n",
    "\n",
    "    # Find the row where the actual hierarchy starts (after \"LoB,Type,Category...\" header)\n",
    "    start_idx = None\n",
    "    for i, row in df_raw.iterrows():\n",
    "        vals = [str(v).strip() for v in row.values if pd.notna(v) and str(v).strip()]\n",
    "        if vals and vals[0] == \"LoB\":\n",
    "            start_idx = i + 1\n",
    "            break\n",
    "\n",
    "    if start_idx is None:\n",
    "        raise ValueError(f\"Could not find hierarchy start in {path}\")\n",
    "\n",
    "    df_hier = df_raw.iloc[start_idx:].reset_index(drop=True)\n",
    "\n",
    "    # Columns: 0=L1(LoB), 1=L2(Type), 2=L3(Category), 3=L4(Sub-category), 4=L5(Special),\n",
    "    #          5=product_code, 6=product_name\n",
    "    current = {\"L1\": None, \"L2\": None, \"L3\": None, \"L4\": None, \"L5\": None}\n",
    "    rows = []\n",
    "\n",
    "    for _, row in df_hier.iterrows():\n",
    "        vals = [str(v).strip() if pd.notna(v) and str(v).strip() else None for v in row.values[:7]]\n",
    "        l1, l2, l3, l4, l5, code, name = (vals + [None] * 7)[:7]\n",
    "\n",
    "        # Update hierarchy context when a level header is present\n",
    "        if l1:\n",
    "            current[\"L1\"] = l1\n",
    "            current[\"L2\"] = None\n",
    "            current[\"L3\"] = None\n",
    "            current[\"L4\"] = None\n",
    "            current[\"L5\"] = None\n",
    "        if l2:\n",
    "            current[\"L2\"] = l2\n",
    "            current[\"L3\"] = None\n",
    "            current[\"L4\"] = None\n",
    "            current[\"L5\"] = None\n",
    "        if l3:\n",
    "            current[\"L3\"] = l3\n",
    "            current[\"L4\"] = None\n",
    "            current[\"L5\"] = None\n",
    "        if l4:\n",
    "            current[\"L4\"] = l4\n",
    "            current[\"L5\"] = None\n",
    "        if l5:\n",
    "            current[\"L5\"] = l5\n",
    "\n",
    "        # If there's a product code, this is a code row\n",
    "        if code:\n",
    "            rows.append({\n",
    "                \"product_code\":           code,\n",
    "                \"product_name\":           name,\n",
    "                \"gt_L1_line_of_business\":  current[\"L1\"],\n",
    "                \"gt_L2_type\":              current[\"L2\"] if current[\"L2\"] else product_domain,\n",
    "                \"gt_L3_category\":          current[\"L3\"],\n",
    "                \"gt_L4_subcategory\":       current[\"L4\"],\n",
    "                \"gt_L5_special\":           current[\"L5\"],\n",
    "                \"product_domain\":          product_domain,\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Strip whitespace\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "            df[col] = df[col].replace({\"None\": None, \"nan\": None, \"\": None})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"parse_hierarchical_product_gt() defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section A.1 — Deposits Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt_deposits = parse_hierarchical_product_gt(DEPOSITS_GT_PATH, product_domain=\"Deposits\")\n",
    "\n",
    "print(f\"Deposit GT rows: {len(df_gt_deposits)}\")\n",
    "print(f\"Unique product codes: {df_gt_deposits['product_code'].nunique()}\")\n",
    "print(f\"\\nLoB distribution:\")\n",
    "print(df_gt_deposits[\"gt_L1_line_of_business\"].value_counts().to_string())\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(df_gt_deposits[\"gt_L3_category\"].value_counts().to_string())\n",
    "print()\n",
    "df_gt_deposits.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section A.2 — Loans Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unmapped loan codes from the top of the file (before the hierarchy)\n",
    "df_loans_raw = pd.read_csv(LOANS_GT_PATH, header=None, encoding=\"latin-1\")\n",
    "\n",
    "unmapped_rows = []\n",
    "for i, row in df_loans_raw.iterrows():\n",
    "    vals = [str(v).strip() if pd.notna(v) and str(v).strip() else None for v in row.values]\n",
    "    # Stop when we hit the blank row or hierarchy headers\n",
    "    if vals[0] is None or vals[0] == \"\" or vals[0] == \"StrategyCorps level configuration\":\n",
    "        break\n",
    "    unmapped_rows.append({\"product_code\": vals[0], \"product_name\": vals[1] if len(vals) > 1 else None})\n",
    "\n",
    "df_unmapped_loans = pd.DataFrame(unmapped_rows)\n",
    "if len(df_unmapped_loans) > 0:\n",
    "    for col in df_unmapped_loans.columns:\n",
    "        if df_unmapped_loans[col].dtype == object:\n",
    "            df_unmapped_loans[col] = df_unmapped_loans[col].str.strip()\n",
    "\n",
    "print(f\"Unmapped loan codes (top of file): {len(df_unmapped_loans)}\")\n",
    "print(df_unmapped_loans.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt_loans = parse_hierarchical_product_gt(LOANS_GT_PATH, product_domain=\"Loans\")\n",
    "\n",
    "print(f\"Loan GT rows: {len(df_gt_loans)}\")\n",
    "print(f\"Unique product codes: {df_gt_loans['product_code'].nunique()}\")\n",
    "print(f\"\\nLoB distribution:\")\n",
    "print(df_gt_loans[\"gt_L1_line_of_business\"].value_counts().to_string())\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(df_gt_loans[\"gt_L3_category\"].value_counts().to_string())\n",
    "print()\n",
    "df_gt_loans.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section A.3 — Merge and Save Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt = pd.concat([df_gt_deposits, df_gt_loans], ignore_index=True)\n",
    "\n",
    "# Replace remaining artifacts\n",
    "df_gt.replace({\"None\": None, \"nan\": None}, inplace=True)\n",
    "\n",
    "print(f\"Combined ground truth: {len(df_gt)} rows, {df_gt['product_code'].nunique()} unique codes\")\n",
    "print(f\"\\nLine of Business:\")\n",
    "print(df_gt[\"gt_L1_line_of_business\"].value_counts().to_string())\n",
    "print(f\"\\nProduct Type (L2):\")\n",
    "print(df_gt[\"gt_L2_type\"].value_counts().to_string())\n",
    "print(f\"\\nCategory (L3):\")\n",
    "print(df_gt[\"gt_L3_category\"].value_counts().to_string())\n",
    "print(f\"\\nDomain:\")\n",
    "print(df_gt[\"product_domain\"].value_counts().to_string())\n",
    "print()\n",
    "df_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save ground truth to Unity Catalog ────────────────────────────\n",
    "try:\n",
    "    sdf_gt = spark.createDataFrame(df_gt)\n",
    "    sdf_gt.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(GT_TABLE)\n",
    "    print(f\"Saved {len(df_gt)} rows to {GT_TABLE}\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC write (run in Databricks).\")\n",
    "    print(f\"DataFrame ready with {len(df_gt)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section B — Product Code Catalog\n",
    "\n",
    "Build a catalog of unique product codes (ACTYPE) from raw Deposit, Loan, and CD files.\n",
    "For each code, capture a description and the account count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load reference lookups ────────────────────────────────────────\n",
    "df_dda_types = pd.read_csv(DDA_TYPES_PATH, encoding=\"latin-1\")\n",
    "df_dda_types.columns = [c.strip() for c in df_dda_types.columns]\n",
    "for col in df_dda_types.columns:\n",
    "    if df_dda_types[col].dtype == object:\n",
    "        df_dda_types[col] = df_dda_types[col].str.strip()\n",
    "\n",
    "# Build code-to-description lookup\n",
    "dda_code_col = df_dda_types.columns[0]\n",
    "dda_desc_col = df_dda_types.columns[1]\n",
    "dda_lookup = dict(zip(df_dda_types[dda_code_col].astype(str), df_dda_types[dda_desc_col]))\n",
    "print(f\"DDA Types lookup: {len(dda_lookup)} entries\")\n",
    "\n",
    "df_loan_types = pd.read_csv(LOAN_TYPES_PATH, encoding=\"latin-1\")\n",
    "df_loan_types.columns = [c.strip() for c in df_loan_types.columns]\n",
    "for col in df_loan_types.columns:\n",
    "    if df_loan_types[col].dtype == object:\n",
    "        df_loan_types[col] = df_loan_types[col].str.strip()\n",
    "\n",
    "loan_code_col = df_loan_types.columns[0]\n",
    "loan_desc_col = df_loan_types.columns[1]\n",
    "loan_lookup = dict(zip(df_loan_types[loan_code_col].astype(str), df_loan_types[loan_desc_col]))\n",
    "print(f\"Loan Types lookup: {len(loan_lookup)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Build Deposit catalog from raw data ────────────────────────────\n",
    "try:\n",
    "    sdf_dep = spark.read.csv(RAW_DEPOSIT_PATH, header=True, inferSchema=True)\n",
    "    df_dep_raw = sdf_dep.toPandas()\n",
    "except NameError:\n",
    "    import glob as globmod\n",
    "    dep_files = globmod.glob(RAW_DEPOSIT_PATH)\n",
    "    if dep_files:\n",
    "        df_dep_raw = pd.concat([pd.read_csv(f, encoding=\"latin-1\") for f in dep_files], ignore_index=True)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No files matching {RAW_DEPOSIT_PATH}\")\n",
    "\n",
    "df_dep_raw.columns = [c.strip() for c in df_dep_raw.columns]\n",
    "df_dep_raw[\"ACTYPE\"] = df_dep_raw[\"ACTYPE\"].astype(str).str.strip()\n",
    "\n",
    "dep_catalog = (\n",
    "    df_dep_raw\n",
    "    .groupby(\"ACTYPE\")\n",
    "    .agg(account_count=(\"ACTYPE\", \"size\"))\n",
    "    .reset_index()\n",
    ")\n",
    "dep_catalog[\"product_name\"] = dep_catalog[\"ACTYPE\"].map(dda_lookup)\n",
    "dep_catalog[\"product_name\"] = dep_catalog[\"product_name\"].fillna(\"(unknown)\")\n",
    "dep_catalog[\"source_file\"] = \"Deposit\"\n",
    "dep_catalog = dep_catalog.rename(columns={\"ACTYPE\": \"product_code\"})\n",
    "\n",
    "print(f\"Deposit catalog: {len(dep_catalog)} unique ACTYPE codes\")\n",
    "print(f\"Total accounts:  {dep_catalog['account_count'].sum():,}\")\n",
    "dep_catalog.sort_values(\"account_count\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Build Loan catalog from raw data ──────────────────────────────\n",
    "try:\n",
    "    sdf_loan = spark.read.csv(RAW_LOAN_PATH, header=True, inferSchema=True)\n",
    "    df_loan_raw = sdf_loan.toPandas()\n",
    "except NameError:\n",
    "    import glob as globmod\n",
    "    loan_files = globmod.glob(RAW_LOAN_PATH)\n",
    "    if loan_files:\n",
    "        df_loan_raw = pd.concat([pd.read_csv(f, encoding=\"latin-1\") for f in loan_files], ignore_index=True)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No files matching {RAW_LOAN_PATH}\")\n",
    "\n",
    "df_loan_raw.columns = [c.strip() for c in df_loan_raw.columns]\n",
    "df_loan_raw[\"ACTYPE\"] = df_loan_raw[\"ACTYPE\"].astype(str).str.strip()\n",
    "\n",
    "# Capture PURCOD and LoanTypeDesc for context\n",
    "loan_catalog = (\n",
    "    df_loan_raw\n",
    "    .groupby(\"ACTYPE\")\n",
    "    .agg(\n",
    "        account_count=(\"ACTYPE\", \"size\"),\n",
    "        sample_purcod=(\"PURCOD\", \"first\"),\n",
    "        sample_purpose_desc=(\"PurposeDescription\", \"first\"),\n",
    "        sample_loan_type_desc=(\"LoanTypeDesc\", \"first\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "loan_catalog[\"product_name\"] = loan_catalog[\"ACTYPE\"].map(loan_lookup)\n",
    "loan_catalog[\"product_name\"] = loan_catalog[\"product_name\"].fillna(\n",
    "    loan_catalog[\"sample_loan_type_desc\"].fillna(\"(unknown)\")\n",
    ")\n",
    "loan_catalog[\"source_file\"] = \"Loan\"\n",
    "loan_catalog = loan_catalog.rename(columns={\"ACTYPE\": \"product_code\"})\n",
    "\n",
    "print(f\"Loan catalog: {len(loan_catalog)} unique ACTYPE codes\")\n",
    "print(f\"Total accounts: {loan_catalog['account_count'].sum():,}\")\n",
    "loan_catalog.sort_values(\"account_count\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Build CD catalog from raw data ────────────────────────────────\n",
    "try:\n",
    "    sdf_cd = spark.read.csv(RAW_CD_PATH, header=True, inferSchema=True)\n",
    "    df_cd_raw = sdf_cd.toPandas()\n",
    "except NameError:\n",
    "    import glob as globmod\n",
    "    cd_files = globmod.glob(RAW_CD_PATH)\n",
    "    if cd_files:\n",
    "        df_cd_raw = pd.concat([pd.read_csv(f, encoding=\"latin-1\") for f in cd_files], ignore_index=True)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No files matching {RAW_CD_PATH}\")\n",
    "\n",
    "df_cd_raw.columns = [c.strip() for c in df_cd_raw.columns]\n",
    "df_cd_raw[\"ACTYPE\"] = df_cd_raw[\"ACTYPE\"].astype(str).str.strip()\n",
    "\n",
    "cd_catalog = (\n",
    "    df_cd_raw\n",
    "    .groupby(\"ACTYPE\")\n",
    "    .agg(account_count=(\"ACTYPE\", \"size\"))\n",
    "    .reset_index()\n",
    ")\n",
    "cd_catalog[\"product_name\"] = \"(CD type \" + cd_catalog[\"ACTYPE\"] + \")\"\n",
    "cd_catalog[\"source_file\"] = \"CD\"\n",
    "cd_catalog = cd_catalog.rename(columns={\"ACTYPE\": \"product_code\"})\n",
    "\n",
    "print(f\"CD catalog: {len(cd_catalog)} unique ACTYPE codes\")\n",
    "print(f\"Total accounts: {cd_catalog['account_count'].sum():,}\")\n",
    "cd_catalog.sort_values(\"account_count\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Combine into a single product catalog ─────────────────────────\n",
    "# Standardize loan catalog columns to match deposit/cd\n",
    "loan_extra_cols = [\"sample_purcod\", \"sample_purpose_desc\", \"sample_loan_type_desc\"]\n",
    "common_cols = [\"product_code\", \"product_name\", \"account_count\", \"source_file\"]\n",
    "\n",
    "df_catalog = pd.concat([\n",
    "    dep_catalog[common_cols],\n",
    "    loan_catalog[common_cols + loan_extra_cols],\n",
    "    cd_catalog[common_cols],\n",
    "], ignore_index=True)\n",
    "\n",
    "# Fill NaN for loan-specific columns in non-loan rows\n",
    "for col in loan_extra_cols:\n",
    "    if col not in df_catalog.columns:\n",
    "        df_catalog[col] = None\n",
    "\n",
    "df_catalog[\"product_code\"] = df_catalog[\"product_code\"].astype(str)\n",
    "\n",
    "print(f\"Combined product catalog: {len(df_catalog)} codes\")\n",
    "print(f\"  Deposit: {len(dep_catalog)}\")\n",
    "print(f\"  Loan:    {len(loan_catalog)}\")\n",
    "print(f\"  CD:      {len(cd_catalog)}\")\n",
    "print(f\"Total accounts: {df_catalog['account_count'].sum():,}\")\n",
    "df_catalog.sort_values(\"account_count\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section C — Layer Assignment\n",
    "\n",
    "Assign each catalog code to a test layer based on how it appears in the ground truth:\n",
    "\n",
    "| Layer | Name | Rule |\n",
    "|-------|------|------|\n",
    "| 1 | Obvious | Exactly 1 unique (L1,L2,L3,L4,L5) mapping in GT |\n",
    "| 2 | Ambiguous | 2+ distinct mappings in GT |\n",
    "| 3 | Unknown | product_code absent from GT entirely |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_mapping_counts = (\n",
    "    df_gt\n",
    "    .groupby(\"product_code\")\n",
    "    .apply(\n",
    "        lambda g: g[\n",
    "            [\"gt_L1_line_of_business\", \"gt_L2_type\", \"gt_L3_category\",\n",
    "             \"gt_L4_subcategory\", \"gt_L5_special\"]\n",
    "        ].drop_duplicates().shape[0]\n",
    "    )\n",
    "    .reset_index(name=\"n_mappings\")\n",
    ")\n",
    "\n",
    "multi_codes  = set(gt_mapping_counts.loc[gt_mapping_counts[\"n_mappings\"] > 1, \"product_code\"])\n",
    "single_codes = set(gt_mapping_counts.loc[gt_mapping_counts[\"n_mappings\"] == 1, \"product_code\"])\n",
    "all_gt_codes = set(df_gt[\"product_code\"].unique())\n",
    "\n",
    "print(f\"GT codes with 1 mapping (Layer 1):  {len(single_codes)}\")\n",
    "print(f\"GT codes with 2+ mappings (Layer 2): {len(multi_codes)}\")\n",
    "if multi_codes:\n",
    "    print(f\"Multi-mapping codes: {sorted(multi_codes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_layer(code):\n",
    "    if code not in all_gt_codes:\n",
    "        return 3  # Unknown\n",
    "    if code in multi_codes:\n",
    "        return 2  # Ambiguous\n",
    "    return 1      # Obvious\n",
    "\n",
    "\n",
    "df_catalog[\"layer\"] = df_catalog[\"product_code\"].apply(assign_layer)\n",
    "\n",
    "# ── Summary ───────────────────────────────────────────────────────\n",
    "layer_summary = (\n",
    "    df_catalog\n",
    "    .groupby(\"layer\")\n",
    "    .agg(codes=(\"product_code\", \"nunique\"), total_accounts=(\"account_count\", \"sum\"))\n",
    "    .reset_index()\n",
    ")\n",
    "layer_summary[\"pct_accounts\"] = (\n",
    "    layer_summary[\"total_accounts\"] / layer_summary[\"total_accounts\"].sum() * 100\n",
    ").round(1)\n",
    "\n",
    "layer_names = {1: \"Obvious\", 2: \"Ambiguous\", 3: \"Unknown\"}\n",
    "layer_summary[\"name\"] = layer_summary[\"layer\"].map(layer_names)\n",
    "\n",
    "print(\"Layer assignment summary:\")\n",
    "print(layer_summary[[\"layer\", \"name\", \"codes\", \"total_accounts\", \"pct_accounts\"]].to_string(index=False))\n",
    "print(f\"\\nTotal codes: {len(df_catalog)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Show codes per layer ──────────────────────────────────────────\n",
    "for layer_num, layer_name in layer_names.items():\n",
    "    layer_df = df_catalog[df_catalog[\"layer\"] == layer_num].sort_values(\n",
    "        \"account_count\", ascending=False\n",
    "    )\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Layer {layer_num} — {layer_name} ({len(layer_df)} codes)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for _, row in layer_df.iterrows():\n",
    "        print(\n",
    "            f\"  code={str(row['product_code']):>3}\"\n",
    "            f\" | accts={row['account_count']:>7,}\"\n",
    "            f\" | {row['source_file']:<7}\"\n",
    "            f\" | {str(row['product_name'])[:45]}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save catalog (with layers) to Unity Catalog ───────────────────\n",
    "try:\n",
    "    sdf_catalog = spark.createDataFrame(df_catalog)\n",
    "    sdf_catalog.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(CATALOG_TABLE)\n",
    "    print(f\"Saved {len(df_catalog)} rows to {CATALOG_TABLE}\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC write (run in Databricks).\")\n",
    "    print(f\"DataFrame ready with {len(df_catalog)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Validation\n",
    "\n",
    "Verify that all Unity Catalog tables were created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for table_name, expected_label in [\n",
    "        (GT_TABLE, \"product_ground_truth_normalized\"),\n",
    "        (CATALOG_TABLE, \"product_code_catalog\"),\n",
    "    ]:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {table_name}\").collect()[0][\"cnt\"]\n",
    "        print(f\"  OK  {expected_label}: {count} rows\")\n",
    "\n",
    "    catalog_cols = [f.name for f in spark.table(CATALOG_TABLE).schema.fields]\n",
    "    assert \"layer\" in catalog_cols, \"Missing 'layer' column in catalog table\"\n",
    "    print(f\"  OK  catalog has 'layer' column\")\n",
    "\n",
    "    print(\"\\nAll validations passed.\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC validation (run in Databricks).\")\n",
    "    print(\"Local DataFrames are ready:\")\n",
    "    print(f\"  df_gt:      {len(df_gt)} rows, {df_gt['product_code'].nunique()} unique codes\")\n",
    "    print(f\"  df_catalog: {len(df_catalog)} rows, columns: {list(df_catalog.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}