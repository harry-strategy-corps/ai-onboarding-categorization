{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 — Categorize Products\n",
    "\n",
    "**Objective:** Classify every product code in the catalog into the StrategyCorp\n",
    "product taxonomy using `ai_query()` with `responseFormat` for structured JSON output.\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "1. Read the `product_code_catalog` from Unity Catalog (produced by `01_prepare_product_data`).\n",
    "2. Load the product taxonomy markdown as prompt context.\n",
    "3. Batch codes (10 per prompt, grouped by source_file) and call `ai_query()` with a JSON schema response format.\n",
    "4. Track token estimates and cost per batch.\n",
    "5. Save all results to a single `product_classification_results` UC table with `layer`, `prompt_version`, and cost metadata.\n",
    "\n",
    "**Runs on:** Databricks Runtime 15.4 LTS or above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Configuration ─────────────────────────────────────────────────\n",
    "CATALOG_NAME    = \"ciq-bp_dummy-dev\"\n",
    "SCHEMA_NAME     = \"default\"\n",
    "MODEL_NAME      = \"databricks-claude-opus-4-6\"\n",
    "PROMPT_VERSION  = \"v1.0\"\n",
    "BATCH_SIZE      = 10\n",
    "TAXONOMY_PATH   = \"../../data/taxonomy/product_categorization_taxonomy.md\"\n",
    "\n",
    "CATALOG_TABLE  = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.product_code_catalog\"\n",
    "RESULTS_TABLE  = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.product_classification_results\"\n",
    "\n",
    "# Token pricing (Databricks Foundation Model Serving — approximate $/1K tokens)\n",
    "PRICING = {\n",
    "    \"databricks-claude-opus-4-6\": {\"input\": 0.015, \"output\": 0.075},\n",
    "    \"databricks-meta-llama-3-1-70b-instruct\": {\"input\": 0.001, \"output\": 0.001},\n",
    "}\n",
    "\n",
    "print(f\"Model:          {MODEL_NAME}\")\n",
    "print(f\"Prompt version: {PROMPT_VERSION}\")\n",
    "print(f\"Batch size:     {BATCH_SIZE}\")\n",
    "print(f\"Catalog table:  {CATALOG_TABLE}\")\n",
    "print(f\"Results table:  {RESULTS_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1 — Validate upstream tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    catalog_sdf = spark.table(CATALOG_TABLE)\n",
    "    catalog_cols = [f.name for f in catalog_sdf.schema.fields]\n",
    "    assert \"layer\" in catalog_cols, \"Missing 'layer' column in catalog — run 01_prepare_product_data first\"\n",
    "    assert \"product_code\" in catalog_cols, \"Missing 'product_code' column in catalog\"\n",
    "    assert \"product_name\" in catalog_cols, \"Missing 'product_name' column in catalog\"\n",
    "\n",
    "    df_catalog = catalog_sdf.toPandas()\n",
    "    df_catalog[\"product_code\"] = df_catalog[\"product_code\"].astype(str).str.strip()\n",
    "\n",
    "    print(f\"Loaded {len(df_catalog)} codes from {CATALOG_TABLE}\")\n",
    "    print(f\"Columns: {list(df_catalog.columns)}\")\n",
    "    print(f\"\\nLayer distribution:\")\n",
    "    for layer in sorted(df_catalog[\"layer\"].unique()):\n",
    "        n = len(df_catalog[df_catalog[\"layer\"] == layer])\n",
    "        print(f\"  Layer {layer}: {n} codes\")\n",
    "    print(f\"\\nSource file distribution:\")\n",
    "    print(df_catalog[\"source_file\"].value_counts().to_string())\n",
    "except NameError:\n",
    "    print(\"Spark session not found — run this notebook in Databricks.\")\n",
    "    raise SystemExit(\"Requires Databricks environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2 — Load taxonomy and build system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(TAXONOMY_PATH):\n",
    "    raise FileNotFoundError(f\"Taxonomy file not found: {TAXONOMY_PATH}\")\n",
    "\n",
    "with open(TAXONOMY_PATH, \"r\") as f:\n",
    "    taxonomy_md = f.read()\n",
    "\n",
    "print(f\"Taxonomy loaded: {len(taxonomy_md)} chars (~{len(taxonomy_md)//4} tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = f\"\"\"You are a product categorization engine for a US bank.\n",
    "Given a list of product codes with descriptions and context, classify each into the\n",
    "StrategyCorp product taxonomy below.\n",
    "\n",
    "{taxonomy_md}\n",
    "\n",
    "### Rules:\n",
    "1. First determine the Line of Business (Level 1): Retail, Business, or Wealth Management.\n",
    "   - For loans: use PURCOD (purpose code) when available. Codes 01-02 = Retail, 03-06/09 = Business.\n",
    "   - For deposits: product names containing \"BUSINESS\", \"BUS\", \"COMMERCIAL\", \"CORPORATE\" = Business.\n",
    "   - Products with \"FIDUCIARY\", \"WMG\", \"Wealth\" = Wealth Management.\n",
    "   - Public Funds products are ambiguous — classify as Business if unclear.\n",
    "2. Determine Product Type (Level 2): Deposits, Loans, Services, Cash Management Services, Securities.\n",
    "3. Determine Category (Level 3) based on the product description and type code.\n",
    "4. Propose Sub-category (Level 4) when the description provides enough context:\n",
    "   - Deposits: Interest Bearing, Non-Interest Bearing, IOLTA, etc.\n",
    "   - Loans: LOC, Loan, specific sub-types.\n",
    "   - Leave null if insufficient information.\n",
    "5. Propose Special (Level 5) when applicable (e.g., ARM, Fixed, Custodial, <$1M, >$1M).\n",
    "   Leave null if insufficient information.\n",
    "6. Use EXACT strings from the taxonomy for Levels 1-3. Levels 4-5 are FI-configurable.\n",
    "7. Use \"Unclassified\" for any level if no mapping fits. Do not guess.\n",
    "8. `confidence`: 0.0 to 1.0 — your certainty in the classification.\n",
    "\n",
    "### Few-Shot Examples:\n",
    "\n",
    "Input: product_code=01, DESC=\"PERSONAL CHECKING\", DOMAIN=Deposit\n",
    "Output: {{{{\n",
    "  \"product_code\": \"01\",\n",
    "  \"line_of_business\": \"Retail\",\n",
    "  \"product_type\": \"Deposits\",\n",
    "  \"product_category\": \"Checking (DDA)\",\n",
    "  \"product_subcategory\": \"Non-interest bearing\",\n",
    "  \"product_special\": null,\n",
    "  \"confidence\": 0.98\n",
    "}}}}\n",
    "\n",
    "Input: product_code=B5, DESC=\"COMMERCIAL ANALYSIS\", DOMAIN=Deposit\n",
    "Output: {{{{\n",
    "  \"product_code\": \"B5\",\n",
    "  \"line_of_business\": \"Business\",\n",
    "  \"product_type\": \"Deposits\",\n",
    "  \"product_category\": \"Checking (DDA)\",\n",
    "  \"product_subcategory\": \"Analyzed\",\n",
    "  \"product_special\": null,\n",
    "  \"confidence\": 0.95\n",
    "}}}}\n",
    "\n",
    "Input: product_code=R1, DESC=\"Res RE - Fixed\", DOMAIN=Loan, PURCOD=02, PURPOSE=\"Real Estate — Residential\"\n",
    "Output: {{{{\n",
    "  \"product_code\": \"R1\",\n",
    "  \"line_of_business\": \"Retail\",\n",
    "  \"product_type\": \"Loans\",\n",
    "  \"product_category\": \"Mortgage Loans\",\n",
    "  \"product_subcategory\": \"Conforming\",\n",
    "  \"product_special\": \"Fixed\",\n",
    "  \"confidence\": 0.92\n",
    "}}}}\n",
    "\"\"\"\n",
    "\n",
    "print(f\"System prompt: {len(SYSTEM_PROMPT)} chars (~{len(SYSTEM_PROMPT)//4} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3 — Define responseFormat JSON schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_SCHEMA = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"product_classifications\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"classifications\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"product_code\":        {\"type\": \"string\"},\n",
    "                            \"line_of_business\":    {\"type\": \"string\"},\n",
    "                            \"product_type\":        {\"type\": \"string\"},\n",
    "                            \"product_category\":    {\"type\": \"string\"},\n",
    "                            \"product_subcategory\": {\"type\": [\"string\", \"null\"]},\n",
    "                            \"product_special\":     {\"type\": [\"string\", \"null\"]},\n",
    "                            \"confidence\":          {\"type\": \"number\"}\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"product_code\", \"line_of_business\", \"product_type\",\n",
    "                            \"product_category\", \"product_subcategory\",\n",
    "                            \"product_special\", \"confidence\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"classifications\"]\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "response_schema_str = json.dumps(RESPONSE_SCHEMA)\n",
    "print(f\"Response schema ready ({len(response_schema_str)} chars)\")\n",
    "print(json.dumps(RESPONSE_SCHEMA, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4 — Batch classification engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_prompt(batch_df):\n",
    "    \"\"\"Build the user prompt listing product codes for a single batch.\"\"\"\n",
    "    lines = []\n",
    "    for _, row in batch_df.iterrows():\n",
    "        desc = str(row[\"product_name\"]).strip()\n",
    "        if desc in (\"nan\", \"\", \"None\", \"(unknown)\"):\n",
    "            desc = \"(no description)\"\n",
    "\n",
    "        domain = str(row[\"source_file\"]).strip()\n",
    "        parts = [f'product_code={row[\"product_code\"]}', f'DESC=\"{desc}\"', f'DOMAIN={domain}']\n",
    "\n",
    "        # Add loan context when available\n",
    "        purcod = row.get(\"sample_purcod\")\n",
    "        if pd.notna(purcod) and str(purcod).strip() not in (\"\", \"nan\", \"None\"):\n",
    "            parts.append(f'PURCOD={purcod}')\n",
    "        purpose = row.get(\"sample_purpose_desc\")\n",
    "        if pd.notna(purpose) and str(purpose).strip() not in (\"\", \"nan\", \"None\"):\n",
    "            parts.append(f'PURPOSE=\"{str(purpose).strip()}\"')\n",
    "\n",
    "        lines.append(\", \".join(parts))\n",
    "\n",
    "    codes_text = \"\\n\".join(f\"  - {line}\" for line in lines)\n",
    "    return (\n",
    "        f\"Classify these {len(batch_df)} product codes:\\n{codes_text}\\n\\n\"\n",
    "        f'Return a JSON object with a \"classifications\" array containing one entry per code.'\n",
    "    )\n",
    "\n",
    "\n",
    "def estimate_tokens(text):\n",
    "    \"\"\"Rough token estimate: ~4 chars per token.\"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "\n",
    "def estimate_cost(tokens_in, tokens_out, model_name):\n",
    "    \"\"\"Calculate estimated cost in USD.\"\"\"\n",
    "    prices = PRICING.get(model_name, {\"input\": 0.0, \"output\": 0.0})\n",
    "    return (tokens_in * prices[\"input\"] + tokens_out * prices[\"output\"]) / 1000\n",
    "\n",
    "\n",
    "# Quick test\n",
    "test_batch = df_catalog.head(3)\n",
    "print(\"Sample user prompt:\")\n",
    "print(build_user_prompt(test_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_batch(batch_df, batch_num, source_file):\n",
    "    \"\"\"\n",
    "    Classify a batch of product codes via ai_query().\n",
    "    Returns a list of result dicts with metadata.\n",
    "    \"\"\"\n",
    "    user_prompt = build_user_prompt(batch_df)\n",
    "    full_prompt = SYSTEM_PROMPT + \"\\n\" + user_prompt\n",
    "    escaped = full_prompt.replace(\"'\", \"''\")\n",
    "    escaped_schema = response_schema_str.replace(\"'\", \"''\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT ai_query(\n",
    "        '{MODEL_NAME}',\n",
    "        '{escaped}',\n",
    "        responseFormat => '{escaped_schema}'\n",
    "    ) as result\n",
    "    \"\"\"\n",
    "\n",
    "    tokens_in = estimate_tokens(full_prompt)\n",
    "\n",
    "    print(f\"  Batch {batch_num} | {source_file} | {len(batch_df)} codes | ~{tokens_in} tokens in\")\n",
    "\n",
    "    result_raw = spark.sql(query).collect()[0][\"result\"]\n",
    "    tokens_out = estimate_tokens(result_raw)\n",
    "    cost = estimate_cost(tokens_in, tokens_out, MODEL_NAME)\n",
    "\n",
    "    parsed = json.loads(result_raw)\n",
    "    classifications = parsed.get(\"classifications\", [])\n",
    "\n",
    "    run_ts = datetime.utcnow().isoformat()\n",
    "\n",
    "    batch_lookup = batch_df.set_index(\"product_code\").to_dict(orient=\"index\")\n",
    "\n",
    "    results = []\n",
    "    for cls in classifications:\n",
    "        pc = str(cls[\"product_code\"]).strip()\n",
    "        meta = batch_lookup.get(pc, {})\n",
    "        results.append({\n",
    "            \"product_code\":        pc,\n",
    "            \"product_name\":        meta.get(\"product_name\", \"\"),\n",
    "            \"source_file\":         meta.get(\"source_file\", source_file),\n",
    "            \"account_count\":       meta.get(\"account_count\", 0),\n",
    "            \"layer\":               meta.get(\"layer\", 3),\n",
    "            \"line_of_business\":    cls.get(\"line_of_business\"),\n",
    "            \"product_type\":        cls.get(\"product_type\"),\n",
    "            \"product_category\":    cls.get(\"product_category\"),\n",
    "            \"product_subcategory\": cls.get(\"product_subcategory\"),\n",
    "            \"product_special\":     cls.get(\"product_special\"),\n",
    "            \"confidence\":          cls.get(\"confidence\"),\n",
    "            \"prompt_version\":      PROMPT_VERSION,\n",
    "            \"model_name\":          MODEL_NAME,\n",
    "            \"run_timestamp\":       run_ts,\n",
    "            \"tokens_in\":           tokens_in,\n",
    "            \"tokens_out\":          tokens_out,\n",
    "            \"estimated_cost\":      cost,\n",
    "            \"llm_raw\":             result_raw,\n",
    "        })\n",
    "\n",
    "    print(f\"    -> Parsed {len(results)} classifications | ~{tokens_out} tokens out | ${cost:.4f}\")\n",
    "    return results\n",
    "\n",
    "print(\"classify_batch() defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5 — Run classification by source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "all_results = []\n",
    "layer_names = {1: \"Obvious\", 2: \"Ambiguous\", 3: \"Unknown\"}\n",
    "\n",
    "for source_file in [\"Deposit\", \"Loan\", \"CD\"]:\n",
    "    source_df = df_catalog[df_catalog[\"source_file\"] == source_file].reset_index(drop=True)\n",
    "    if len(source_df) == 0:\n",
    "        print(f\"\\nSkipping {source_file}: no codes\")\n",
    "        continue\n",
    "\n",
    "    n_batches = math.ceil(len(source_df) / BATCH_SIZE)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{source_file} | {len(source_df)} codes | {n_batches} batches\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        batch = source_df.iloc[i * BATCH_SIZE : (i + 1) * BATCH_SIZE]\n",
    "        try:\n",
    "            batch_results = classify_batch(batch, i + 1, source_file)\n",
    "            all_results.extend(batch_results)\n",
    "        except Exception as e:\n",
    "            print(f\"    ERROR in batch {i+1}: {e}\")\n",
    "            for _, row in batch.iterrows():\n",
    "                all_results.append({\n",
    "                    \"product_code\":        str(row[\"product_code\"]),\n",
    "                    \"product_name\":        row.get(\"product_name\", \"\"),\n",
    "                    \"source_file\":         source_file,\n",
    "                    \"account_count\":       row.get(\"account_count\", 0),\n",
    "                    \"layer\":               row.get(\"layer\", 3),\n",
    "                    \"line_of_business\":    \"ERROR\",\n",
    "                    \"product_type\":        str(e)[:200],\n",
    "                    \"product_category\":    None,\n",
    "                    \"product_subcategory\": None,\n",
    "                    \"product_special\":     None,\n",
    "                    \"confidence\":          0.0,\n",
    "                    \"prompt_version\":      PROMPT_VERSION,\n",
    "                    \"model_name\":          MODEL_NAME,\n",
    "                    \"run_timestamp\":       datetime.utcnow().isoformat(),\n",
    "                    \"tokens_in\":           0,\n",
    "                    \"tokens_out\":          0,\n",
    "                    \"estimated_cost\":      0.0,\n",
    "                    \"llm_raw\":             str(e),\n",
    "                })\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TOTAL: {len(all_results)} classifications collected\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6 — Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "print(f\"Total results: {len(df_results)}\")\n",
    "print(f\"Columns: {list(df_results.columns)}\\n\")\n",
    "\n",
    "for source in [\"Deposit\", \"Loan\", \"CD\"]:\n",
    "    src_df = df_results[df_results[\"source_file\"] == source]\n",
    "    errors = len(src_df[src_df[\"line_of_business\"] == \"ERROR\"])\n",
    "\n",
    "    print(f\"{source}:\")\n",
    "    print(f\"  Codes:      {len(src_df)}\")\n",
    "    print(f\"  Errors:     {errors}\")\n",
    "    print(f\"  Tokens in:  {src_df['tokens_in'].sum():,}\")\n",
    "    print(f\"  Tokens out: {src_df['tokens_out'].sum():,}\")\n",
    "    print(f\"  Est. cost:  ${src_df['estimated_cost'].sum():.4f}\")\n",
    "    print()\n",
    "\n",
    "total_cost = df_results[\"estimated_cost\"].sum()\n",
    "print(f\"Total estimated cost: ${total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = df_results[df_results[\"line_of_business\"] != \"ERROR\"]\n",
    "\n",
    "print(\"Line of Business (L1) distribution:\")\n",
    "print(valid[\"line_of_business\"].value_counts().to_string())\n",
    "\n",
    "print(\"\\nProduct Type (L2) distribution:\")\n",
    "print(valid[\"product_type\"].value_counts().to_string())\n",
    "\n",
    "print(\"\\nProduct Category (L3) distribution:\")\n",
    "print(valid[\"product_category\"].value_counts().to_string())\n",
    "\n",
    "print(\"\\nConfidence statistics:\")\n",
    "print(valid[\"confidence\"].describe().to_string())\n",
    "\n",
    "low_conf = valid[valid[\"confidence\"] < 0.8]\n",
    "if len(low_conf) > 0:\n",
    "    print(f\"\\nLow-confidence ({len(low_conf)} codes < 0.8):\")\n",
    "    for _, row in low_conf.iterrows():\n",
    "        print(\n",
    "            f\"  code={row['product_code']:>3} | conf={row['confidence']:.2f}\"\n",
    "            f\" | {row['line_of_business']} > {row['product_type']} > {row['product_category']}\"\n",
    "        )\n",
    "else:\n",
    "    print(\"\\nNo low-confidence classifications (all >= 0.8).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7 — Save results to Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save = df_results.copy()\n",
    "df_to_save[\"account_count\"] = df_to_save[\"account_count\"].astype(int)\n",
    "df_to_save[\"tokens_in\"] = df_to_save[\"tokens_in\"].astype(int)\n",
    "df_to_save[\"tokens_out\"] = df_to_save[\"tokens_out\"].astype(int)\n",
    "df_to_save[\"confidence\"] = df_to_save[\"confidence\"].astype(float)\n",
    "df_to_save[\"estimated_cost\"] = df_to_save[\"estimated_cost\"].astype(float)\n",
    "\n",
    "try:\n",
    "    sdf_results = spark.createDataFrame(df_to_save)\n",
    "    sdf_results.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(RESULTS_TABLE)\n",
    "    print(f\"Saved {len(df_to_save)} rows to {RESULTS_TABLE}\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC write.\")\n",
    "    print(f\"DataFrame ready with {len(df_to_save)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8 — Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {RESULTS_TABLE}\").collect()[0][\"cnt\"]\n",
    "    print(f\"  OK  {RESULTS_TABLE}: {count} rows\")\n",
    "\n",
    "    result_cols = [f.name for f in spark.table(RESULTS_TABLE).schema.fields]\n",
    "    required = [\n",
    "        \"product_code\", \"layer\", \"line_of_business\", \"product_type\",\n",
    "        \"product_category\", \"prompt_version\", \"model_name\",\n",
    "        \"tokens_in\", \"tokens_out\", \"estimated_cost\",\n",
    "    ]\n",
    "    missing = [c for c in required if c not in result_cols]\n",
    "    assert not missing, f\"Missing columns: {missing}\"\n",
    "    print(f\"  OK  All required columns present\")\n",
    "\n",
    "    err_count = spark.sql(\n",
    "        f\"SELECT COUNT(*) as cnt FROM {RESULTS_TABLE} WHERE line_of_business = 'ERROR'\"\n",
    "    ).collect()[0][\"cnt\"]\n",
    "    if err_count > 0:\n",
    "        print(f\"  WARN  {err_count} codes had classification errors\")\n",
    "    else:\n",
    "        print(f\"  OK  No classification errors\")\n",
    "\n",
    "    source_counts = spark.sql(\n",
    "        f\"SELECT source_file, COUNT(*) as cnt FROM {RESULTS_TABLE} GROUP BY source_file ORDER BY source_file\"\n",
    "    ).toPandas()\n",
    "    print(f\"\\n  Source file coverage:\")\n",
    "    for _, row in source_counts.iterrows():\n",
    "        print(f\"    {row['source_file']}: {row['cnt']} codes\")\n",
    "\n",
    "    print(\"\\nAll validations passed.\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping validation (run in Databricks).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}