{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 â€” Evaluate Accuracy\n",
    "\n",
    "**Objective:** Quantify the performance of the transaction categorization model by comparing LLM predictions against the normalized ground truth.\n",
    "\n",
    "### Metrics to Calculate:\n",
    "1. **Exact Match Rate:** L1 through L4 all match correctly.\n",
    "2. **Partial Match Rate:** L1 and L2 match (correct block and primary category).\n",
    "3. **Per-Level Accuracy:** Accuracy scores for L1, L2, L3, and L4 independently.\n",
    "4. **Per-Layer Accuracy:** Accuracy breakdown by test layer (Obvious vs. Ambiguous).\n",
    "5. **Failure Analysis:** Identification of specific codes where the model consistently fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Paths\n",
    "gt_path = \"../taxonomy/data/ground_truth_normalized.csv\"\n",
    "results_path = \"../results/latest_run.csv\" # Path to results exported from Step 04\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "df_gt = pd.read_csv(gt_path, dtype={'TRANCD': str})\n",
    "\n",
    "if os.path.exists(results_path):\n",
    "    df_res = pd.read_csv(results_path, dtype={'TRANCD': str})\n",
    "    print(f\"Loaded {len(df_res)} result rows.\")\n",
    "else:\n",
    "    print(f\"Results file not found at {results_path}. Run Notebook 04 first.\")\n",
    "    # Creating dummy results for structure demonstration\n",
    "    df_res = pd.DataFrame(columns=['TRANCD', 'category_1', 'category_2', 'category_3', 'category_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df_merged):\n",
    "    # Exact matches\n",
    "    df_merged['match_l1'] = (df_merged['category_1'] == df_merged['L1'])\n",
    "    df_merged['match_l2'] = (df_merged['category_2'] == df_merged['L2'])\n",
    "    df_merged['match_l3'] = (df_merged['category_3'].fillna('null') == df_merged['L3'].fillna('null'))\n",
    "    df_merged['match_l4'] = (df_merged['category_4'].fillna('null') == df_merged['L4'].fillna('null'))\n",
    "    \n",
    "    df_merged['exact_match'] = df_merged[['match_l1', 'match_l2', 'match_l3', 'match_l4']].all(axis=1)\n",
    "    df_merged['partial_match'] = df_merged[['match_l1', 'match_l2']].all(axis=1)\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "if not df_res.empty:\n",
    "    # Merge results with ground truth on TRANCD\n",
    "    df_eval = pd.merge(df_res, df_gt, on='TRANCD', how='inner', suffixes=('_llm', '_gt'))\n",
    "    df_eval = calculate_metrics(df_eval)\n",
    "    \n",
    "    print(\"--- Overall Accuracy ---\")\n",
    "    print(f\"Exact Match Rate: {df_eval['exact_match'].mean():.2%}\")\n",
    "    print(f\"Partial Match (L1+L2): {df_eval['partial_match'].mean():.2%}\")\n",
    "    print(f\"L1 Accuracy: {df_eval['match_l1'].mean():.2%}\")\n",
    "    print(f\"L2 Accuracy: {df_eval['match_l2'].mean():.2%}\")\n",
    "    print(f\"L3 Accuracy: {df_eval['match_l3'].mean():.2%}\")\n",
    "    print(f\"L4 Accuracy: {df_eval['match_l4'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_res.empty:\n",
    "    print(\"--- Top 10 Failures ---\")\n",
    "    failures = df_eval[~df_eval['exact_match']].copy()\n",
    "    # Show expected vs actual for the first few levels\n",
    "    cols_to_show = ['TRANCD', 'sample_desc_1', 'L1', 'category_1', 'L2', 'category_2']\n",
    "    print(failures[cols_to_show].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Evaluation Report\n",
    "Outputs a detailed CSV of matches/mismatches for manual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_res.empty:\n",
    "    output_eval_path = \"../results/eval_report_latest.csv\"\n",
    "    df_eval.to_csv(output_eval_path, index=False)\n",
    "    print(f\"Evaluation report saved to: {output_eval_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
