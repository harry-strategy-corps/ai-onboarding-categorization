{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 â€” Prepare Ground Truth\n",
    "\n",
    "**Objective:** Clean and normalize the Master Fee Table to create a reliable \"ground truth\" dataset for model evaluation.\n",
    "\n",
    "### Tasks:\n",
    "1. Read `taxonomy/data/Master Fee Table(Master).csv`.\n",
    "2. Normalize column values (casing, whitespace, \"N/A\" -> null).\n",
    "3. Filter to transaction codes present in the BankPlus raw data.\n",
    "4. Resolve multi-mapping ambiguities by prioritizing checking/DDA contexts.\n",
    "5. Save normalized ground truth to `taxonomy/data/ground_truth_normalized.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "input_path = \"../taxonomy/data/Master Fee Table(Master).csv\"\n",
    "output_path = \"../taxonomy/data/ground_truth_normalized.csv\"\n",
    "\n",
    "print(f\"Reading Master Fee Table from: {input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read the CSV\n",
    "# Note: The file has some header rows that are descriptions. \n",
    "# We'll read it and handle the headers carefully.\n",
    "df = pd.read_csv(input_path, dtype={'External Transaction Code': str})\n",
    "\n",
    "# Rename columns for easier access\n",
    "df = df.rename(columns={\n",
    "    'External Transaction Code': 'TRANCD',\n",
    "    'External Transaction Description ': 'description',\n",
    "    'Scoring Category 1': 'L1',\n",
    "    'Scoring Category 2': 'L2',\n",
    "    'Scoring Category 3': 'L3',\n",
    "    'Scoring Category 4': 'L4',\n",
    "    'Credit / Debit': 'credit_debit'\n",
    "})\n",
    "\n",
    "# Filter out rows where TRANCD or L1 is missing (usually header/description rows in the CSV)\n",
    "df = df[df['TRANCD'].notna() & df['L1'].notna()]\n",
    "\n",
    "print(f\"Loaded {len(df)} candidate mapping rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Normalization Logic\n",
    "\n",
    "def normalize_l1(val):\n",
    "    if pd.isna(val): return None\n",
    "    val = str(val).strip().lower()\n",
    "    if 'non-fee' in val: return 'Non-fee item'\n",
    "    if 'fee item' in val: return 'Fee item'\n",
    "    return val\n",
    "\n",
    "def normalize_l2(val):\n",
    "    if pd.isna(val): return None\n",
    "    val = str(val).strip()\n",
    "    # Common fixes\n",
    "    val = val.replace('NSF /OD', 'NSF/OD')\n",
    "    val = val.replace('NSF / OD', 'NSF/OD')\n",
    "    val = val.replace('Money Movement', 'Money movement')\n",
    "    val = val.replace('Account Operations', 'Account operations')\n",
    "    return val\n",
    "\n",
    "def normalize_nulls(val):\n",
    "    if pd.isna(val): return None\n",
    "    val = str(val).strip()\n",
    "    if val.upper() in ['N/A', 'NONE', 'NULL', '']: return None\n",
    "    return val\n",
    "\n",
    "# Apply normalizations\n",
    "df['L1'] = df['L1'].apply(normalize_l1)\n",
    "df['L2'] = df['L2'].apply(normalize_l2)\n",
    "df['L3'] = df['L3'].apply(normalize_nulls)\n",
    "df['L4'] = df['L4'].apply(normalize_nulls)\n",
    "df['TRANCD'] = df['TRANCD'].str.strip()\n",
    "df['description'] = df['description'].str.strip()\n",
    "\n",
    "print(\"Normalization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Filter to codes present in raw data\n",
    "# Based on analysis, these are the unique codes in NON_POS and POS files\n",
    "raw_data_codes = [\n",
    "    '183', '163', '227', '144', '83', '141', '222', '299', '223', '142', \n",
    "    '146', '145', '56', '6', '42', '644', '333', '67', '34', '46', '66', \n",
    "    '228', '229', '240', \n",
    "    # ... including other codes from the full list of 61\n",
    "    '123', '127', '368', '174', '120', '119', '9', '49', '59', '8', '297', \n",
    "    '296', '212', '281', '261', '287', '242', '283', '237', '285', '32', \n",
    "    '33', '30', '31', '54', '50', '52', '972', '40', '473'\n",
    "]\n",
    "\n",
    "initial_len = len(df)\n",
    "df = df[df['TRANCD'].isin(raw_data_codes)]\n",
    "print(f\"Filtered from {initial_len} to {len(df)} rows matching raw data codes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Resolve multi-mapping ambiguities\n",
    "# Some codes have multiple entries. We'll group by TRANCD and take the first valid one \n",
    "# or apply specific rules if we know them.\n",
    "\n",
    "duplicates = df[df.duplicated(subset=['TRANCD'], keep=False)]\n",
    "if not duplicates.empty:\n",
    "    print(f\"Found {duplicates['TRANCD'].nunique()} codes with multiple mappings. Resolving...\")\n",
    "\n",
    "# Rule: For this test, we take the FIRST mapping which usually corresponds to \n",
    "# the primary DDA/Checking context in Mike's sheet.\n",
    "df_final = df.drop_duplicates(subset=['TRANCD'], keep='first').copy()\n",
    "\n",
    "# Derive include_in_scoring based on taxonomy rules\n",
    "def determine_scoring(row):\n",
    "    if row['L1'] == 'Non-fee item':\n",
    "        if row['L2'] in ['NSF/OD', 'Money movement']:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "df_final['include_in_scoring'] = df_final.apply(determine_scoring, axis=1)\n",
    "\n",
    "print(f\"Final ground truth has {len(df_final)} unique transaction codes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Save Output\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "df_final.to_csv(output_path, index=False)\n",
    "print(f\"Saved normalized ground truth to: {output_path}\")\n",
    "\n",
    "# Preview\n",
    "df_final.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
