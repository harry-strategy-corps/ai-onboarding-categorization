{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 â€” Discover Available Models\n",
    "\n",
    "**Objective:** Enumerate the model serving endpoints available in the current Databricks workspace and verify that `ai_query()` and `ai_classify()` are functioning correctly.\n",
    "\n",
    "### Why this matters:\n",
    "The available foundation models (Llama, Claude, DBRX, etc.) vary by region and workspace configuration. We need to identify which model to use for the categorization task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. List Serving Endpoints\n",
    "Run this cell in a Databricks environment to see available foundation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This requires databricks-sdk\n",
    "try:\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    w = WorkspaceClient()\n",
    "    print(\"Available Serving Endpoints:\")\n",
    "    for endpoint in w.serving_endpoints.list():\n",
    "        print(f\"- {endpoint.name} ({endpoint.state.config_update})\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not list endpoints via SDK: {e}\")\n",
    "    print(\"Try running SQL 'SHOW SERVING ENDPOINTS' if available, or check the 'Serving' UI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test `ai_classify()`\n",
    "Verify the task-specific classification function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Databricks, you can run this as a SQL cell or via spark.sql()\n",
    "test_classify_sql = \"\"\"\n",
    "SELECT ai_classify(\"Business Elite Checking\", ARRAY(\"Checking\", \"Savings\", \"CD\", \"Money Market\", \"Loan\"))\n",
    "\"\"\"\n",
    "try:\n",
    "    display(spark.sql(test_classify_sql))\n",
    "except NameError:\n",
    "    print(\"Spark session not found. Run this in a Databricks notebook cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test `ai_query()`\n",
    "Verify the general-purpose LLM function works. Replace the model name if needed based on the output of Step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Databricks model names:\n",
    "# - databricks-meta-llama-3-1-70b-instruct\n",
    "# - databricks-meta-llama-3-1-405b-instruct\n",
    "# - databricks-dbrx-instruct\n",
    "\n",
    "model_name = \"databricks-meta-llama-3-1-70b-instruct\" \n",
    "\n",
    "test_query_sql = f\"\"\"\n",
    "SELECT ai_query(\n",
    "  '{model_name}',\n",
    "  'Classify this bank transaction as Fee or Non-fee: ATM Withdrawal. Return only one word.'\n",
    ")\n",
    "\"\"\"\n",
    "try:\n",
    "    display(spark.sql(test_query_sql))\n",
    "except NameError:\n",
    "    print(\"Spark session not found. Run this in a Databricks notebook cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conclusion\n",
    "Document the chosen model and endpoint for use in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
