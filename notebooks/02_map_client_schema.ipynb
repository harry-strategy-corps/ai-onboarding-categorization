{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf53fa70-fccd-4989-9501-5f9b3cc0ad4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 02 — Map Client Schema (Prototype)\n",
    "\n",
    "**Objective:** Prototype an AI-based column mapping tool.\n",
    "\n",
    "**Note:** This logic has been integrated into `01_prepare_data.ipynb` as Section B.0.\n",
    "This notebook remains as a standalone tool for testing and refining the mapping\n",
    "prompt independently of the full data preparation pipeline.\n",
    "\n",
    "### Why this matters\n",
    "\n",
    "Every FI's core banking system exports transaction data with different column names.\n",
    "Bank Plus (Jack Henry SilverLake) uses `TRANCD`, `EFHDS1`, `EFHDS2`, etc. Another\n",
    "FI on FIS or Fiserv might call them `TxnCode`, `Description`, `Memo`. Before the\n",
    "categorization pipeline can run, we need all client data normalized to a single\n",
    "universal format.\n",
    "\n",
    "### Canonical output columns\n",
    "\n",
    "| Canonical Column | Description |\n",
    "|-----------------|-------------|\n",
    "| `transaction_code` | Numeric transaction code (primary key for categorization) |\n",
    "| `description_1` | Primary transaction description text |\n",
    "| `description_2` | Secondary description / memo line |\n",
    "| `amount` | Transaction amount |\n",
    "| `transaction_date` | Date the transaction occurred |\n",
    "| `posting_date` | Date the transaction was posted |\n",
    "| `account_number` | Account identifier |\n",
    "| `account_status` | Account status code |\n",
    "| `internal_account` | Internal/alternate account number |\n",
    "| `transaction_desc` | Transaction type description (may be empty) |\n",
    "\n",
    "### Approach\n",
    "\n",
    "1. Take Bank Plus NON_POS data (which has client-specific column names like `TRANCD`, `EFHDS1`).\n",
    "2. Use `ai_query()` to ask the LLM to map each client column to the canonical schema.\n",
    "3. Validate against the known correct mapping.\n",
    "4. Apply the mapping to produce a standardized DataFrame.\n",
    "5. Save the mapping to Unity Catalog for downstream use.\n",
    "\n",
    "**Runs on:** Databricks Runtime 15.4 LTS or above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1893ee28-5816-427f-a2a9-aae18d175b49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Configuration ─────────────────────────────────────────────────\n",
    "CATALOG_NAME = \"ciq-bp_dummy-dev\"\n",
    "SCHEMA_NAME  = \"default\"\n",
    "MODEL_NAME   = \"databricks-claude-opus-4-6\"\n",
    "\n",
    "RAW_NON_POS_PATH = \"../data/bank-plus-data/raw/CheckingIQ_NON_POS_Daily_012626_rerun.csv\"\n",
    "\n",
    "MAPPINGS_TABLE = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.client_column_mappings\"\n",
    "\n",
    "# Generic canonical schema — universal column names for ALL clients.\n",
    "# Every client's data gets mapped to these same column names.\n",
    "CANONICAL_SCHEMA = {\n",
    "    \"transaction_code\":  \"Numeric transaction code (primary key for categorization)\",\n",
    "    \"description_1\":     \"Primary transaction description text\",\n",
    "    \"description_2\":     \"Secondary description / memo line\",\n",
    "    \"amount\":            \"Transaction amount\",\n",
    "    \"transaction_date\":  \"Date the transaction occurred\",\n",
    "    \"posting_date\":      \"Date the transaction was posted\",\n",
    "    \"account_number\":    \"Account identifier\",\n",
    "    \"account_status\":    \"Account status code\",\n",
    "    \"internal_account\":  \"Internal/alternate account number\",\n",
    "    \"transaction_desc\":  \"Transaction type description (may be empty)\",\n",
    "}\n",
    "\n",
    "# For Bank Plus, we know the correct mapping from their columns to canonical.\n",
    "# This is the ground truth used to validate the AI output.\n",
    "BANK_PLUS_CORRECT_MAPPING = {\n",
    "    \"TRANCD\":      \"transaction_code\",\n",
    "    \"EFHDS1\":      \"description_1\",\n",
    "    \"EFHDS2\":      \"description_2\",\n",
    "    \"AMT\":         \"amount\",\n",
    "    \"TRDATE\":      \"transaction_date\",\n",
    "    \"PostingDate\":  \"posting_date\",\n",
    "    \"ACCTNO\":      \"account_number\",\n",
    "    \"status\":      \"account_status\",\n",
    "    \"Account#\":    \"internal_account\",\n",
    "    \"description\": \"transaction_desc\",\n",
    "}\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Output table: {MAPPINGS_TABLE}\")\n",
    "print(f\"Canonical columns: {list(CANONICAL_SCHEMA.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29beaf6f-7114-43b8-adf8-60b30aa64b0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Step 1 — Load client data sample\n",
    "\n",
    "We take the first 20 rows of Bank Plus NON_POS data. These columns (`TRANCD`,\n",
    "`EFHDS1`, `EFHDS2`, etc.) are client-specific — the AI must map them to our\n",
    "generic canonical column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b15885c8-3bed-4c7a-96c8-60f4ecd2353b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def load_raw_csv_sample(path, n_rows=20):\n",
    "    \"\"\"\n",
    "    Robust CSV loader that handles unquoted commas in EFHDS1/EFHDS2.\n",
    "    Parses from both ends: front 6 cols + tail N cols are safe,\n",
    "    middle gets merged back into EFHDS1 + EFHDS2.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "\n",
    "    header = [c.strip() for c in header]\n",
    "    n_expected = len(header)\n",
    "    n_front = 6                        # ACCTNO → AMT\n",
    "    n_tail  = n_expected - 6 - 2       # Account# → end\n",
    "\n",
    "    rows = []\n",
    "    with open(path, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # skip header\n",
    "        for fields in reader:\n",
    "            if len(rows) >= n_rows:\n",
    "                break\n",
    "            n = len(fields)\n",
    "            if n == n_expected:\n",
    "                rows.append(fields)\n",
    "            elif n > n_expected:\n",
    "                front  = fields[:n_front]\n",
    "                tail   = fields[n - n_tail:]\n",
    "                middle = fields[n_front : n - n_tail]\n",
    "                efhds1 = \",\".join(middle[:-1])\n",
    "                efhds2 = middle[-1]\n",
    "                rows.append(front + [efhds1, efhds2] + tail)\n",
    "\n",
    "    return pd.DataFrame(rows, columns=header)\n",
    "\n",
    "\n",
    "df_original = load_raw_csv_sample(RAW_NON_POS_PATH, n_rows=20)\n",
    "df_original[\"TRANCD\"] = df_original[\"TRANCD\"].astype(str).str.strip()\n",
    "\n",
    "print(f\"Original columns: {list(df_original.columns)}\")\n",
    "print(f\"Sample rows: {len(df_original)}\")\n",
    "\n",
    "# Quick sanity check — values should make sense under each column\n",
    "print(f\"\\n  ACCTNO samples:  {df_original['ACCTNO'].head(3).tolist()}\")\n",
    "print(f\"  TRANCD samples:  {df_original['TRANCD'].head(3).tolist()}\")\n",
    "print(f\"  AMT samples:     {df_original['AMT'].head(3).tolist()}\")\n",
    "print(f\"  EFHDS1 samples:  {df_original['EFHDS1'].str.strip().head(3).tolist()}\")\n",
    "\n",
    "df_original.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45208e7c-c5a4-453a-9987-2b08e455be70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bank Plus columns are already \"client-specific\" — no simulation needed.\n",
    "# The AI must map these raw column names to the generic canonical schema.\n",
    "df_client = df_original.copy()\n",
    "\n",
    "print(f\"Client columns (Bank Plus / Jack Henry SilverLake):\")\n",
    "print(f\"  {list(df_client.columns)}\")\n",
    "print(f\"\\nKnown correct mapping (ground truth for validation):\")\n",
    "for client_col, canon_col in BANK_PLUS_CORRECT_MAPPING.items():\n",
    "    print(f\"  {client_col:<15} → {canon_col}\")\n",
    "\n",
    "df_client.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db983615-aa9b-447d-a80c-c8c9afef49ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Step 2 — Build the AI mapping prompt\n",
    "\n",
    "We send the LLM the client's actual column names, a few sample rows, and the\n",
    "generic canonical schema definition. The LLM must return a JSON object mapping\n",
    "each client column to a canonical column (or null if no match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59ac5bf1-66b1-4bcd-b0d7-a72d019d07fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_client = df_original.copy()\n",
    "\n",
    "# Build a rich column profile — name + inferred type + sample values\n",
    "column_profiles = []\n",
    "for col in df_client.columns:\n",
    "    vals = df_client[col].dropna().astype(str).str.strip()\n",
    "    vals = vals[vals != \"\"].head(5).tolist()\n",
    "\n",
    "    # Simple type inference\n",
    "    if all(v.replace(\".\", \"\").replace(\"-\", \"\").isdigit() for v in vals if v):\n",
    "        if any(\".\" in v for v in vals):\n",
    "            dtype = \"decimal numbers\"\n",
    "        elif all(len(v) <= 3 for v in vals):\n",
    "            dtype = \"small integers\"\n",
    "        else:\n",
    "            dtype = \"integers\"\n",
    "        # Check if looks like dates\n",
    "        if any(\"-\" in v and len(v) == 10 for v in vals):\n",
    "            dtype = \"dates (YYYY-MM-DD)\"\n",
    "    elif all(\"-\" in v and len(v) == 10 for v in vals if v):\n",
    "        dtype = \"dates (YYYY-MM-DD)\"\n",
    "    else:\n",
    "        dtype = \"text\"\n",
    "\n",
    "    column_profiles.append(f\"  - Column: \\\"{col}\\\"\\n    Data type: {dtype}\\n    Sample values: {vals}\")\n",
    "\n",
    "column_profiles_text = \"\\n\".join(column_profiles)\n",
    "\n",
    "canonical_schema_text = \"\\n\".join(\n",
    "    f\"  - {col}: {desc}\" for col, desc in CANONICAL_SCHEMA.items()\n",
    ")\n",
    "\n",
    "mapping_prompt = f\"\"\"You are a data engineering expert specializing in US banking core systems.\n",
    "\n",
    "A financial institution has provided a transaction data export. Their column names\n",
    "are abbreviations from their core banking system (e.g., Jack Henry SilverLake,\n",
    "Fiserv, FIS). Your task is to map each column to our canonical schema.\n",
    "\n",
    "### Canonical Schema (target — map TO these names)\n",
    "{canonical_schema_text}\n",
    "\n",
    "### Client Data Profile (source — map FROM these columns)\n",
    "{column_profiles_text}\n",
    "\n",
    "### Key Banking Domain Knowledge\n",
    "- Core systems use abbreviations: TRANCD = transaction code, EFHDS = extended\n",
    "  field header/description, AMT = amount, ACCTNO = account number, etc.\n",
    "- \"EFHDS1\" and \"EFHDS2\" are Jack Henry's names for description lines 1 and 2.\n",
    "- Transaction files often have TWO date columns: a transaction date and a posting\n",
    "  date. They may look similar but serve different purposes.\n",
    "- \"status\" is typically a small integer (1, 2, 3) indicating account status.\n",
    "- \"description\" (if mostly empty) is a legacy field for transaction type description.\n",
    "- There may be TWO account number columns: an external-facing one and an internal one.\n",
    "\n",
    "### Few-Shot Example (different bank, similar task)\n",
    "A Fiserv bank had these columns: TxnCode, Desc1, Desc2, TxnAmt, TxnDate,\n",
    "PostDate, AcctNum, AcctStat, IntAcct, TxnDesc.\n",
    "\n",
    "Correct mapping:\n",
    "  TxnCode  → transaction_code\n",
    "  Desc1    → description_1\n",
    "  Desc2    → description_2\n",
    "  TxnAmt   → amount\n",
    "  TxnDate  → transaction_date\n",
    "  PostDate → posting_date\n",
    "  AcctNum  → account_number\n",
    "  AcctStat → account_status\n",
    "  IntAcct  → internal_account\n",
    "  TxnDesc  → transaction_desc\n",
    "\n",
    "### Instructions\n",
    "1. MATCH BY DATA VALUES, not just column names. Look at the sample values to\n",
    "   determine what each column actually contains.\n",
    "2. Map each client column to exactly one canonical column, or null if no match.\n",
    "3. Each canonical column can only be used once.\n",
    "4. Think step by step:\n",
    "   - Which column has small integers (like 183, 163)? → transaction_code\n",
    "   - Which column has dollar amounts (like 258.20)? → amount\n",
    "   - Which column has text descriptions? → description_1 (primary), description_2 (secondary)\n",
    "   - Which columns have dates? → the one named for transaction date, the other for posting date\n",
    "   - Which column has alphanumeric account IDs? → account_number\n",
    "   - Which column has numeric-only account numbers? → internal_account\n",
    "   - Which column has single-digit status codes? → account_status\n",
    "   - Which column is mostly empty text? → transaction_desc\n",
    "\n",
    "Return ONLY a JSON object. Keys = client column names, values = canonical column names (or null).\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Prompt length: {len(mapping_prompt)} chars (~{len(mapping_prompt)//4} tokens)\")\n",
    "print(\"\\n--- Column profiles sent to LLM ---\")\n",
    "print(column_profiles_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f19edbc9-a770-4520-964c-1c36268466b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Step 3 — Call `ai_query()` for column mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bf29363-4d8c-46a8-bf0b-5acc57dc400c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 9"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "escaped_prompt = mapping_prompt.replace(\"'\", \"''\")\n",
    "\n",
    "# Build responseFormat: keys = client's column names, values = canonical names or null\n",
    "client_cols = list(df_client.columns)\n",
    "\n",
    "# Sanitize column names for JSON schema (replace invalid chars with underscore)\n",
    "# Pattern: ^[a-zA-Z0-9_.-]{1,64}$\n",
    "def sanitize_column_name(col_name):\n",
    "    return re.sub(r'[^a-zA-Z0-9_.-]', '_', col_name)\n",
    "\n",
    "sanitized_to_original = {sanitize_column_name(col): col for col in client_cols}\n",
    "properties = {sanitize_column_name(col): {\"type\": [\"string\", \"null\"]} for col in client_cols}\n",
    "\n",
    "response_schema = json.dumps({\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"column_mapping\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": properties,\n",
    "        },\n",
    "        \"strict\": True,\n",
    "    }\n",
    "})\n",
    "\n",
    "mapping_query = f\"\"\"\n",
    "SELECT ai_query(\n",
    "    '{MODEL_NAME}',\n",
    "    '{escaped_prompt}',\n",
    "    responseFormat => '{response_schema}'\n",
    ") as mapping_result\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    print(\"Executing ai_query for column mapping...\")\n",
    "    result_df = spark.sql(mapping_query)\n",
    "    mapping_raw_sanitized = result_df.collect()[0][\"mapping_result\"]\n",
    "    \n",
    "    # Convert sanitized keys back to original column names\n",
    "    mapping_sanitized = json.loads(mapping_raw_sanitized)\n",
    "    mapping_raw = json.dumps({\n",
    "        sanitized_to_original[k]: v for k, v in mapping_sanitized.items()\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nRaw LLM response:\\n{mapping_raw}\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — using mock response for local testing.\")\n",
    "    # Mock: client columns → generic canonical names\n",
    "    mapping_raw = json.dumps({\n",
    "        \"ACCTNO\":      \"account_number\",\n",
    "        \"status\":      \"account_status\",\n",
    "        \"TRANCD\":      \"transaction_code\",\n",
    "        \"description\": \"transaction_desc\",\n",
    "        \"TRDATE\":      \"transaction_date\",\n",
    "        \"AMT\":         \"amount\",\n",
    "        \"EFHDS1\":      \"description_1\",\n",
    "        \"EFHDS2\":      \"description_2\",\n",
    "        \"Account#\":    \"internal_account\",\n",
    "        \"PostingDate\": \"posting_date\",\n",
    "    })\n",
    "    print(f\"Mock response:\\n{mapping_raw}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b18de141-59bd-4206-9088-b0f034ab2820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Step 4 — Validate the AI mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81b801ad-252b-43cc-aec2-a17665e8def7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parse the LLM response\n",
    "ai_mapping = json.loads(mapping_raw)\n",
    "\n",
    "print(\"AI-proposed mapping:\")\n",
    "for client_col, canonical_col in ai_mapping.items():\n",
    "    print(f\"  {client_col:<20} → {canonical_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cca080f9-ad21-43be-9d55-48d86327f5ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare AI mapping against the known correct mapping for Bank Plus\n",
    "total = len(BANK_PLUS_CORRECT_MAPPING)\n",
    "correct = 0\n",
    "results = []\n",
    "\n",
    "for client_col, expected_canon in BANK_PLUS_CORRECT_MAPPING.items():\n",
    "    ai_canon = ai_mapping.get(client_col)\n",
    "    match = (ai_canon == expected_canon)\n",
    "    if match:\n",
    "        correct += 1\n",
    "    results.append({\n",
    "        \"client_column\": client_col,\n",
    "        \"expected\": expected_canon,\n",
    "        \"ai_proposed\": ai_canon,\n",
    "        \"match\": \"Y\" if match else \"X\",\n",
    "    })\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"COLUMN MAPPING VALIDATION — {correct}/{total} correct ({accuracy:.0f}%)\")\n",
    "print(\"=\" * 70)\n",
    "for r in results:\n",
    "    status = r['match']\n",
    "    print(f\"  [{status}] {r['client_column']:<15} → AI: {str(r['ai_proposed']):<20} | Expected: {r['expected']}\")\n",
    "\n",
    "if accuracy == 100:\n",
    "    print(\"\\nAll columns mapped correctly to generic canonical names.\")\n",
    "else:\n",
    "    print(f\"\\n{total - correct} column(s) mapped incorrectly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49da03fd-7bd5-4615-8fba-e4fdde9f0915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Step 5 — Save mapping to Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf5a8bb-cc19-48e5-ac07-17ec3f85dfea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Build a DataFrame with the mapping result\n",
    "mapping_records = []\n",
    "for client_col, canonical_col in ai_mapping.items():\n",
    "    mapping_records.append({\n",
    "        \"client_name\":    \"simulated_bank_plus\",\n",
    "        \"client_column\":  client_col,\n",
    "        \"canonical_column\": canonical_col,\n",
    "        \"model_name\":     MODEL_NAME,\n",
    "        \"run_timestamp\":  datetime.utcnow().isoformat(),\n",
    "    })\n",
    "\n",
    "df_mapping = pd.DataFrame(mapping_records)\n",
    "\n",
    "try:\n",
    "    sdf_mapping = spark.createDataFrame(df_mapping)\n",
    "    sdf_mapping.write.mode(\"overwrite\").saveAsTable(MAPPINGS_TABLE)\n",
    "    print(f\"Saved {len(df_mapping)} column mappings to {MAPPINGS_TABLE}\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC write (run in Databricks).\")\n",
    "    print(f\"DataFrame ready with {len(df_mapping)} rows:\")\n",
    "    print(df_mapping.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de84b323-90e8-4303-88e4-404939eea2fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Step 6 — Apply mapping (demonstration)\n",
    "\n",
    "Show how the mapping renames client-specific columns to the generic\n",
    "canonical names. After this step, every client's data looks the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d9a57e3-c335-41f4-a997-5b6f20245a86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply the AI mapping to rename client columns → generic canonical names\n",
    "rename_map = {client: canon for client, canon in ai_mapping.items() if canon is not None}\n",
    "df_canonical = df_client.rename(columns=rename_map)\n",
    "\n",
    "print(\"Columns after applying AI mapping:\")\n",
    "print(f\"  {list(df_canonical.columns)}\")\n",
    "\n",
    "# Verify all canonical columns needed by the categorization pipeline are present\n",
    "required_cols = [\"transaction_code\", \"description_1\", \"amount\", \"transaction_date\", \"account_number\"]\n",
    "missing = [c for c in required_cols if c not in df_canonical.columns]\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nWARNING: Missing required columns after mapping: {missing}\")\n",
    "else:\n",
    "    print(\"\\nAll required pipeline columns present — ready for categorization.\")\n",
    "\n",
    "df_canonical.head(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_map_client_schema",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
