{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 — Map Client Schema\n",
    "\n",
    "**Objective:** Prototype an AI-based column mapping tool that can take a new client's\n",
    "transaction file — with arbitrary column names — and automatically map each column\n",
    "to the canonical schema used by the categorization pipeline.\n",
    "\n",
    "### Why this matters\n",
    "\n",
    "Every FI's core banking system exports transaction data with different column names.\n",
    "Bank Plus (Jack Henry SilverLake) uses `TRANCD`, `EFHDS1`, `EFHDS2`, etc. Another\n",
    "FI on FIS or Fiserv might call them `TxnCode`, `Description`, `Memo`. Before the\n",
    "categorization pipeline can run, we need a mapping from the client's columns to our\n",
    "canonical schema.\n",
    "\n",
    "### Approach\n",
    "\n",
    "1. Take Bank Plus NON_POS data and **rename columns** to simulate an unknown client.\n",
    "2. Use `ai_query()` to ask the LLM to propose a column mapping.\n",
    "3. Validate against the known correct mapping.\n",
    "4. Save the mapping to Unity Catalog for downstream use.\n",
    "\n",
    "**Runs on:** Databricks Runtime 15.4 LTS or above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Configuration ─────────────────────────────────────────────────\n",
    "CATALOG_NAME = \"ciq-bp_dummy-dev\"\n",
    "SCHEMA_NAME  = \"default\"\n",
    "MODEL_NAME   = \"databricks-claude-opus-4-6\"\n",
    "\n",
    "RAW_NON_POS_PATH = \"../data/bank-plus-data/raw/CheckingIQ_NON_POS_Daily_012626_rerun.csv\"\n",
    "\n",
    "MAPPINGS_TABLE = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.client_column_mappings\"\n",
    "\n",
    "# Canonical schema — the column names our pipeline expects\n",
    "CANONICAL_SCHEMA = {\n",
    "    \"ACCTNO\":      \"Masked account number\",\n",
    "    \"status\":       \"Account status code\",\n",
    "    \"TRANCD\":       \"Transaction code (numeric, primary key for categorization)\",\n",
    "    \"description\":  \"Transaction description (may be empty)\",\n",
    "    \"TRDATE\":       \"Transaction date\",\n",
    "    \"AMT\":          \"Transaction amount\",\n",
    "    \"EFHDS1\":       \"Extended description line 1 (primary text for categorization)\",\n",
    "    \"EFHDS2\":       \"Extended description line 2 (secondary text)\",\n",
    "    \"Account#\":     \"Internal account number\",\n",
    "    \"PostingDate\":  \"Posting date\",\n",
    "}\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Output table: {MAPPINGS_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1 — Load sample data and simulate a new client\n",
    "\n",
    "We take the first 20 rows of Bank Plus NON_POS data and rename columns to simulate\n",
    "what a different FI's export might look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df_original = pd.read_csv(RAW_NON_POS_PATH, nrows=20, dtype=str)\n",
    "df_original.columns = [c.strip() for c in df_original.columns]\n",
    "\n",
    "print(f\"Original columns: {list(df_original.columns)}\")\n",
    "print(f\"Sample rows: {len(df_original)}\")\n",
    "df_original.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a new client by renaming columns to plausible alternatives\n",
    "SIMULATED_RENAME = {\n",
    "    \"ACCTNO\":      \"Account_Num\",\n",
    "    \"status\":      \"Acct_Status\",\n",
    "    \"TRANCD\":      \"Trans_Code\",\n",
    "    \"description\": \"Trans_Desc\",\n",
    "    \"TRDATE\":      \"Transaction_Date\",\n",
    "    \"AMT\":         \"Amount\",\n",
    "    \"EFHDS1\":      \"Description_1\",\n",
    "    \"EFHDS2\":      \"Memo_Line\",\n",
    "    \"Account#\":    \"Internal_Acct\",\n",
    "    \"PostingDate\": \"Post_Date\",\n",
    "}\n",
    "\n",
    "# The reverse mapping is the ground truth for validation\n",
    "CORRECT_MAPPING = {v: k for k, v in SIMULATED_RENAME.items()}\n",
    "\n",
    "df_simulated = df_original.rename(columns=SIMULATED_RENAME)\n",
    "\n",
    "print(f\"Simulated client columns: {list(df_simulated.columns)}\")\n",
    "print(f\"\\nCorrect mapping (ground truth for validation):\")\n",
    "for sim, canon in CORRECT_MAPPING.items():\n",
    "    print(f\"  {sim:<20} → {canon}\")\n",
    "\n",
    "df_simulated.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2 — Build the AI mapping prompt\n",
    "\n",
    "We send the LLM the simulated column names, a few sample rows, and the canonical\n",
    "schema definition. The LLM must return a JSON mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format sample data as a compact table for the prompt\n",
    "sample_rows = df_simulated.head(5).to_csv(index=False)\n",
    "\n",
    "canonical_schema_text = \"\\n\".join(\n",
    "    f\"  - {col}: {desc}\" for col, desc in CANONICAL_SCHEMA.items()\n",
    ")\n",
    "\n",
    "mapping_prompt = f\"\"\"You are a data engineering assistant for a banking analytics platform.\n",
    "\n",
    "A new financial institution has provided a transaction data file. Your task is to map\n",
    "each column in their file to the corresponding column in our canonical schema.\n",
    "\n",
    "### Canonical Schema (target columns)\n",
    "{canonical_schema_text}\n",
    "\n",
    "### Client's Column Names\n",
    "{list(df_simulated.columns)}\n",
    "\n",
    "### Sample Data (first 5 rows)\n",
    "{sample_rows}\n",
    "\n",
    "### Instructions\n",
    "1. Examine each client column name and its sample values.\n",
    "2. Map each client column to the most appropriate canonical column.\n",
    "3. If a client column has no match in the canonical schema, map it to null.\n",
    "4. Every canonical column should be mapped to at most one client column.\n",
    "5. Return ONLY a JSON object where keys are the client's column names and values\n",
    "   are the canonical column names (or null if no match).\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Prompt length: {len(mapping_prompt)} chars (~{len(mapping_prompt)//4} tokens)\")\n",
    "print(\"\\n--- Prompt preview (first 500 chars) ---\")\n",
    "print(mapping_prompt[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3 — Call `ai_query()` for column mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "escaped_prompt = mapping_prompt.replace(\"'\", \"''\")\n",
    "\n",
    "# Build the canonical schema as a responseFormat JSON schema\n",
    "client_cols = list(df_simulated.columns)\n",
    "properties = {col: {\"type\": [\"string\", \"null\"]} for col in client_cols}\n",
    "\n",
    "response_schema = json.dumps({\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"column_mapping\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": properties,\n",
    "        },\n",
    "        \"strict\": True,\n",
    "    }\n",
    "})\n",
    "\n",
    "mapping_query = f\"\"\"\n",
    "SELECT ai_query(\n",
    "    '{MODEL_NAME}',\n",
    "    '{escaped_prompt}',\n",
    "    responseFormat => '{response_schema}'\n",
    ") as mapping_result\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    print(\"Executing ai_query for column mapping...\")\n",
    "    result_df = spark.sql(mapping_query)\n",
    "    mapping_raw = result_df.collect()[0][\"mapping_result\"]\n",
    "    print(f\"\\nRaw LLM response:\\n{mapping_raw}\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — using mock response for local testing.\")\n",
    "    mapping_raw = json.dumps({\n",
    "        \"Account_Num\":       \"ACCTNO\",\n",
    "        \"Acct_Status\":       \"status\",\n",
    "        \"Trans_Code\":        \"TRANCD\",\n",
    "        \"Trans_Desc\":        \"description\",\n",
    "        \"Transaction_Date\":  \"TRDATE\",\n",
    "        \"Amount\":            \"AMT\",\n",
    "        \"Description_1\":     \"EFHDS1\",\n",
    "        \"Memo_Line\":         \"EFHDS2\",\n",
    "        \"Internal_Acct\":     \"Account#\",\n",
    "        \"Post_Date\":         \"PostingDate\",\n",
    "    })\n",
    "    print(f\"Mock response:\\n{mapping_raw}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4 — Validate the AI mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the LLM response\n",
    "ai_mapping = json.loads(mapping_raw)\n",
    "\n",
    "print(\"AI-proposed mapping:\")\n",
    "for client_col, canonical_col in ai_mapping.items():\n",
    "    print(f\"  {client_col:<20} → {canonical_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare AI mapping against the known correct mapping\n",
    "total = len(CORRECT_MAPPING)\n",
    "correct = 0\n",
    "results = []\n",
    "\n",
    "for client_col, expected_canon in CORRECT_MAPPING.items():\n",
    "    ai_canon = ai_mapping.get(client_col)\n",
    "    match = (ai_canon == expected_canon)\n",
    "    if match:\n",
    "        correct += 1\n",
    "    results.append({\n",
    "        \"client_column\": client_col,\n",
    "        \"expected\": expected_canon,\n",
    "        \"ai_proposed\": ai_canon,\n",
    "        \"match\": \"Y\" if match else \"X\",\n",
    "    })\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(f\"COLUMN MAPPING VALIDATION — {correct}/{total} correct ({accuracy:.0f}%)\")\n",
    "print(\"=\" * 65)\n",
    "for r in results:\n",
    "    status = r['match']\n",
    "    print(f\"  [{status}] {r['client_column']:<20} → AI: {str(r['ai_proposed']):<15} | Expected: {r['expected']}\")\n",
    "\n",
    "if accuracy == 100:\n",
    "    print(\"\\nAll columns mapped correctly.\")\n",
    "else:\n",
    "    print(f\"\\n{total - correct} column(s) mapped incorrectly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5 — Save mapping to Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Build a DataFrame with the mapping result\n",
    "mapping_records = []\n",
    "for client_col, canonical_col in ai_mapping.items():\n",
    "    mapping_records.append({\n",
    "        \"client_name\":    \"simulated_bank_plus\",\n",
    "        \"client_column\":  client_col,\n",
    "        \"canonical_column\": canonical_col,\n",
    "        \"model_name\":     MODEL_NAME,\n",
    "        \"run_timestamp\":  datetime.utcnow().isoformat(),\n",
    "    })\n",
    "\n",
    "df_mapping = pd.DataFrame(mapping_records)\n",
    "\n",
    "try:\n",
    "    sdf_mapping = spark.createDataFrame(df_mapping)\n",
    "    sdf_mapping.write.mode(\"overwrite\").saveAsTable(MAPPINGS_TABLE)\n",
    "    print(f\"Saved {len(df_mapping)} column mappings to {MAPPINGS_TABLE}\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC write (run in Databricks).\")\n",
    "    print(f\"DataFrame ready with {len(df_mapping)} rows:\")\n",
    "    print(df_mapping.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6 — Apply mapping (demonstration)\n",
    "\n",
    "Show how the mapping would be applied to rename a client's file\n",
    "into the canonical schema for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the AI mapping to rename columns back to canonical\n",
    "rename_map = {client: canon for client, canon in ai_mapping.items() if canon is not None}\n",
    "df_renamed = df_simulated.rename(columns=rename_map)\n",
    "\n",
    "print(\"Columns after applying AI mapping:\")\n",
    "print(f\"  {list(df_renamed.columns)}\")\n",
    "\n",
    "# Verify all canonical columns needed by the pipeline are present\n",
    "required_cols = [\"TRANCD\", \"EFHDS1\", \"AMT\", \"TRDATE\", \"ACCTNO\"]\n",
    "missing = [c for c in required_cols if c not in df_renamed.columns]\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nWARNING: Missing required columns after mapping: {missing}\")\n",
    "else:\n",
    "    print(\"\\nAll required pipeline columns present — ready for categorization.\")\n",
    "\n",
    "df_renamed.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
