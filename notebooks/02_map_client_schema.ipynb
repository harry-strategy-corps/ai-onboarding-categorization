{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 — Map Client Schema\n",
        "\n",
        "**Objective:** Prototype an AI-based column mapping tool that can take a new client's\n",
        "transaction file — with arbitrary column names — and automatically map each column\n",
        "to a **generic, standardized schema** that is the same for every client.\n",
        "\n",
        "### Why this matters\n",
        "\n",
        "Every FI's core banking system exports transaction data with different column names.\n",
        "Bank Plus (Jack Henry SilverLake) uses `TRANCD`, `EFHDS1`, `EFHDS2`, etc. Another\n",
        "FI on FIS or Fiserv might call them `TxnCode`, `Description`, `Memo`. Before the\n",
        "categorization pipeline can run, we need all client data normalized to a single\n",
        "universal format.\n",
        "\n",
        "### Canonical output columns\n",
        "\n",
        "| Canonical Column | Description |\n",
        "|-----------------|-------------|\n",
        "| `transaction_code` | Numeric transaction code (primary key for categorization) |\n",
        "| `description_1` | Primary transaction description text |\n",
        "| `description_2` | Secondary description / memo line |\n",
        "| `amount` | Transaction amount |\n",
        "| `transaction_date` | Date the transaction occurred |\n",
        "| `posting_date` | Date the transaction was posted |\n",
        "| `account_number` | Account identifier |\n",
        "| `account_status` | Account status code |\n",
        "| `internal_account` | Internal/alternate account number |\n",
        "| `transaction_desc` | Transaction type description (may be empty) |\n",
        "\n",
        "### Approach\n",
        "\n",
        "1. Take Bank Plus NON_POS data (which has client-specific column names like `TRANCD`, `EFHDS1`).\n",
        "2. Use `ai_query()` to ask the LLM to map each client column to the canonical schema.\n",
        "3. Validate against the known correct mapping.\n",
        "4. Apply the mapping to produce a standardized DataFrame.\n",
        "5. Save the mapping to Unity Catalog for downstream use.\n",
        "\n",
        "**Runs on:** Databricks Runtime 15.4 LTS or above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Configuration ─────────────────────────────────────────────────\n",
        "CATALOG_NAME = \"ciq-bp_dummy-dev\"\n",
        "SCHEMA_NAME  = \"default\"\n",
        "MODEL_NAME   = \"databricks-claude-opus-4-6\"\n",
        "\n",
        "RAW_NON_POS_PATH = \"../data/bank-plus-data/raw/CheckingIQ_NON_POS_Daily_012626_rerun.csv\"\n",
        "\n",
        "MAPPINGS_TABLE = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.client_column_mappings\"\n",
        "\n",
        "# Generic canonical schema — universal column names for ALL clients.\n",
        "# Every client's data gets mapped to these same column names.\n",
        "CANONICAL_SCHEMA = {\n",
        "    \"transaction_code\":  \"Numeric transaction code (primary key for categorization)\",\n",
        "    \"description_1\":     \"Primary transaction description text\",\n",
        "    \"description_2\":     \"Secondary description / memo line\",\n",
        "    \"amount\":            \"Transaction amount\",\n",
        "    \"transaction_date\":  \"Date the transaction occurred\",\n",
        "    \"posting_date\":      \"Date the transaction was posted\",\n",
        "    \"account_number\":    \"Account identifier\",\n",
        "    \"account_status\":    \"Account status code\",\n",
        "    \"internal_account\":  \"Internal/alternate account number\",\n",
        "    \"transaction_desc\":  \"Transaction type description (may be empty)\",\n",
        "}\n",
        "\n",
        "# For Bank Plus, we know the correct mapping from their columns to canonical.\n",
        "# This is the ground truth used to validate the AI output.\n",
        "BANK_PLUS_CORRECT_MAPPING = {\n",
        "    \"TRANCD\":      \"transaction_code\",\n",
        "    \"EFHDS1\":      \"description_1\",\n",
        "    \"EFHDS2\":      \"description_2\",\n",
        "    \"AMT\":         \"amount\",\n",
        "    \"TRDATE\":      \"transaction_date\",\n",
        "    \"PostingDate\":  \"posting_date\",\n",
        "    \"ACCTNO\":      \"account_number\",\n",
        "    \"status\":      \"account_status\",\n",
        "    \"Account#\":    \"internal_account\",\n",
        "    \"description\": \"transaction_desc\",\n",
        "}\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Output table: {MAPPINGS_TABLE}\")\n",
        "print(f\"Canonical columns: {list(CANONICAL_SCHEMA.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 1 — Load client data sample\n",
        "\n",
        "We take the first 20 rows of Bank Plus NON_POS data. These columns (`TRANCD`,\n",
        "`EFHDS1`, `EFHDS2`, etc.) are client-specific — the AI must map them to our\n",
        "generic canonical column names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "df_original = pd.read_csv(RAW_NON_POS_PATH, nrows=20, dtype=str)\n",
        "df_original.columns = [c.strip() for c in df_original.columns]\n",
        "\n",
        "print(f\"Original columns: {list(df_original.columns)}\")\n",
        "print(f\"Sample rows: {len(df_original)}\")\n",
        "df_original.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bank Plus columns are already \"client-specific\" — no simulation needed.\n",
        "# The AI must map these raw column names to the generic canonical schema.\n",
        "df_client = df_original.copy()\n",
        "\n",
        "print(f\"Client columns (Bank Plus / Jack Henry SilverLake):\")\n",
        "print(f\"  {list(df_client.columns)}\")\n",
        "print(f\"\\nKnown correct mapping (ground truth for validation):\")\n",
        "for client_col, canon_col in BANK_PLUS_CORRECT_MAPPING.items():\n",
        "    print(f\"  {client_col:<15} → {canon_col}\")\n",
        "\n",
        "df_client.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 2 — Build the AI mapping prompt\n",
        "\n",
        "We send the LLM the client's actual column names, a few sample rows, and the\n",
        "generic canonical schema definition. The LLM must return a JSON object mapping\n",
        "each client column to a canonical column (or null if no match)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_rows = df_client.head(5).to_csv(index=False)\n",
        "\n",
        "canonical_schema_text = \"\\n\".join(\n",
        "    f\"  - {col}: {desc}\" for col, desc in CANONICAL_SCHEMA.items()\n",
        ")\n",
        "\n",
        "mapping_prompt = f\"\"\"You are a data engineering assistant for a banking analytics platform.\n",
        "\n",
        "A new financial institution has provided a transaction data file. Your task is to map\n",
        "each column in their file to the corresponding column in our standardized canonical\n",
        "schema. The canonical schema uses generic, descriptive names — NOT the client's\n",
        "original column names.\n",
        "\n",
        "### Canonical Schema (target columns — these are the OUTPUT names)\n",
        "{canonical_schema_text}\n",
        "\n",
        "### Client's Column Names (these are the INPUT names to map FROM)\n",
        "{list(df_client.columns)}\n",
        "\n",
        "### Sample Data (first 5 rows)\n",
        "{sample_rows}\n",
        "\n",
        "### Instructions\n",
        "1. Examine each client column name and its sample values.\n",
        "2. Map each client column to the most appropriate canonical column.\n",
        "3. If a client column has no match in the canonical schema, map it to null.\n",
        "4. Every canonical column should be mapped to at most one client column.\n",
        "5. Return ONLY a JSON object where keys are the client's column names and values\n",
        "   are the canonical column names (or null if no match).\n",
        "\n",
        "IMPORTANT: The values in your JSON must be the canonical column names (e.g.\n",
        "\"transaction_code\", \"description_1\", \"amount\"), NOT the client's original names.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Prompt length: {len(mapping_prompt)} chars (~{len(mapping_prompt)//4} tokens)\")\n",
        "print(\"\\n--- Prompt preview (first 500 chars) ---\")\n",
        "print(mapping_prompt[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 3 — Call `ai_query()` for column mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "escaped_prompt = mapping_prompt.replace(\"'\", \"''\")\n",
        "\n",
        "# Build responseFormat: keys = client's column names, values = canonical names or null\n",
        "client_cols = list(df_client.columns)\n",
        "properties = {col: {\"type\": [\"string\", \"null\"]} for col in client_cols}\n",
        "\n",
        "response_schema = json.dumps({\n",
        "    \"type\": \"json_schema\",\n",
        "    \"json_schema\": {\n",
        "        \"name\": \"column_mapping\",\n",
        "        \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": properties,\n",
        "        },\n",
        "        \"strict\": True,\n",
        "    }\n",
        "})\n",
        "\n",
        "mapping_query = f\"\"\"\n",
        "SELECT ai_query(\n",
        "    '{MODEL_NAME}',\n",
        "    '{escaped_prompt}',\n",
        "    responseFormat => '{response_schema}'\n",
        ") as mapping_result\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    print(\"Executing ai_query for column mapping...\")\n",
        "    result_df = spark.sql(mapping_query)\n",
        "    mapping_raw = result_df.collect()[0][\"mapping_result\"]\n",
        "    print(f\"\\nRaw LLM response:\\n{mapping_raw}\")\n",
        "except NameError:\n",
        "    print(\"Spark session not found — using mock response for local testing.\")\n",
        "    # Mock: client columns → generic canonical names\n",
        "    mapping_raw = json.dumps({\n",
        "        \"ACCTNO\":      \"account_number\",\n",
        "        \"status\":      \"account_status\",\n",
        "        \"TRANCD\":      \"transaction_code\",\n",
        "        \"description\": \"transaction_desc\",\n",
        "        \"TRDATE\":      \"transaction_date\",\n",
        "        \"AMT\":         \"amount\",\n",
        "        \"EFHDS1\":      \"description_1\",\n",
        "        \"EFHDS2\":      \"description_2\",\n",
        "        \"Account#\":    \"internal_account\",\n",
        "        \"PostingDate\": \"posting_date\",\n",
        "    })\n",
        "    print(f\"Mock response:\\n{mapping_raw}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 4 — Validate the AI mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse the LLM response\n",
        "ai_mapping = json.loads(mapping_raw)\n",
        "\n",
        "print(\"AI-proposed mapping:\")\n",
        "for client_col, canonical_col in ai_mapping.items():\n",
        "    print(f\"  {client_col:<20} → {canonical_col}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare AI mapping against the known correct mapping for Bank Plus\n",
        "total = len(BANK_PLUS_CORRECT_MAPPING)\n",
        "correct = 0\n",
        "results = []\n",
        "\n",
        "for client_col, expected_canon in BANK_PLUS_CORRECT_MAPPING.items():\n",
        "    ai_canon = ai_mapping.get(client_col)\n",
        "    match = (ai_canon == expected_canon)\n",
        "    if match:\n",
        "        correct += 1\n",
        "    results.append({\n",
        "        \"client_column\": client_col,\n",
        "        \"expected\": expected_canon,\n",
        "        \"ai_proposed\": ai_canon,\n",
        "        \"match\": \"Y\" if match else \"X\",\n",
        "    })\n",
        "\n",
        "accuracy = correct / total * 100\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"COLUMN MAPPING VALIDATION — {correct}/{total} correct ({accuracy:.0f}%)\")\n",
        "print(\"=\" * 70)\n",
        "for r in results:\n",
        "    status = r['match']\n",
        "    print(f\"  [{status}] {r['client_column']:<15} → AI: {str(r['ai_proposed']):<20} | Expected: {r['expected']}\")\n",
        "\n",
        "if accuracy == 100:\n",
        "    print(\"\\nAll columns mapped correctly to generic canonical names.\")\n",
        "else:\n",
        "    print(f\"\\n{total - correct} column(s) mapped incorrectly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 5 — Save mapping to Unity Catalog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Build a DataFrame with the mapping result\n",
        "mapping_records = []\n",
        "for client_col, canonical_col in ai_mapping.items():\n",
        "    mapping_records.append({\n",
        "        \"client_name\":    \"simulated_bank_plus\",\n",
        "        \"client_column\":  client_col,\n",
        "        \"canonical_column\": canonical_col,\n",
        "        \"model_name\":     MODEL_NAME,\n",
        "        \"run_timestamp\":  datetime.utcnow().isoformat(),\n",
        "    })\n",
        "\n",
        "df_mapping = pd.DataFrame(mapping_records)\n",
        "\n",
        "try:\n",
        "    sdf_mapping = spark.createDataFrame(df_mapping)\n",
        "    sdf_mapping.write.mode(\"overwrite\").saveAsTable(MAPPINGS_TABLE)\n",
        "    print(f\"Saved {len(df_mapping)} column mappings to {MAPPINGS_TABLE}\")\n",
        "except NameError:\n",
        "    print(\"Spark session not found — skipping UC write (run in Databricks).\")\n",
        "    print(f\"DataFrame ready with {len(df_mapping)} rows:\")\n",
        "    print(df_mapping.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 6 — Apply mapping (demonstration)\n",
        "\n",
        "Show how the mapping renames client-specific columns to the generic\n",
        "canonical names. After this step, every client's data looks the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply the AI mapping to rename client columns → generic canonical names\n",
        "rename_map = {client: canon for client, canon in ai_mapping.items() if canon is not None}\n",
        "df_canonical = df_client.rename(columns=rename_map)\n",
        "\n",
        "print(\"Columns after applying AI mapping:\")\n",
        "print(f\"  {list(df_canonical.columns)}\")\n",
        "\n",
        "# Verify all canonical columns needed by the categorization pipeline are present\n",
        "required_cols = [\"transaction_code\", \"description_1\", \"amount\", \"transaction_date\", \"account_number\"]\n",
        "missing = [c for c in required_cols if c not in df_canonical.columns]\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\nWARNING: Missing required columns after mapping: {missing}\")\n",
        "else:\n",
        "    print(\"\\nAll required pipeline columns present — ready for categorization.\")\n",
        "\n",
        "df_canonical.head(5)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
