{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 — Evaluate Transaction Categorization Accuracy\n",
    "\n",
    "**Objective:** Evaluate LLM classification results against the Master Fee Table ground truth,\n",
    "compute accuracy metrics, analyze failures, and support cross-version comparison.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| Per-Level Accuracy | L1, L2, L3, L4 independently (case-insensitive, null-safe) |\n",
    "| Exact Match | All 4 levels correct |\n",
    "| Partial Match | L1+L2 correct, L1+L2+L3 correct |\n",
    "| Volume-Weighted | Weighted by raw transaction count |\n",
    "| Per-Layer | Obvious (Layer 1) vs Ambiguous (Layer 2) vs Unknown (Layer 3) |\n",
    "| Failure Analysis | Categorize mismatches by root cause |\n",
    "| Cost Summary | Total tokens and estimated cost for the run |\n",
    "| Cross-Version | Compare accuracy across prompt versions |\n",
    "\n",
    "### Layer Evaluation Rules\n",
    "\n",
    "- **Layer 1 (Obvious):** Standard per-level comparison against single GT mapping.\n",
    "- **Layer 2 (Ambiguous):** Match against ANY valid GT mapping for that transaction_code.\n",
    "- **Layer 3 (Unknown):** Flagged as NEEDS MANUAL REVIEW — no accuracy calculation.\n",
    "\n",
    "**Runs on:** Databricks Runtime 15.4 LTS or above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Configuration ─────────────────────────────────────────────────\n",
    "CATALOG_NAME    = \"ciq-bp_dummy-dev\"\n",
    "SCHEMA_NAME     = \"default\"\n",
    "PROMPT_VERSION  = \"v1.0\"  # Set to None to evaluate the latest run\n",
    "\n",
    "RESULTS_TABLE = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.classification_results\"\n",
    "GT_TABLE      = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.ground_truth_normalized\"\n",
    "EVAL_TABLE    = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.evaluation_results\"\n",
    "\n",
    "# Target: >= 80% volume-weighted exact match accuracy\n",
    "TARGET_ACCURACY = 0.80\n",
    "\n",
    "print(f\"Results table:  {RESULTS_TABLE}\")\n",
    "print(f\"GT table:       {GT_TABLE}\")\n",
    "print(f\"Eval table:     {EVAL_TABLE}\")\n",
    "print(f\"Prompt version: {PROMPT_VERSION or '(latest)'}\")\n",
    "print(f\"Target:         >= {TARGET_ACCURACY:.0%} vol-weighted exact match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1 — Validate upstream tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    for table, label in [(RESULTS_TABLE, \"classification_results\"), (GT_TABLE, \"ground_truth_normalized\")]:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {table}\").collect()[0][\"cnt\"]\n",
    "        print(f\"  OK  {label}: {count} rows\")\n",
    "    print(\"\\nUpstream tables validated.\")\n",
    "except NameError:\n",
    "    raise SystemExit(\"Spark session not found — run this notebook in Databricks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2 — Load results and ground truth from Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load classification results ──────────────────────────────────\n",
    "if PROMPT_VERSION:\n",
    "    df_results = (\n",
    "        spark.sql(f\"SELECT * FROM {RESULTS_TABLE} WHERE prompt_version = '{PROMPT_VERSION}'\")\n",
    "        .toPandas()\n",
    "    )\n",
    "    print(f\"Loaded results for prompt_version='{PROMPT_VERSION}': {len(df_results)} rows\")\n",
    "else:\n",
    "    latest_version = (\n",
    "        spark.sql(f\"SELECT prompt_version FROM {RESULTS_TABLE} ORDER BY run_timestamp DESC LIMIT 1\")\n",
    "        .collect()[0][\"prompt_version\"]\n",
    "    )\n",
    "    df_results = (\n",
    "        spark.sql(f\"SELECT * FROM {RESULTS_TABLE} WHERE prompt_version = '{latest_version}'\")\n",
    "        .toPandas()\n",
    "    )\n",
    "    print(f\"Loaded latest results (prompt_version='{latest_version}'): {len(df_results)} rows\")\n",
    "\n",
    "df_results[\"transaction_code\"] = df_results[\"transaction_code\"].astype(str)\n",
    "\n",
    "# ── Load ground truth ─────────────────────────────────────────────\n",
    "df_gt = spark.table(GT_TABLE).toPandas()\n",
    "df_gt[\"transaction_code\"] = df_gt[\"transaction_code\"].astype(str)\n",
    "\n",
    "print(f\"Ground truth: {len(df_gt)} rows, {df_gt['transaction_code'].nunique()} unique codes\")\n",
    "print(f\"\\nResults columns: {list(df_results.columns)}\")\n",
    "print(f\"GT columns: {list(df_gt.columns)}\")\n",
    "print(f\"\\nLayer distribution in results:\")\n",
    "for layer in sorted(df_results[\"layer\"].unique()):\n",
    "    n = len(df_results[df_results[\"layer\"] == layer])\n",
    "    print(f\"  Layer {layer}: {n} codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3 — Comparison helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_NULL = \"__null__\"\n",
    "\n",
    "def _canon(val):\n",
    "    \"\"\"Canonicalize a value for comparison: lowercase, strip, null-safe.\"\"\"\n",
    "    if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "        return _NULL\n",
    "    s = str(val).strip().lower()\n",
    "    if s in (\"\", \"none\", \"nan\", \"null\", \"n/a\"):\n",
    "        return _NULL\n",
    "    return s\n",
    "\n",
    "\n",
    "def levels_match(row, llm_col, gt_col):\n",
    "    return _canon(row[llm_col]) == _canon(row[gt_col])\n",
    "\n",
    "\n",
    "def add_match_columns(df):\n",
    "    \"\"\"Add per-level and aggregate match booleans.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"match_L1\"] = df.apply(levels_match, axis=1, llm_col=\"category_1\", gt_col=\"gt_L1\")\n",
    "    df[\"match_L2\"] = df.apply(levels_match, axis=1, llm_col=\"category_2\", gt_col=\"gt_L2\")\n",
    "    df[\"match_L3\"] = df.apply(levels_match, axis=1, llm_col=\"category_3\", gt_col=\"gt_L3\")\n",
    "    df[\"match_L4\"] = df.apply(levels_match, axis=1, llm_col=\"category_4\", gt_col=\"gt_L4\")\n",
    "\n",
    "    df[\"exact_match\"]          = df[[\"match_L1\", \"match_L2\", \"match_L3\", \"match_L4\"]].all(axis=1)\n",
    "    df[\"partial_match_L1L2\"]   = df[[\"match_L1\", \"match_L2\"]].all(axis=1)\n",
    "    df[\"partial_match_L1L2L3\"] = df[[\"match_L1\", \"match_L2\", \"match_L3\"]].all(axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "LAYER_NAMES = {1: \"Obvious\", 2: \"Ambiguous\", 3: \"Unknown\"}\n",
    "\n",
    "print(\"Comparison helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4 — Prepare GT lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify single-mapping vs multi-mapping codes in the ground truth\n",
    "gt_mapping_counts = (\n",
    "    df_gt\n",
    "    .groupby(\"transaction_code\")\n",
    "    .apply(lambda g: g[[\"gt_L1\", \"gt_L2\", \"gt_L3\", \"gt_L4\"]].drop_duplicates().shape[0])\n",
    "    .reset_index(name=\"n_mappings\")\n",
    ")\n",
    "\n",
    "multi_codes  = set(gt_mapping_counts.loc[gt_mapping_counts[\"n_mappings\"] > 1, \"transaction_code\"])\n",
    "single_codes = set(gt_mapping_counts.loc[gt_mapping_counts[\"n_mappings\"] == 1, \"transaction_code\"])\n",
    "all_gt_codes = set(df_gt[\"transaction_code\"].unique())\n",
    "\n",
    "# GT lookup for single-mapping codes (one row per code)\n",
    "gt_cols = [\"transaction_code\", \"gt_desc\", \"gt_L1\", \"gt_L2\", \"gt_L3\", \"gt_L4\", \"gt_credit_debit\"]\n",
    "df_gt_single = (\n",
    "    df_gt[df_gt[\"transaction_code\"].isin(single_codes)]\n",
    "    .drop_duplicates(subset=\"transaction_code\", keep=\"first\")[gt_cols]\n",
    ")\n",
    "\n",
    "print(f\"GT codes with 1 mapping (Layer 1):  {len(single_codes)}\")\n",
    "print(f\"GT codes with 2+ mappings (Layer 2): {len(multi_codes)}\")\n",
    "print(f\"Multi-mapping codes: {sorted(multi_codes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5 — Layer 1 evaluation (Obvious codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l1 = pd.merge(\n",
    "    df_results[df_results[\"layer\"] == 1],\n",
    "    df_gt_single,\n",
    "    on=\"transaction_code\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "df_l1 = add_match_columns(df_l1)\n",
    "n_l1 = len(df_l1)\n",
    "vol_l1 = df_l1[\"volume\"].sum()\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(f\"LAYER 1 — OBVIOUS CODES  (n = {n_l1}, volume = {vol_l1:,})\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"  L1 (Fee vs Non-fee):      {df_l1['match_L1'].mean():.1%}\")\n",
    "print(f\"  L2 (Category):            {df_l1['match_L2'].mean():.1%}\")\n",
    "print(f\"  L3 (Channel):             {df_l1['match_L3'].mean():.1%}\")\n",
    "print(f\"  L4 (Subtype):             {df_l1['match_L4'].mean():.1%}\")\n",
    "print(f\"  ─────────────────────────────\")\n",
    "print(f\"  Partial (L1+L2):          {df_l1['partial_match_L1L2'].mean():.1%}\")\n",
    "print(f\"  Partial (L1+L2+L3):       {df_l1['partial_match_L1L2L3'].mean():.1%}\")\n",
    "print(f\"  Exact Match (all 4):      {df_l1['exact_match'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Volume-weighted accuracy ──────────────────────────────────────\n",
    "print(\"=\" * 65)\n",
    "print(f\"VOLUME-WEIGHTED ACCURACY  (total: {vol_l1:,} transactions)\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "for col, label in [\n",
    "    (\"match_L1\",              \"L1 (Fee vs Non-fee)\"),\n",
    "    (\"match_L2\",              \"L2 (Category)\"),\n",
    "    (\"match_L3\",              \"L3 (Channel)\"),\n",
    "    (\"match_L4\",              \"L4 (Subtype)\"),\n",
    "    (\"partial_match_L1L2\",    \"Partial (L1+L2)\"),\n",
    "    (\"partial_match_L1L2L3\",  \"Partial (L1+L2+L3)\"),\n",
    "    (\"exact_match\",           \"Exact (all 4)\"),\n",
    "]:\n",
    "    w = (df_l1[col] * df_l1[\"volume\"]).sum() / vol_l1\n",
    "    print(f\"  {label:<25} {w:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6 — Layer 2 evaluation (Ambiguous codes)\n",
    "\n",
    "A prediction is correct if it matches **any** of the valid GT mappings for that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l2_src = df_results[df_results[\"layer\"] == 2].copy()\n",
    "\n",
    "l2_rows = []\n",
    "for _, row in df_l2_src.iterrows():\n",
    "    code = row[\"transaction_code\"]\n",
    "    gt_maps = (\n",
    "        df_gt[df_gt[\"transaction_code\"] == code][[\"gt_L1\", \"gt_L2\", \"gt_L3\", \"gt_L4\"]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    llm = tuple(_canon(row[c]) for c in [\"category_1\", \"category_2\", \"category_3\", \"category_4\"])\n",
    "\n",
    "    best_levels = 0\n",
    "    matched_any = False\n",
    "    for _, g in gt_maps.iterrows():\n",
    "        gt = tuple(_canon(g[c]) for c in [\"gt_L1\", \"gt_L2\", \"gt_L3\", \"gt_L4\"])\n",
    "        n_match = sum(a == b for a, b in zip(llm, gt))\n",
    "        best_levels = max(best_levels, n_match)\n",
    "        if llm == gt:\n",
    "            matched_any = True\n",
    "\n",
    "    l2_rows.append({\n",
    "        \"transaction_code\": code,\n",
    "        \"description_1\":    row[\"description_1\"],\n",
    "        \"volume\":           row[\"volume\"],\n",
    "        \"llm_path\":         f\"{row['category_1']} > {row['category_2']} > {row['category_3']}\",\n",
    "        \"exact_match_any\":  matched_any,\n",
    "        \"best_levels_matched\": best_levels,\n",
    "        \"n_valid_mappings\": len(gt_maps),\n",
    "        \"confidence\":       row[\"confidence\"],\n",
    "    })\n",
    "\n",
    "df_l2 = pd.DataFrame(l2_rows) if l2_rows else pd.DataFrame()\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(f\"LAYER 2 — AMBIGUOUS CODES  (n = {len(df_l2)})\")\n",
    "print(\"=\" * 65)\n",
    "if len(df_l2) > 0:\n",
    "    print(f\"  Exact match (any valid mapping):  {df_l2['exact_match_any'].mean():.1%}\")\n",
    "    print(f\"  Avg best levels matched:          {df_l2['best_levels_matched'].mean():.1f} / 4\")\n",
    "    print()\n",
    "    print(df_l2.to_string(index=False))\n",
    "else:\n",
    "    print(\"  No ambiguous codes in results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all valid GT mappings for each ambiguous code\n",
    "if len(df_l2) > 0:\n",
    "    print(\"Valid GT mappings for ambiguous codes:\")\n",
    "    for code in sorted(df_l2[\"transaction_code\"].unique()):\n",
    "        maps = (\n",
    "            df_gt[df_gt[\"transaction_code\"] == code][[\"gt_desc\", \"gt_L1\", \"gt_L2\", \"gt_L3\"]]\n",
    "            .drop_duplicates()\n",
    "        )\n",
    "        llm_row = df_l2_src[df_l2_src[\"transaction_code\"] == code].iloc[0]\n",
    "        print(f\"\\n  code={code}  (LLM: {llm_row['category_1']} > {llm_row['category_2']} > {llm_row['category_3']})\")\n",
    "        for _, m in maps.iterrows():\n",
    "            print(f\"    GT: {m['gt_L1']:<15} > {m['gt_L2']:<22} > {str(m['gt_L3']):<25} | {str(m['gt_desc'])[:45]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7 — Layer 3 (Unknown codes — manual review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l3 = df_results[df_results[\"layer\"] == 3].copy()\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(f\"LAYER 3 — UNKNOWN CODES  (n = {len(df_l3)}, no ground truth)\")\n",
    "print(\"=\" * 65)\n",
    "print(\"These codes are absent from the Master Fee Table and need manual review.\\n\")\n",
    "\n",
    "for _, r in df_l3.sort_values(\"volume\", ascending=False).iterrows():\n",
    "    print(f\"  code={str(r['transaction_code']):>5} | vol={r['volume']:>6,} | conf={r['confidence']:.2f}\")\n",
    "    print(f\"    desc: {r['description_1']}\")\n",
    "    print(f\"    LLM:  {r['category_1']} > {r['category_2']} > {r['category_3']} > {r['category_4']}\")\n",
    "    print(f\"    scoring: {r['include_in_scoring']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8 — Failure analysis (Layer 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = df_l1[~df_l1[\"exact_match\"]].copy()\n",
    "\n",
    "def failure_type(row):\n",
    "    if not row[\"match_L1\"]:\n",
    "        return \"WRONG BLOCK (L1)\"\n",
    "    if not row[\"match_L2\"]:\n",
    "        return \"WRONG CATEGORY (L2)\"\n",
    "    if not row[\"match_L3\"]:\n",
    "        return \"WRONG CHANNEL (L3)\"\n",
    "    if not row[\"match_L4\"]:\n",
    "        return \"WRONG SUBTYPE (L4)\"\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "failures[\"failure_type\"] = failures.apply(failure_type, axis=1)\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(f\"FAILURE ANALYSIS — {len(failures)} mismatches out of {len(df_l1)} obvious codes\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "if len(failures) > 0:\n",
    "    ft = (\n",
    "        failures.groupby(\"failure_type\")\n",
    "        .agg(\n",
    "            count=(\"transaction_code\", \"count\"),\n",
    "            volume=(\"volume\", \"sum\"),\n",
    "            examples=(\"transaction_code\", lambda x: \", \".join(x.head(4))),\n",
    "        )\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "    print(\"\\nFailure type distribution:\")\n",
    "    print(ft.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo failures — all Layer 1 codes matched exactly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Detailed mismatch table ───────────────────────────────────────\n",
    "if len(failures) > 0:\n",
    "    print(\"=\" * 65)\n",
    "    print(\"DETAILED FAILURES (sorted by volume, highest impact first)\")\n",
    "    print(\"=\" * 65)\n",
    "\n",
    "    for _, r in failures.sort_values(\"volume\", ascending=False).iterrows():\n",
    "        print(\n",
    "            f\"\\n  code={str(r['transaction_code']):>5} | vol={r['volume']:>6,}\"\n",
    "            f\" | conf={r['confidence']:.2f} | {r['failure_type']}\"\n",
    "        )\n",
    "        print(f\"    description: {str(r['description_1'])[:55]}\")\n",
    "        print(f\"    gt desc:     {str(r.get('gt_desc', ''))[:55]}\")\n",
    "\n",
    "        for lvl_num, llm_col, gt_col in [\n",
    "            (\"L1\", \"category_1\", \"gt_L1\"),\n",
    "            (\"L2\", \"category_2\", \"gt_L2\"),\n",
    "            (\"L3\", \"category_3\", \"gt_L3\"),\n",
    "            (\"L4\", \"category_4\", \"gt_L4\"),\n",
    "        ]:\n",
    "            llm_val = str(r[llm_col])\n",
    "            gt_val  = str(r[gt_col])\n",
    "            ok      = \"Y\" if r[f\"match_{lvl_num}\"] else \"X\"\n",
    "            if not r[f\"match_{lvl_num}\"]:\n",
    "                print(f'    {lvl_num}: [{ok}] LLM=\"{llm_val}\" vs GT=\"{gt_val}\"')\n",
    "            else:\n",
    "                print(f'    {lvl_num}: [{ok}] \"{llm_val}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9 — Cost summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 65)\n",
    "print(\"COST SUMMARY\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "total_tokens_in  = df_results[\"tokens_in\"].sum()\n",
    "total_tokens_out = df_results[\"tokens_out\"].sum()\n",
    "total_cost       = df_results[\"estimated_cost\"].sum()\n",
    "model_name       = df_results[\"model_name\"].iloc[0] if len(df_results) > 0 else \"N/A\"\n",
    "\n",
    "print(f\"  Model:            {model_name}\")\n",
    "print(f\"  Total tokens in:  {total_tokens_in:,}\")\n",
    "print(f\"  Total tokens out: {total_tokens_out:,}\")\n",
    "print(f\"  Total tokens:     {total_tokens_in + total_tokens_out:,}\")\n",
    "print(f\"  Estimated cost:   ${total_cost:.4f}\")\n",
    "print(f\"  Codes classified: {len(df_results)}\")\n",
    "print(f\"  Cost per code:    ${total_cost / max(len(df_results), 1):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10 — Cross-version comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if multiple prompt versions exist in the results table\n",
    "all_versions = (\n",
    "    spark.sql(f\"SELECT DISTINCT prompt_version FROM {RESULTS_TABLE} ORDER BY prompt_version\")\n",
    "    .toPandas()[\"prompt_version\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(f\"Prompt versions in results table: {all_versions}\")\n",
    "\n",
    "if len(all_versions) > 1:\n",
    "    version_metrics = []\n",
    "\n",
    "    for version in all_versions:\n",
    "        v_results = (\n",
    "            spark.sql(f\"SELECT * FROM {RESULTS_TABLE} WHERE prompt_version = '{version}'\")\n",
    "            .toPandas()\n",
    "        )\n",
    "        v_results[\"transaction_code\"] = v_results[\"transaction_code\"].astype(str)\n",
    "\n",
    "        # Layer 1 evaluation for this version\n",
    "        v_l1 = pd.merge(\n",
    "            v_results[v_results[\"layer\"] == 1],\n",
    "            df_gt_single,\n",
    "            on=\"transaction_code\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        v_l1 = add_match_columns(v_l1)\n",
    "        v_vol = v_l1[\"volume\"].sum()\n",
    "\n",
    "        vw_exact   = (v_l1[\"exact_match\"] * v_l1[\"volume\"]).sum() / max(v_vol, 1)\n",
    "        vw_partial = (v_l1[\"partial_match_L1L2\"] * v_l1[\"volume\"]).sum() / max(v_vol, 1)\n",
    "        v_cost     = v_results[\"estimated_cost\"].sum()\n",
    "\n",
    "        version_metrics.append({\n",
    "            \"prompt_version\":     version,\n",
    "            \"codes\":              len(v_results),\n",
    "            \"l1_codes\":           len(v_l1),\n",
    "            \"exact_match\":        f\"{v_l1['exact_match'].mean():.1%}\",\n",
    "            \"partial_L1L2\":       f\"{v_l1['partial_match_L1L2'].mean():.1%}\",\n",
    "            \"vol_weighted_exact\": f\"{vw_exact:.1%}\",\n",
    "            \"vol_weighted_partial\": f\"{vw_partial:.1%}\",\n",
    "            \"L1_accuracy\":        f\"{v_l1['match_L1'].mean():.1%}\",\n",
    "            \"L2_accuracy\":        f\"{v_l1['match_L2'].mean():.1%}\",\n",
    "            \"L3_accuracy\":        f\"{v_l1['match_L3'].mean():.1%}\",\n",
    "            \"L4_accuracy\":        f\"{v_l1['match_L4'].mean():.1%}\",\n",
    "            \"estimated_cost\":     f\"${v_cost:.4f}\",\n",
    "        })\n",
    "\n",
    "    df_comparison = pd.DataFrame(version_metrics)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 65)\n",
    "    print(\"CROSS-VERSION COMPARISON\")\n",
    "    print(\"=\" * 65)\n",
    "    print(df_comparison.to_string(index=False))\n",
    "\n",
    "    # Highlight regressions and improvements between consecutive versions\n",
    "    if len(version_metrics) >= 2:\n",
    "        prev = version_metrics[-2]\n",
    "        curr = version_metrics[-1]\n",
    "        print(f\"\\n  Comparing {prev['prompt_version']} → {curr['prompt_version']}:\")\n",
    "        print(f\"    Vol-weighted exact: {prev['vol_weighted_exact']} → {curr['vol_weighted_exact']}\")\n",
    "        print(f\"    Vol-weighted partial: {prev['vol_weighted_partial']} → {curr['vol_weighted_partial']}\")\n",
    "        print(f\"    Cost: {prev['estimated_cost']} → {curr['estimated_cost']}\")\n",
    "else:\n",
    "    print(\"Only one prompt version found — cross-version comparison will be available after additional runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11 — Summary report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_total  = df_results[\"volume\"].sum()\n",
    "vw_exact   = (df_l1[\"exact_match\"] * df_l1[\"volume\"]).sum() / max(vol_l1, 1)\n",
    "vw_partial = (df_l1[\"partial_match_L1L2\"] * df_l1[\"volume\"]).sum() / max(vol_l1, 1)\n",
    "amb_match  = df_l2[\"exact_match_any\"].mean() if len(df_l2) > 0 else 0\n",
    "target_met = vw_exact >= TARGET_ACCURACY\n",
    "status     = \"MET\" if target_met else \"BELOW TARGET\"\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"  TRANSACTION CATEGORIZATION — ACCURACY SUMMARY\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"\")\n",
    "print(f\"  Prompt version:                  {df_results['prompt_version'].iloc[0]}\")\n",
    "print(f\"  Model:                           {model_name}\")\n",
    "print(f\"  Total codes evaluated:           {len(df_results)}\")\n",
    "print(f\"  Total transaction volume:        {vol_total:,}\")\n",
    "print(f\"\")\n",
    "print(f\"  LAYER 1 — Obvious (single-mapping)\")\n",
    "print(f\"    Codes: {len(df_l1)}    Volume: {vol_l1:,}\")\n",
    "print(f\"    L1 (Block):           {df_l1['match_L1'].mean():.1%}\")\n",
    "print(f\"    L2 (Category):        {df_l1['match_L2'].mean():.1%}\")\n",
    "print(f\"    L3 (Channel):         {df_l1['match_L3'].mean():.1%}\")\n",
    "print(f\"    L4 (Subtype):         {df_l1['match_L4'].mean():.1%}\")\n",
    "print(f\"    Exact Match (all 4):  {df_l1['exact_match'].mean():.1%}  (vol-wt: {vw_exact:.1%})\")\n",
    "print(f\"    Partial (L1+L2):      {df_l1['partial_match_L1L2'].mean():.1%}  (vol-wt: {vw_partial:.1%})\")\n",
    "print(f\"\")\n",
    "print(f\"  LAYER 2 — Ambiguous: {len(df_l2)} codes\")\n",
    "print(f\"    Match (any valid):    {amb_match:.1%}\")\n",
    "print(f\"\")\n",
    "print(f\"  LAYER 3 — Unknown: {len(df_l3)} codes (NEEDS MANUAL REVIEW)\")\n",
    "print(f\"\")\n",
    "if len(failures) > 0:\n",
    "    print(f\"  KEY FAILURES:\")\n",
    "    for ft_name in failures[\"failure_type\"].value_counts().index:\n",
    "        ft_count = len(failures[failures[\"failure_type\"] == ft_name])\n",
    "        print(f\"    {ft_name}: {ft_count} codes\")\n",
    "    print(f\"\")\n",
    "print(f\"  COST: ${total_cost:.4f} ({len(df_results)} codes)\")\n",
    "print(f\"\")\n",
    "print(f\"  TARGET: >= {TARGET_ACCURACY:.0%} vol-weighted exact match → {status}\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 12 — Save evaluation results to Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the evaluation output DataFrame\n",
    "# Layer 1: full match details\n",
    "eval_l1 = df_l1[[\n",
    "    \"transaction_code\", \"layer\", \"prompt_version\", \"model_name\",\n",
    "    \"description_1\", \"volume\", \"source_file\",\n",
    "    \"category_1\", \"category_2\", \"category_3\", \"category_4\",\n",
    "    \"gt_L1\", \"gt_L2\", \"gt_L3\", \"gt_L4\",\n",
    "    \"match_L1\", \"match_L2\", \"match_L3\", \"match_L4\",\n",
    "    \"exact_match\", \"partial_match_L1L2\", \"partial_match_L1L2L3\",\n",
    "    \"confidence\",\n",
    "]].copy()\n",
    "eval_l1[\"failure_type\"] = None\n",
    "if len(failures) > 0:\n",
    "    eval_l1.loc[eval_l1[\"transaction_code\"].isin(failures[\"transaction_code\"]), \"failure_type\"] = (\n",
    "        failures.set_index(\"transaction_code\")[\"failure_type\"]\n",
    "    )\n",
    "eval_l1[\"review_status\"] = \"EVALUATED\"\n",
    "\n",
    "# Layer 2: use match_any as exact_match\n",
    "eval_l2_records = []\n",
    "for _, row in df_l2_src.iterrows():\n",
    "    code = row[\"transaction_code\"]\n",
    "    l2_match = df_l2[df_l2[\"transaction_code\"] == code]\n",
    "    is_match = bool(l2_match[\"exact_match_any\"].iloc[0]) if len(l2_match) > 0 else False\n",
    "    eval_l2_records.append({\n",
    "        \"transaction_code\": code,\n",
    "        \"layer\":            row[\"layer\"],\n",
    "        \"prompt_version\":   row[\"prompt_version\"],\n",
    "        \"model_name\":       row[\"model_name\"],\n",
    "        \"description_1\":    row[\"description_1\"],\n",
    "        \"volume\":           row[\"volume\"],\n",
    "        \"source_file\":      row[\"source_file\"],\n",
    "        \"category_1\":       row[\"category_1\"],\n",
    "        \"category_2\":       row[\"category_2\"],\n",
    "        \"category_3\":       row[\"category_3\"],\n",
    "        \"category_4\":       row[\"category_4\"],\n",
    "        \"gt_L1\":            None,\n",
    "        \"gt_L2\":            None,\n",
    "        \"gt_L3\":            None,\n",
    "        \"gt_L4\":            None,\n",
    "        \"match_L1\":         None,\n",
    "        \"match_L2\":         None,\n",
    "        \"match_L3\":         None,\n",
    "        \"match_L4\":         None,\n",
    "        \"exact_match\":      is_match,\n",
    "        \"partial_match_L1L2\":   None,\n",
    "        \"partial_match_L1L2L3\": None,\n",
    "        \"confidence\":       row[\"confidence\"],\n",
    "        \"failure_type\":     None if is_match else \"AMBIGUOUS_NO_MATCH\",\n",
    "        \"review_status\":    \"EVALUATED\",\n",
    "    })\n",
    "eval_l2 = pd.DataFrame(eval_l2_records) if eval_l2_records else pd.DataFrame()\n",
    "\n",
    "# Layer 3: flagged for manual review\n",
    "eval_l3_records = []\n",
    "for _, row in df_l3.iterrows():\n",
    "    eval_l3_records.append({\n",
    "        \"transaction_code\": row[\"transaction_code\"],\n",
    "        \"layer\":            row[\"layer\"],\n",
    "        \"prompt_version\":   row[\"prompt_version\"],\n",
    "        \"model_name\":       row[\"model_name\"],\n",
    "        \"description_1\":    row[\"description_1\"],\n",
    "        \"volume\":           row[\"volume\"],\n",
    "        \"source_file\":      row[\"source_file\"],\n",
    "        \"category_1\":       row[\"category_1\"],\n",
    "        \"category_2\":       row[\"category_2\"],\n",
    "        \"category_3\":       row[\"category_3\"],\n",
    "        \"category_4\":       row[\"category_4\"],\n",
    "        \"gt_L1\":            None,\n",
    "        \"gt_L2\":            None,\n",
    "        \"gt_L3\":            None,\n",
    "        \"gt_L4\":            None,\n",
    "        \"match_L1\":         None,\n",
    "        \"match_L2\":         None,\n",
    "        \"match_L3\":         None,\n",
    "        \"match_L4\":         None,\n",
    "        \"exact_match\":      None,\n",
    "        \"partial_match_L1L2\":   None,\n",
    "        \"partial_match_L1L2L3\": None,\n",
    "        \"confidence\":       row[\"confidence\"],\n",
    "        \"failure_type\":     None,\n",
    "        \"review_status\":    \"NEEDS MANUAL REVIEW\",\n",
    "    })\n",
    "eval_l3_df = pd.DataFrame(eval_l3_records) if eval_l3_records else pd.DataFrame()\n",
    "\n",
    "# Combine all layers\n",
    "eval_parts = [df for df in [eval_l1, eval_l2, eval_l3_df] if len(df) > 0]\n",
    "df_eval = pd.concat(eval_parts, ignore_index=True) if eval_parts else pd.DataFrame()\n",
    "\n",
    "print(f\"Evaluation results: {len(df_eval)} rows\")\n",
    "print(f\"  Layer 1: {len(eval_l1)}\")\n",
    "print(f\"  Layer 2: {len(eval_l2)}\")\n",
    "print(f\"  Layer 3: {len(eval_l3_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sdf_eval = spark.createDataFrame(df_eval)\n",
    "    sdf_eval.write.mode(\"overwrite\").saveAsTable(EVAL_TABLE)\n",
    "    print(f\"Saved {len(df_eval)} rows to {EVAL_TABLE}\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC write.\")\n",
    "    print(f\"DataFrame ready with {len(df_eval)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {EVAL_TABLE}\").collect()[0][\"cnt\"]\n",
    "    print(f\"  OK  {EVAL_TABLE}: {count} rows\")\n",
    "\n",
    "    eval_cols = [f.name for f in spark.table(EVAL_TABLE).schema.fields]\n",
    "    required = [\n",
    "        \"transaction_code\", \"layer\", \"prompt_version\", \"model_name\",\n",
    "        \"exact_match\", \"failure_type\", \"review_status\",\n",
    "        \"category_1\", \"category_2\", \"gt_L1\", \"gt_L2\",\n",
    "    ]\n",
    "    missing = [c for c in required if c not in eval_cols]\n",
    "    assert not missing, f\"Missing columns: {missing}\"\n",
    "    print(f\"  OK  All required columns present\")\n",
    "\n",
    "    # Check review_status distribution\n",
    "    status_counts = spark.sql(\n",
    "        f\"SELECT review_status, COUNT(*) as cnt FROM {EVAL_TABLE} GROUP BY review_status\"\n",
    "    ).toPandas()\n",
    "    print(f\"\\n  Review status:\")\n",
    "    for _, row in status_counts.iterrows():\n",
    "        print(f\"    {row['review_status']}: {row['cnt']} codes\")\n",
    "\n",
    "    print(\"\\nAll validations passed.\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC validation.\")\n",
    "    print(f\"Local DataFrame ready: {len(df_eval)} rows, columns: {list(df_eval.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
