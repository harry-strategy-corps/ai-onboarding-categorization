{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d598bf1e-42a8-4bbb-bb85-a3939403042d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 04 — Transaction Categorization Test\n",
    "\n",
    "**Objective:** Execute the transaction categorization using Databricks `ai_query()` against the three test layers (Obvious, Ambiguous, Unknown).\n",
    "\n",
    "### Tasks:\n",
    "1. Define the system prompt with full taxonomy context and few-shot examples.\n",
    "2. Ensure the catalog data is available in a Unity Catalog table.\n",
    "3. Run `ai_query()` on the transaction catalog.\n",
    "4. Save all results to the `results/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9df9bc3-1b5a-40da-97e4-b1a5243843e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"databricks-claude-opus-4-6\"  # Update based on Step 03 discovery\n",
    "\n",
    "# Unity Catalog details\n",
    "# 'ciq-bp_dummy-dev' is the CATALOG name. We need a SCHEMA and a TABLE name.\n",
    "CATALOG_NAME = \"ciq-bp_dummy-dev\"\n",
    "SCHEMA_NAME = \"default\"  # You can change this to 'bronze' or another schema if preferred\n",
    "TABLE_NAME = \"transaction_code_catalog\"\n",
    "\n",
    "# Construct the full path with backticks to handle the hyphens in the catalog name\n",
    "CATALOG_TABLE_PATH = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.`{TABLE_NAME}`\"\n",
    "\n",
    "TAXONOMY_PATH = \"../taxonomy/transaction_categorization_taxonomy.md\"\n",
    "LOCAL_CATALOG_PATH = \"../taxonomy/data/transaction_code_catalog.csv\"\n",
    "\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "print(f\"Target table: {CATALOG_TABLE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd5373e0-58ca-4276-aefc-895381d2919e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Upload Local Data to Unity Catalog\n",
    "If the table doesn't exist in Unity Catalog yet, we need to create it from our local CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae62e819-7108-4200-b324-c4576d3dbf59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    print(f\"Reading local catalog from: {LOCAL_CATALOG_PATH}\")\n",
    "    pdf = pd.read_csv(LOCAL_CATALOG_PATH)\n",
    "    \n",
    "    # Convert to Spark DataFrame\n",
    "    sdf = spark.createDataFrame(pdf)\n",
    "    \n",
    "    # Write to Unity Catalog\n",
    "    print(f\"Writing to Unity Catalog: {CATALOG_TABLE_PATH}...\")\n",
    "    sdf.write.mode(\"overwrite\").saveAsTable(CATALOG_TABLE_PATH)\n",
    "    print(\"Table created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading table: {e}\")\n",
    "    print(\"If running locally, this step will fail. Ensure you are in a Databricks Notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "791d33b7-f4c4-45de-bbae-28bac2c21b6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Build Prompt\n",
    "We load the full taxonomy markdown file to provide rich context to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee3d937b-f7d4-449c-8583-6ef6349446ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(TAXONOMY_PATH, \"r\") as f:\n",
    "    taxonomy_md = f.read()\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a transaction categorization engine for a US bank. \n",
    "Given a transaction code (TRANCD) and description (DESC), classify it into the StrategyCorp taxonomy below.\n",
    "\n",
    "{taxonomy_md}\n",
    "\n",
    "### Rules:\n",
    "1. First determine Block A (Non-fee item) or Block B (Fee item). Fee items typically contain: \"fee\", \"charge\", \"surcharge\", \"penalty\", \"service charge\", \"reversal\".\n",
    "2. Refunds/Reversals of fees: Must be Block A > Money movement > Deposits.\n",
    "3. Classify through Level 2 > Level 3 > Level 4. Use EXACT strings from the taxonomy.\n",
    "4. Use \"Unclassified\" if no mapping fits. Do not guess.\n",
    "5. Return ONLY valid JSON. Do not include any explanation, reasoning, or markdown outside the JSON block. \n",
    "\n",
    "Your entire response must be a single JSON object with these keys: \n",
    "category_1, category_2, category_3, category_4, include_in_scoring, credit_debit, confidence.\n",
    "\"\"\"\n",
    "\n",
    "few_shot_examples = \"\"\"\n",
    "### Few-Shot Examples:\n",
    "Input: TRANCD=183, DESC=\"ACH Debit - SERMONS\"\n",
    "Output: {{\n",
    "  \"category_1\":\"Non-fee item\",\n",
    "  \"category_2\":\"Money movement\",\n",
    "  \"category_3\":\"ACH\",\n",
    "  \"category_4\":null,\n",
    "  \"include_in_scoring\":true,\n",
    "  \"credit_debit\":\"Debit\",\n",
    "  \"confidence\":0.99\n",
    "}}\n",
    "\n",
    "Input: TRANCD=299, DESC=\"ATM Service Charge\"\n",
    "Output: {{\n",
    "  \"category_1\":\"Fee item\",\n",
    "  \"category_2\":\"All others\",\n",
    "  \"category_3\":\"Money movement\",\n",
    "  \"category_4\":\"ATM\",\n",
    "  \"include_in_scoring\":false,\n",
    "  \"credit_debit\":\"Debit\",\n",
    "  \"confidence\":0.98\n",
    "}}\n",
    "\n",
    "Input: TRANCD=141, DESC=\"Transfer from DDA\"\n",
    "Output: {{\n",
    "  \"category_1\":\"Non-fee item\",\n",
    "  \"category_2\":\"Money movement\",\n",
    "  \"category_3\":\"Transfers & Payments\",\n",
    "  \"category_4\":null,\n",
    "  \"include_in_scoring\":true,\n",
    "  \"credit_debit\":\"Credit\",\n",
    "  \"confidence\":0.95\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "full_system_prompt = system_prompt + \"\\n\" + few_shot_examples\n",
    "print(\"Prompt prepared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "514b043c-c402-4777-ae29-8f2bfa8bd868",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Execute Classification (SQL)\n",
    "We use `ai_query()` to call the model and `from_json()` to parse the results into structured columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f675c5-4b09-47a9-bec8-587e6e0c67cb",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"llm_raw\":750},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1772138279899}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "escaped_prompt = full_system_prompt.replace(\"'\", \"''\")\n",
    "\n",
    "classification_query = f\"\"\"\n",
    "WITH raw_results AS (\n",
    "  SELECT \n",
    "    TRANCD,\n",
    "    sample_desc_1,\n",
    "    volume,\n",
    "    source_file,\n",
    "    ai_query(\n",
    "      '{MODEL_NAME}',\n",
    "      CONCAT('{escaped_prompt}', '\\\\nClassify: TRANCD=', TRANCD, ', DESC=\"', sample_desc_1, '\"')\n",
    "    ) as llm_raw\n",
    "  FROM {CATALOG_TABLE_PATH}\n",
    "),\n",
    "cleaned AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    -- Clean LLM output before JSON parsing:\n",
    "    --   1. Remove markdown code fences (```json ... ```)\n",
    "    --   2. Replace double braces {{{{ → {{  and  }}}} → }}\n",
    "    --   3. Trim whitespace\n",
    "    TRIM(\n",
    "      REPLACE(\n",
    "        REPLACE(\n",
    "          REPLACE(\n",
    "            REPLACE(llm_raw, '```json', ''),\n",
    "          '```', ''),\n",
    "        '{{{{', '{{'),\n",
    "      '}}}}', '}}')\n",
    "    ) as llm_clean\n",
    "  FROM raw_results\n",
    ")\n",
    "SELECT \n",
    "  TRANCD,\n",
    "  sample_desc_1,\n",
    "  volume,\n",
    "  source_file,\n",
    "  llm_raw,\n",
    "  llm_clean,\n",
    "  from_json(\n",
    "    llm_clean, \n",
    "    'category_1 STRING, category_2 STRING, category_3 STRING, category_4 STRING, include_in_scoring BOOLEAN, credit_debit STRING, confidence DOUBLE'\n",
    "  ) as parsed\n",
    "FROM cleaned\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    print(\"Executing categorization query...\")\n",
    "    results_df = spark.sql(classification_query)\n",
    "    display(results_df)\n",
    "\n",
    "    # Save results to Unity Catalog\n",
    "    output_table_name = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.transaction_classification_results\"\n",
    "    print(f\"Saving results to table: {output_table_name}...\")\n",
    "    results_df.write.mode(\"overwrite\").saveAsTable(output_table_name)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during classification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cdf47fe-ba53-47fd-a2e5-c4aa080ea457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Task-Specific AI Functions (ai_classify & ai_extract)\n",
    "\n",
    "We can compare `ai_query` (general-purpose) with task-specific functions like `ai_classify()` for flat categorization and `ai_extract()` for identifying specific attributes. These are optimized for simpler tasks and might be faster or more reliable for the first level of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b63621-ba40-4d35-91f1-c41dba2ebc9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test 1: Classify into Block A (Non-fee item) or Block B (Fee item)\n",
    "labels = [\"Non-fee item\", \"Fee item\"]\n",
    "\n",
    "classify_l1_query = f\"\"\"\n",
    "SELECT \n",
    "  TRANCD,\n",
    "  sample_desc_1 as description,\n",
    "  ai_classify(sample_desc_1, ARRAY('Non-fee item', 'Fee item')) as l1_prediction\n",
    "FROM {CATALOG_TABLE_PATH}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    print(\"Executing ai_classify for Level 1 (Block assignment)...\\n\")\n",
    "    display(spark.sql(classify_l1_query))\n",
    "except Exception as e:\n",
    "    print(f\"Error with ai_classify: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "858b0965-2dfb-4544-8b96-08274b4236f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test 2: Classify Level 2 categories\n",
    "l2_labels = [\"Money movement\", \"Account operations\", \"NSF/OD\", \"Misc\", \"Service Charges\", \"Interchange\"]\n",
    "labels_sql = \", \".join([f\"'{l}'\" for l in l2_labels])\n",
    "\n",
    "classify_l2_query = f\"\"\"\n",
    "SELECT \n",
    "  TRANCD,\n",
    "  sample_desc_1 as description,\n",
    "  ai_classify(sample_desc_1, ARRAY({labels_sql})) as l2_prediction\n",
    "FROM {CATALOG_TABLE_PATH}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    print(\"Executing ai_classify for Level 2...\\n\")\n",
    "    display(spark.sql(classify_l2_query))\n",
    "except Exception as e:\n",
    "    print(f\"Error with ai_classify: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3d7fb32-d8cd-4d53-bf0d-628890b6e5be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test 3: Extracting attributes using ai_extract\n",
    "# Useful for spotting 'reversals', 'refunds', or 'fee' types within the text.\n",
    "entities = [\"transaction_method\", \"is_reversal\", \"is_fee\", \"is_refund\"]\n",
    "entities_sql = \", \".join([f\"'{e}'\" for e in entities])\n",
    "\n",
    "extract_query = f\"\"\"\n",
    "SELECT \n",
    "  TRANCD,\n",
    "  sample_desc_1 as description,\n",
    "  ai_extract(sample_desc_1, ARRAY({entities_sql})) as extracted_info\n",
    "FROM {CATALOG_TABLE_PATH}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    print(\"Executing ai_extract for specific attributes...\\n\")\n",
    "    display(spark.sql(extract_query))\n",
    "except Exception as e:\n",
    "    print(f\"Error with ai_extract: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_transaction_categorization_test",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
