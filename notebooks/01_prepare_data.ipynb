{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acdd4fa3-3c0f-4a7b-a000-d25df2829193",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 01 — Prepare Data\n",
    "\n",
    "**Objective:** Build the three foundational Unity Catalog tables that the rest of the pipeline depends on.\n",
    "\n",
    "| Section | Output Table | Description |\n",
    "|---------|-------------|-------------|\n",
    "| **A** | `ground_truth_normalized` | Cleaned, normalized Master Fee Table (431 codes) |\n",
    "| **B.0** | `client_column_mappings` | AI-generated mapping from client columns to canonical schema |\n",
    "| **B** | `transaction_code_catalog` | Unique standardized codes from raw data with descriptions and volume |\n",
    "| **C** | Layer assignment | Adds `layer` column to the catalog (Obvious / Ambiguous / Unknown) |\n",
    "\n",
    "**Runs on:** Databricks Runtime 15.4 LTS or above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec9d4529-101d-4413-a633-73b96520c469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Configuration ─────────────────────────────────────────────────\n",
    "CATALOG_NAME = \"ciq-bp_dummy-dev\"\n",
    "SCHEMA_NAME  = \"default\"\n",
    "MODEL_NAME   = \"databricks-claude-opus-4-6\"\n",
    "\n",
    "GT_PATH          = \"../data/bank-plus-data/source-of-truth/Master Fee Table(Master).csv\"\n",
    "RAW_NON_POS_PATH = \"../data/bank-plus-data/raw/CheckingIQ_NON_POS_Daily_012626_rerun.csv\"\n",
    "RAW_POS_PATH     = \"../data/bank-plus-data/raw/CheckingIQ_POS_Daily_012626_rerun.csv\"\n",
    "\n",
    "GT_TABLE       = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.ground_truth_normalized\"\n",
    "CATALOG_TABLE  = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.transaction_code_catalog\"\n",
    "MAPPINGS_TABLE = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.client_column_mappings\"\n",
    "\n",
    "# Generic canonical schema — universal column names for ALL clients.\n",
    "CANONICAL_SCHEMA = {\n",
    "    \"transaction_code\":  \"Numeric transaction code (primary key for categorization)\",\n",
    "    \"description_1\":     \"Primary transaction description text\",\n",
    "    \"description_2\":     \"Secondary description / memo line\",\n",
    "    \"amount\":            \"Transaction amount\",\n",
    "    \"transaction_date\":  \"Date the transaction occurred\",\n",
    "    \"posting_date\":      \"Date the transaction was posted\",\n",
    "    \"account_number\":    \"Account identifier\",\n",
    "    \"account_status\":    \"Account status code\",\n",
    "    \"internal_account\":  \"Internal/alternate account number\",\n",
    "    \"transaction_desc\":  \"Transaction type description (may be empty)\",\n",
    "}\n",
    "\n",
    "print(f\"Catalog:        {CATALOG_NAME}\")\n",
    "print(f\"Schema:         {SCHEMA_NAME}\")\n",
    "print(f\"GT table:       {GT_TABLE}\")\n",
    "print(f\"Catalog table:  {CATALOG_TABLE}\")\n",
    "print(f\"Mappings table: {MAPPINGS_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d5a3889-6b11-43d3-a7c5-5d0fe3dfacab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Validation: check that input files exist ─────────────────────\n",
    "import os\n",
    "\n",
    "for path, label in [\n",
    "    (GT_PATH, \"Master Fee Table\"),\n",
    "    (RAW_NON_POS_PATH, \"NON_POS raw data\"),\n",
    "    (RAW_POS_PATH, \"POS raw data\"),\n",
    "]:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"  OK  {label}: {path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Missing {label}: {path}\")\n",
    "\n",
    "print(\"\\nAll input files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6600600c-be0a-4093-909a-bbcb06660fb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Section A — Ground Truth Normalization\n",
    "\n",
    "Read the Master Fee Table, clean it, normalize casing inconsistencies, and save to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01600063-da60-4660-9000-066060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_gt_raw = pd.read_csv(GT_PATH, encoding=\"latin-1\")\n",
    "\n",
    "# Strip whitespace from column names\n",
    "df_gt_raw.columns = [c.strip() for c in df_gt_raw.columns]\n",
    "\n",
    "print(f\"Raw rows loaded: {len(df_gt_raw)}\")\n",
    "print(f\"Columns: {list(df_gt_raw.columns)}\")\n",
    "df_gt_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60606060-6060-4060-a060-606060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gt = df_gt_raw.copy()\n",
    "\n",
    "# Strip whitespace from all string cells\n",
    "for col in df_gt.columns:\n",
    "    if df_gt[col].dtype == object:\n",
    "        df_gt[col] = df_gt[col].astype(str).str.strip()\n",
    "\n",
    "# Drop rows where External Transaction Code is empty or non-numeric\n",
    "# These are section headers (e.g. \"ATM activities\"), generic descriptions\n",
    "# (no transaction_code), and trailing blank rows.\n",
    "df_gt = df_gt[\n",
    "    df_gt[\"External Transaction Code\"].notna()\n",
    "    & (df_gt[\"External Transaction Code\"].astype(str).str.strip() != \"\")\n",
    "    & (df_gt[\"External Transaction Code\"].astype(str).str.strip() != \"nan\")\n",
    "].copy()\n",
    "\n",
    "df_gt[\"External Transaction Code\"] = df_gt[\"External Transaction Code\"].astype(str).str.strip()\n",
    "\n",
    "# Drop header-leak rows (the header row repeated mid-file at the Fee Items boundary)\n",
    "df_gt = df_gt[\n",
    "    df_gt[\"Scoring Category 1\"].astype(str).str.strip() != \"Scoring Category 1\"\n",
    "].copy()\n",
    "\n",
    "# Keep only rows where the transaction_code is a valid integer\n",
    "df_gt = df_gt[\n",
    "    df_gt[\"External Transaction Code\"].str.match(r\"^\\d+$\")\n",
    "].copy()\n",
    "\n",
    "print(f\"After cleaning: {len(df_gt)} rows, {df_gt['External Transaction Code'].nunique()} unique codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60606060-6060-4060-b060-606060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Normalization maps ────────────────────────────────────────────\n",
    "# Fix casing inconsistencies found in the spreadsheet.\n",
    "\n",
    "L1_NORM = {\n",
    "    \"Fee Item\":  \"Fee item\",\n",
    "    \"Fee item\":  \"Fee item\",\n",
    "    \"Non-fee item\": \"Non-fee item\",\n",
    "}\n",
    "\n",
    "L2_NORM = {\n",
    "    \"NSF /OD\":            \"NSF/OD\",\n",
    "    \"NSF/OD\":             \"NSF/OD\",\n",
    "    \"Money Movement\":     \"Money movement\",\n",
    "    \"Money movement\":     \"Money movement\",\n",
    "    \"Account Operations\": \"Account operations\",\n",
    "    \"Account operations\": \"Account operations\",\n",
    "    \"All others\":         \"All others\",\n",
    "    \"Service Charges\":    \"Service Charges\",\n",
    "    \"Interchange\":        \"Interchange\",\n",
    "    \"Miscellaneous\":      \"Miscellaneous\",\n",
    "    \"Unclassified\":       \"Unclassified\",\n",
    "}\n",
    "\n",
    "L3_NORM = {\n",
    "    \"N/A\": None,\n",
    "    \"nan\": None,\n",
    "    \"Money Movement\":     \"Money movement\",\n",
    "    \"Account Operations\": \"Account operations\",\n",
    "}\n",
    "\n",
    "L4_NORM = {\n",
    "    \"N/A\": None,\n",
    "    \"nan\": None,\n",
    "}\n",
    "\n",
    "\n",
    "def _apply_map(series, norm_map):\n",
    "    \"\"\"Map values through a normalization dict, keeping unmapped values as-is.\"\"\"\n",
    "    mapped = series.map(norm_map)\n",
    "    # Where the map returned a value (including explicit None), use it.\n",
    "    # Where the key was not in the map, keep the original.\n",
    "    has_mapping = series.isin(norm_map.keys())\n",
    "    return mapped.where(has_mapping, series)\n",
    "\n",
    "\n",
    "df_gt[\"Scoring Category 1\"] = _apply_map(df_gt[\"Scoring Category 1\"], L1_NORM)\n",
    "df_gt[\"Scoring Category 2\"] = _apply_map(df_gt[\"Scoring Category 2\"], L2_NORM)\n",
    "df_gt[\"Scoring Category 3\"] = _apply_map(df_gt[\"Scoring Category 3\"], L3_NORM)\n",
    "df_gt[\"Scoring Category 4\"] = _apply_map(df_gt[\"Scoring Category 4\"], L4_NORM)\n",
    "\n",
    "# Drop rows where L1 ended up as None (shouldn't happen after cleaning, but safety)\n",
    "df_gt = df_gt[df_gt[\"Scoring Category 1\"].notna()].copy()\n",
    "\n",
    "print(\"After normalization:\")\n",
    "print(f\"  L1 values: {sorted(df_gt['Scoring Category 1'].dropna().unique())}\")\n",
    "print(f\"  L2 values: {sorted(df_gt['Scoring Category 2'].dropna().unique())}\")\n",
    "print(f\"  L3 values: {sorted(df_gt['Scoring Category 3'].dropna().unique())}\")\n",
    "l4_vals = df_gt['Scoring Category 4'].dropna().unique()\n",
    "print(f\"  L4 values: {sorted([v for v in l4_vals if v is not None])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60606060-6060-4060-c060-606060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Rename to canonical column names ──────────────────────────────\n",
    "df_gt = df_gt.rename(columns={\n",
    "    \"External Transaction Code\":        \"transaction_code\",\n",
    "    \"External Transaction Description\": \"gt_desc\",\n",
    "    \"Scoring Category 1\":               \"gt_L1\",\n",
    "    \"Scoring Category 2\":               \"gt_L2\",\n",
    "    \"Scoring Category 3\":               \"gt_L3\",\n",
    "    \"Scoring Category 4\":               \"gt_L4\",\n",
    "    \"Credit / Debit\":                   \"gt_credit_debit\",\n",
    "})\n",
    "\n",
    "gt_cols = [\"transaction_code\", \"gt_desc\", \"gt_L1\", \"gt_L2\", \"gt_L3\", \"gt_L4\", \"gt_credit_debit\"]\n",
    "df_gt = df_gt[gt_cols].copy()\n",
    "\n",
    "# Replace remaining string 'None' / 'nan' artifacts\n",
    "df_gt.replace({\"None\": None, \"nan\": None}, inplace=True)\n",
    "\n",
    "print(f\"Ground truth ready: {len(df_gt)} rows, {df_gt['transaction_code'].nunique()} unique codes\")\n",
    "df_gt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60606060-6060-4060-d060-606060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Save ground truth to Unity Catalog ────────────────────────────\n",
    "try:\n",
    "    sdf_gt = spark.createDataFrame(df_gt)\n",
    "    sdf_gt.write.mode(\"overwrite\").saveAsTable(GT_TABLE)\n",
    "    print(f\"Saved {len(df_gt)} rows to {GT_TABLE}\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC write (run in Databricks).\")\n",
    "    print(f\"DataFrame ready with {len(df_gt)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01606060-da60-4660-9000-066060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_raw_csv(path, label=\"\"):\n",
    "    \"\"\"\n",
    "    Robust CSV loader for BankPlus raw files.\n",
    "    \n",
    "    Handles unquoted commas in EFHDS1/EFHDS2 by parsing from both ends:\n",
    "      - Front 6 columns (ACCTNO → AMT) are always safe\n",
    "      - Tail columns (Account# → end) are always safe\n",
    "      - Middle = EFHDS1 + EFHDS2, may contain extra commas\n",
    "    \n",
    "    Also filters out NUL bytes (\\x00) which are invalid in CSV files.\n",
    "    \n",
    "    Returns a DataFrame with the ORIGINAL client column names.\n",
    "    Column renaming to canonical names is handled separately in Section B.\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        raw_bytes = f.read()\n",
    "    \n",
    "    cleaned_bytes = raw_bytes.replace(b'\\x00', b'')\n",
    "    text = cleaned_bytes.decode('utf-8', errors='replace')\n",
    "    lines = text.splitlines()\n",
    "    \n",
    "    reader = csv.reader(lines)\n",
    "    header = next(reader)\n",
    "\n",
    "    header = [c.strip() for c in header]\n",
    "    n_expected = len(header)\n",
    "    n_front = 6\n",
    "    n_tail  = n_expected - 6 - 2\n",
    "\n",
    "    rows = []\n",
    "    n_fixed = 0\n",
    "\n",
    "    reader = csv.reader(lines[1:])\n",
    "    for line_num, fields in enumerate(reader, start=2):\n",
    "        n = len(fields)\n",
    "\n",
    "        if n == n_expected:\n",
    "            rows.append(fields)\n",
    "\n",
    "        elif n > n_expected:\n",
    "            front  = fields[:n_front]\n",
    "            tail   = fields[n - n_tail:]\n",
    "            middle = fields[n_front : n - n_tail]\n",
    "\n",
    "            efhds1 = \",\".join(middle[:-1])\n",
    "            efhds2 = middle[-1]\n",
    "\n",
    "            rows.append(front + [efhds1, efhds2] + tail)\n",
    "            n_fixed += 1\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=header)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    total = len(df)\n",
    "    print(f\"{label} rows: {total:,}\")\n",
    "    print(f\"  → Rows with commas in EFHDS1 (fixed): {n_fixed:,}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "231b8cbb-4052-4d72-a6f0-6321c8351451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Section B.0 — AI Column Mapping\n",
    "\n",
    "Identify the mapping from client-specific column names to the generic canonical schema\n",
    "using `ai_query()`. This allows the pipeline to handle different core banking systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0.1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import json\n",
    "\n",
    "def get_column_profiles(path, n_rows=20):\n",
    "    \"\"\"\n",
    "    Load a sample of raw client data and infer basic column types for the AI prompt.\n",
    "    Returns the original client column names — no renaming is applied.\n",
    "    \"\"\"\n",
    "    df_sample = load_raw_csv(path, label=\"Profiling\")\n",
    "    df_sample = df_sample.head(n_rows)\n",
    "\n",
    "    profiles = []\n",
    "    for col in df_sample.columns:\n",
    "        vals = df_sample[col].dropna().astype(str).str.strip()\n",
    "        vals = vals[vals != \"\"].head(5).tolist()\n",
    "        \n",
    "        dtype = \"text\"\n",
    "        if all(v.replace(\".\", \"\").replace(\"-\", \"\").isdigit() for v in vals if v):\n",
    "            if any(\".\" in v for v in vals): dtype = \"decimal numbers\"\n",
    "            elif all(len(v) <= 3 for v in vals): dtype = \"small integers\"\n",
    "            else: dtype = \"integers\"\n",
    "            \n",
    "        if any(\"-\" in v and len(v) == 10 for v in vals):\n",
    "            dtype = \"dates (YYYY-MM-DD)\"\n",
    "\n",
    "        profiles.append(f\"  - Column: \\\"{col}\\\"\\n    Data type: {dtype}\\n    Sample values: {vals}\")\n",
    "    return \"\\n\".join(profiles), list(df_sample.columns)\n",
    "\n",
    "print(\"Building column profiles from NON_POS data (raw client column names)...\")\n",
    "profiles_text, client_cols = get_column_profiles(RAW_NON_POS_PATH)\n",
    "print(f\"\\nClient columns to map: {client_cols}\")\n",
    "\n",
    "canonical_schema_text = \"\\n\".join(f\"  - {col}: {desc}\" for col, desc in CANONICAL_SCHEMA.items())\n",
    "\n",
    "mapping_prompt = f\"\"\"You are a data engineering expert specializing in US banking core systems.\n",
    "\n",
    "A financial institution has provided a transaction data export. Their column names\n",
    "are abbreviations from their core banking system (e.g., Jack Henry SilverLake,\n",
    "Fiserv, FIS). Your task is to map each column to our canonical schema.\n",
    "\n",
    "### Canonical Schema (target — map TO these names)\n",
    "{canonical_schema_text}\n",
    "\n",
    "### Client Data Profile (source — map FROM these columns)\n",
    "{profiles_text}\n",
    "\n",
    "### Key Banking Domain Knowledge\n",
    "- Core systems use abbreviations: TRANCD = transaction code, EFHDS = extended\n",
    "  field header/description, AMT = amount, ACCTNO = account number, etc.\n",
    "- \\\"EFHDS1\\\" and \\\"EFHDS2\\\" are Jack Henry's names for description lines 1 and 2.\n",
    "- Transaction files often have TWO date columns: a transaction date and a posting\n",
    "  date. They may look similar but serve different purposes.\n",
    "- \\\"status\\\" is typically a small integer (1, 2, 3) indicating account status.\n",
    "- \\\"description\\\" (if mostly empty) is a legacy field for transaction type description.\n",
    "- There may be TWO account number columns: an external-facing one and an internal one.\n",
    "\n",
    "### Few-Shot Example (different bank, similar task)\n",
    "A Fiserv bank had these columns: TxnCode, Desc1, Desc2, TxnAmt, TxnDate,\n",
    "PostDate, AcctNum, AcctStat, IntAcct, TxnDesc.\n",
    "\n",
    "Correct mapping:\n",
    "  TxnCode  → transaction_code\n",
    "  Desc1    → description_1\n",
    "  Desc2    → description_2\n",
    "  TxnAmt   → amount\n",
    "  TxnDate  → transaction_date\n",
    "  PostDate → posting_date\n",
    "  AcctNum  → account_number\n",
    "  AcctStat → account_status\n",
    "  IntAcct  → internal_account\n",
    "  TxnDesc  → transaction_desc\n",
    "\n",
    "### Instructions\n",
    "1. MATCH BY DATA VALUES, not just column names. Look at the sample values to\n",
    "   determine what each column actually contains.\n",
    "2. Map each client column to exactly one canonical column, or null if no match.\n",
    "3. Each canonical column can only be used once.\n",
    "4. Think step by step:\n",
    "   - Which column has small integers (like 183, 163)? → transaction_code\n",
    "   - Which column has dollar amounts (like 258.20)? → amount\n",
    "   - Which column has text descriptions? → description_1 (primary), description_2 (secondary)\n",
    "   - Which columns have dates? → the one named for transaction date, the other for posting date\n",
    "   - Which column has alphanumeric account IDs? → account_number\n",
    "   - Which column has numeric-only account numbers? → internal_account\n",
    "   - Which column has single-digit status codes? → account_status\n",
    "   - Which column is mostly empty text? → transaction_desc\n",
    "\n",
    "Return ONLY a JSON object. Keys = client column names, values = canonical column names (or null).\n",
    "\"\"\"\n",
    "print(\"Prompt prepared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0.2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "escaped_prompt = mapping_prompt.replace(\"'\", \"''\")\n",
    "def sanitize(c): return re.sub(r'[^a-zA-Z0-9_.-]', '_', c)\n",
    "san_to_orig = {sanitize(c): c for c in client_cols}\n",
    "props = {sanitize(c): {\"type\": [\"string\", \"null\"]} for c in client_cols}\n",
    "schema = json.dumps({\"type\": \"json_schema\", \"json_schema\": {\"name\": \"mapping\", \"schema\": {\"type\": \"object\", \"properties\": props}, \"strict\": True}})\n",
    "\n",
    "query = f\"SELECT ai_query('{MODEL_NAME}', '{escaped_prompt}', responseFormat => '{schema}') as res\"\n",
    "try:\n",
    "    print(\"Calling ai_query...\")\n",
    "    res_raw = spark.sql(query).collect()[0][\"res\"]\n",
    "    ai_mapping_san = json.loads(res_raw)\n",
    "    CLIENT_RENAME_MAP = {san_to_orig[k]: v for k, v in ai_mapping_san.items() if v}\n",
    "    print(f\"Mapping derived for {len(CLIENT_RENAME_MAP)} columns.\")\n",
    "except NameError:\n",
    "    print(\"Spark not found — using hardcoded Bank Plus mapping.\")\n",
    "    CLIENT_RENAME_MAP = {\"TRANCD\": \"transaction_code\", \"EFHDS1\": \"description_1\", \"EFHDS2\": \"description_2\", \"AMT\": \"amount\", \"TRDATE\": \"transaction_date\", \"PostingDate\": \"posting_date\", \"ACCTNO\": \"account_number\", \"status\": \"account_status\", \"Account#\": \"internal_account\", \"description\": \"transaction_desc\"}\n",
    "\n",
    "print(\"Final Rename Map:\")\n",
    "for k, v in CLIENT_RENAME_MAP.items(): print(f\"  {k:<15} -> {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0.3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "mapping_df = pd.DataFrame([{\"client_column\": k, \"canonical_column\": v, \"run_timestamp\": datetime.utcnow().isoformat()} for k, v in CLIENT_RENAME_MAP.items()])\n",
    "try:\n",
    "    spark.createDataFrame(mapping_df).write.mode(\"overwrite\").saveAsTable(MAPPINGS_TABLE)\n",
    "    print(f\"Saved mapping to {MAPPINGS_TABLE}\")\n",
    "except NameError: print(\"Skipping UC write.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "231b8cbb-4052-4d72-a6f0-6321c8351451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Section B — Transaction Code Catalog\n",
    "\n",
    "Build a catalog of unique transaction codes from the raw NON_POS and POS daily files.\n",
    "For each code, capture one sample description and the total transaction volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60606060-6060-4060-e060-606060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_non_pos = load_raw_csv(RAW_NON_POS_PATH, label=\"NON_POS\")\n",
    "\n",
    "# Apply the AI-derived column mapping from Section B.0\n",
    "df_non_pos = df_non_pos.rename(columns=CLIENT_RENAME_MAP)\n",
    "df_non_pos[\"transaction_code\"] = df_non_pos[\"transaction_code\"].astype(str).str.strip()\n",
    "print(f\"\\nColumns after applying CLIENT_RENAME_MAP: {list(df_non_pos.columns)}\")\n",
    "print(f\"NON_POS unique codes: {df_non_pos['transaction_code'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60606060-6060-4060-f060-606060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(df_non_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60606060-6060-4060-0060-606060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pos = load_raw_csv(RAW_POS_PATH, label=\"POS\")\n",
    "\n",
    "# Apply the AI-derived column mapping from Section B.0\n",
    "df_pos = df_pos.rename(columns=CLIENT_RENAME_MAP)\n",
    "df_pos[\"transaction_code\"] = df_pos[\"transaction_code\"].astype(str).str.strip()\n",
    "print(f\"\\nColumns after applying CLIENT_RENAME_MAP: {list(df_pos.columns)}\")\n",
    "print(f\"POS unique codes: {df_pos['transaction_code'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60606060-6060-4060-1060-606060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Build NON_POS catalog entries ─────────────────────────────────\n",
    "# description_1 is the primary description field (mapped from EFHDS1 for Bank Plus)\n",
    "non_pos_catalog = (\n",
    "    df_non_pos\n",
    "    .groupby(\"transaction_code\")\n",
    "    .agg(\n",
    "        description_1=(\"description_1\", \"first\"),\n",
    "        volume=(\"transaction_code\", \"size\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "non_pos_catalog[\"source_file\"] = \"NON_POS\"\n",
    "non_pos_catalog[\"description_1\"] = non_pos_catalog[\"description_1\"].astype(str).str.strip()\n",
    "\n",
    "print(f\"NON_POS catalog: {len(non_pos_catalog)} unique codes\")\n",
    "non_pos_catalog.sort_values(\"volume\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60606060-6060-4060-2060-606060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Build POS catalog entries ─────────────────────────────────────\n",
    "# transaction_desc is the populated description column for POS\n",
    "pos_catalog = (\n",
    "    df_pos\n",
    "    .groupby(\"transaction_code\")\n",
    "    .agg(\n",
    "        description_1=(\"transaction_desc\", \"first\"),\n",
    "        volume=(\"transaction_code\", \"size\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "pos_catalog[\"source_file\"] = \"POS\"\n",
    "pos_catalog[\"description_1\"] = pos_catalog[\"description_1\"].astype(str).str.strip()\n",
    "\n",
    "print(f\"POS catalog: {len(pos_catalog)} unique codes\")\n",
    "pos_catalog.sort_values(\"volume\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60606060-6060-4060-3060-606060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Combine into a single catalog ─────────────────────────────────\n",
    "df_catalog = pd.concat([non_pos_catalog, pos_catalog], ignore_index=True)\n",
    "df_catalog[\"transaction_code\"] = df_catalog[\"transaction_code\"].astype(str)\n",
    "\n",
    "print(f\"Combined catalog: {len(df_catalog)} codes\")\n",
    "print(f\"Total transaction volume: {df_catalog['volume'].sum():,}\")\n",
    "df_catalog.sort_values(\"volume\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60606060-6060-4060-4060-606060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Section C — Layer Assignment\n",
    "\n",
    "Assign each catalog code to a test layer based on how it appears in the ground truth:\n",
    "\n",
    "| Layer | Name | Rule |\n",
    "|-------|------|------|\n",
    "| 1 | Obvious | Exactly 1 unique (L1,L2,L3,L4) mapping in GT |\n",
    "| 2 | Ambiguous | 2+ distinct mappings in GT |\n",
    "| 3 | Unknown | transaction_code absent from GT entirely |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60606060-6060-4060-5060-606060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count distinct mappings per transaction_code in ground truth\n",
    "gt_mapping_counts = (\n",
    "    df_gt\n",
    "    .groupby(\"transaction_code\")\n",
    "    .apply(lambda g: g[[\"gt_L1\", \"gt_L2\", \"gt_L3\", \"gt_L4\"]].drop_duplicates().shape[0])\n",
    "    .reset_index(name=\"n_mappings\")\n",
    ")\n",
    "\n",
    "multi_codes  = set(gt_mapping_counts.loc[gt_mapping_counts[\"n_mappings\"] > 1, \"transaction_code\"])\n",
    "single_codes = set(gt_mapping_counts.loc[gt_mapping_counts[\"n_mappings\"] == 1, \"transaction_code\"])\n",
    "all_gt_codes = set(df_gt[\"transaction_code\"].unique())\n",
    "\n",
    "print(f\"GT codes with 1 mapping (Layer 1):  {len(single_codes)}\")\n",
    "print(f\"GT codes with 2+ mappings (Layer 2): {len(multi_codes)}\")\n",
    "print(f\"Multi-mapping codes: {sorted(multi_codes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60606060-6060-4060-6060-606060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def assign_layer(trancd):\n",
    "    if trancd not in all_gt_codes:\n",
    "        return 3  # Unknown\n",
    "    if trancd in multi_codes:\n",
    "        return 2  # Ambiguous\n",
    "    return 1      # Obvious\n",
    "\n",
    "\n",
    "df_catalog[\"layer\"] = df_catalog[\"transaction_code\"].apply(assign_layer)\n",
    "\n",
    "# ── Summary ───────────────────────────────────────────────────────\n",
    "layer_summary = (\n",
    "    df_catalog\n",
    "    .groupby(\"layer\")\n",
    "    .agg(codes=(\"transaction_code\", \"nunique\"), total_volume=(\"volume\", \"sum\"))\n",
    "    .reset_index()\n",
    ")\n",
    "layer_summary[\"pct_volume\"] = (\n",
    "    layer_summary[\"total_volume\"] / layer_summary[\"total_volume\"].sum() * 100\n",
    ").round(1)\n",
    "\n",
    "layer_names = {1: \"Obvious\", 2: \"Ambiguous\", 3: \"Unknown\"}\n",
    "layer_summary[\"name\" ] = layer_summary[\"layer\"].map(layer_names)\n",
    "\n",
    "print(\"Layer assignment summary:\")\n",
    "print(layer_summary[[\"layer\", \"name\", \"codes\", \"total_volume\", \"pct_volume\"]].to_string(index=False))\n",
    "print(f\"\\nTotal codes: {len(df_catalog)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60606060-6060-4060-7060-606060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Show codes per layer ──────────────────────────────────────────\n",
    "for layer_num, layer_name in layer_names.items():\n",
    "    layer_df = df_catalog[df_catalog[\"layer\"] == layer_num].sort_values(\"volume\", ascending=False)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Layer {layer_num} — {layer_name} ({len(layer_df)} codes)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for _, row in layer_df.iterrows():\n",
    "        print(f\"  transaction_code={row['transaction_code']:>5} | vol={row['volume']:>7,} | {row['source_file']:<7} | {str(row['description_1'])[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60606060-6060-4060-8060-606060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Save catalog (with layers) to Unity Catalog ───────────────────\n",
    "try:\n",
    "    df_catalog_to_save = df_catalog.copy()\n",
    "    df_catalog_to_save[\"transaction_code\"] = df_catalog_to_save[\"transaction_code\"].astype(int)\n",
    "    \n",
    "    sdf_catalog = spark.createDataFrame(df_catalog_to_save)\n",
    "    sdf_catalog.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(CATALOG_TABLE)\n",
    "    print(f\"Saved {len(df_catalog)} rows to {CATALOG_TABLE}\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC write (run in Databricks).\")\n",
    "    print(f\"DataFrame ready with {len(df_catalog)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60606060-6060-4060-9060-606060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Validation\n",
    "\n",
    "Verify that all Unity Catalog tables were created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60606060-6060-4060-a060-606060606060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Validate UC tables ────────────────────────────────────────────\n",
    "try:\n",
    "    for table_name, expected_label in [\n",
    "        (GT_TABLE, \"ground_truth_normalized\"),\n",
    "        (CATALOG_TABLE, \"transaction_code_catalog\"),\n",
    "    ]:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {table_name}\").collect()[0][\"cnt\"]\n",
    "        print(f\"  OK  {expected_label}: {count} rows\")\n",
    "\n",
    "    # Verify the catalog has a layer column\n",
    "    catalog_cols = [f.name for f in spark.table(CATALOG_TABLE).schema.fields]\n",
    "    assert \"layer\" in catalog_cols, \"Missing 'layer' column in catalog table\"\n",
    "    print(f\"  OK  catalog has 'layer' column\")\n",
    "\n",
    "    print(\"\\nAll validations passed.\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC validation (run in Databricks).\")\n",
    "    print(\"Local DataFrames are ready:\")\n",
    "    print(f\"  df_gt:      {len(df_gt)} rows, {df_gt['transaction_code'].nunique()} unique codes\")\n",
    "    print(f\"  df_catalog: {len(df_catalog)} rows, columns: {list(df_catalog.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "nuid": "ec9d4529-101d-4413-a633-73b96520c469",
    "pythonIndentUnit": 4
   },
   "notebookName": "01_prepare_data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
