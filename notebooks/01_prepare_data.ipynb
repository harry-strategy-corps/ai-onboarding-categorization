{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acdd4fa3-3c0f-4a7b-a000-d25df2829193",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 01 — Prepare Data\n",
    "\n",
    "**Objective:** Build the three foundational Unity Catalog tables that the rest of the pipeline depends on.\n",
    "\n",
    "| Section | Output Table | Description |\n",
    "|---------|-------------|-------------|\n",
    "| **A** | `ground_truth_normalized` | Cleaned, normalized Master Fee Table (431 codes) |\n",
    "| **B** | `transaction_code_catalog` | Unique TRANCD codes from raw data with sample descriptions and volume |\n",
    "| **C** | Layer assignment | Adds `layer` column to the catalog (Obvious / Ambiguous / Unknown) |\n",
    "\n",
    "**Runs on:** Databricks Runtime 15.4 LTS or above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec9d4529-101d-4413-a633-73b96520c469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Configuration ─────────────────────────────────────────────────\n",
    "CATALOG_NAME = \"ciq-bp_dummy-dev\"\n",
    "SCHEMA_NAME  = \"default\"\n",
    "\n",
    "GT_PATH          = \"../data/bank-plus-data/source-of-truth/Master Fee Table(Master).csv\"\n",
    "RAW_NON_POS_PATH = \"../data/bank-plus-data/raw/CheckingIQ_NON_POS_Daily_012626_rerun.csv\"\n",
    "RAW_POS_PATH     = \"../data/bank-plus-data/raw/CheckingIQ_POS_Daily_012626_rerun.csv\"\n",
    "\n",
    "GT_TABLE      = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.ground_truth_normalized\"\n",
    "CATALOG_TABLE = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.transaction_code_catalog\"\n",
    "\n",
    "print(f\"Catalog:        {CATALOG_NAME}\")\n",
    "print(f\"Schema:         {SCHEMA_NAME}\")\n",
    "print(f\"GT table:       {GT_TABLE}\")\n",
    "print(f\"Catalog table:  {CATALOG_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d5a3889-6b11-43d3-a7c5-5d0fe3dfacab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Validation: check that input files exist ─────────────────────\n",
    "import os\n",
    "\n",
    "for path, label in [\n",
    "    (GT_PATH, \"Master Fee Table\"),\n",
    "    (RAW_NON_POS_PATH, \"NON_POS raw data\"),\n",
    "    (RAW_POS_PATH, \"POS raw data\"),\n",
    "]:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"  OK  {label}: {path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Missing {label}: {path}\")\n",
    "\n",
    "print(\"\\nAll input files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf956b1-d81b-42c3-bf67-1de3820698f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Section A — Ground Truth Normalization\n",
    "\n",
    "Read the Master Fee Table, clean it, normalize casing inconsistencies, and save to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba2bc0b5-532d-4835-8566-0caeee478e08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_gt_raw = pd.read_csv(GT_PATH, encoding=\"latin-1\")\n",
    "\n",
    "# Strip whitespace from column names\n",
    "df_gt_raw.columns = [c.strip() for c in df_gt_raw.columns]\n",
    "\n",
    "print(f\"Raw rows loaded: {len(df_gt_raw)}\")\n",
    "print(f\"Columns: {list(df_gt_raw.columns)}\")\n",
    "df_gt_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a015948b-6bde-4b0c-bcff-a53dc37fc49d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gt = df_gt_raw.copy()\n",
    "\n",
    "# Strip whitespace from all string cells\n",
    "for col in df_gt.columns:\n",
    "    if df_gt[col].dtype == object:\n",
    "        df_gt[col] = df_gt[col].astype(str).str.strip()\n",
    "\n",
    "# Drop rows where External Transaction Code is empty or non-numeric\n",
    "# These are section headers (e.g. \"ATM activities\"), generic descriptions\n",
    "# (no TRANCD), and trailing blank rows.\n",
    "df_gt = df_gt[\n",
    "    df_gt[\"External Transaction Code\"].notna()\n",
    "    & (df_gt[\"External Transaction Code\"].astype(str).str.strip() != \"\")\n",
    "    & (df_gt[\"External Transaction Code\"].astype(str).str.strip() != \"nan\")\n",
    "].copy()\n",
    "\n",
    "df_gt[\"External Transaction Code\"] = df_gt[\"External Transaction Code\"].astype(str).str.strip()\n",
    "\n",
    "# Drop header-leak rows (the header row repeated mid-file at the Fee Items boundary)\n",
    "df_gt = df_gt[\n",
    "    df_gt[\"Scoring Category 1\"].astype(str).str.strip() != \"Scoring Category 1\"\n",
    "].copy()\n",
    "\n",
    "# Keep only rows where the TRANCD is a valid integer\n",
    "df_gt = df_gt[\n",
    "    df_gt[\"External Transaction Code\"].str.match(r\"^\\d+$\")\n",
    "].copy()\n",
    "\n",
    "print(f\"After cleaning: {len(df_gt)} rows, {df_gt['External Transaction Code'].nunique()} unique codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d6ae179-2dab-4bf9-80b9-56b8f133a199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Normalization maps ────────────────────────────────────────────\n",
    "# Fix casing inconsistencies found in the spreadsheet.\n",
    "\n",
    "L1_NORM = {\n",
    "    \"Fee Item\":  \"Fee item\",\n",
    "    \"Fee item\":  \"Fee item\",\n",
    "    \"Non-fee item\": \"Non-fee item\",\n",
    "}\n",
    "\n",
    "L2_NORM = {\n",
    "    \"NSF /OD\":            \"NSF/OD\",\n",
    "    \"NSF/OD\":             \"NSF/OD\",\n",
    "    \"Money Movement\":     \"Money movement\",\n",
    "    \"Money movement\":     \"Money movement\",\n",
    "    \"Account Operations\": \"Account operations\",\n",
    "    \"Account operations\": \"Account operations\",\n",
    "    \"All others\":         \"All others\",\n",
    "    \"Service Charges\":    \"Service Charges\",\n",
    "    \"Interchange\":        \"Interchange\",\n",
    "    \"Miscellaneous\":      \"Miscellaneous\",\n",
    "    \"Unclassified\":       \"Unclassified\",\n",
    "}\n",
    "\n",
    "L3_NORM = {\n",
    "    \"N/A\": None,\n",
    "    \"nan\": None,\n",
    "    \"Money Movement\":     \"Money movement\",\n",
    "    \"Account Operations\": \"Account operations\",\n",
    "}\n",
    "\n",
    "L4_NORM = {\n",
    "    \"N/A\": None,\n",
    "    \"nan\": None,\n",
    "}\n",
    "\n",
    "\n",
    "def _apply_map(series, norm_map):\n",
    "    \"\"\"Map values through a normalization dict, keeping unmapped values as-is.\"\"\"\n",
    "    mapped = series.map(norm_map)\n",
    "    # Where the map returned a value (including explicit None), use it.\n",
    "    # Where the key was not in the map, keep the original.\n",
    "    has_mapping = series.isin(norm_map.keys())\n",
    "    return mapped.where(has_mapping, series)\n",
    "\n",
    "\n",
    "df_gt[\"Scoring Category 1\"] = _apply_map(df_gt[\"Scoring Category 1\"], L1_NORM)\n",
    "df_gt[\"Scoring Category 2\"] = _apply_map(df_gt[\"Scoring Category 2\"], L2_NORM)\n",
    "df_gt[\"Scoring Category 3\"] = _apply_map(df_gt[\"Scoring Category 3\"], L3_NORM)\n",
    "df_gt[\"Scoring Category 4\"] = _apply_map(df_gt[\"Scoring Category 4\"], L4_NORM)\n",
    "\n",
    "# Drop rows where L1 ended up as None (shouldn't happen after cleaning, but safety)\n",
    "df_gt = df_gt[df_gt[\"Scoring Category 1\"].notna()].copy()\n",
    "\n",
    "print(\"After normalization:\")\n",
    "print(f\"  L1 values: {sorted(df_gt['Scoring Category 1'].dropna().unique())}\")\n",
    "print(f\"  L2 values: {sorted(df_gt['Scoring Category 2'].dropna().unique())}\")\n",
    "print(f\"  L3 values: {sorted(df_gt['Scoring Category 3'].dropna().unique())}\")\n",
    "l4_vals = df_gt['Scoring Category 4'].dropna().unique()\n",
    "print(f\"  L4 values: {sorted([v for v in l4_vals if v is not None])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d526d9-0a32-4d8f-96f6-3a4160373da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Rename to canonical column names ──────────────────────────────\n",
    "df_gt = df_gt.rename(columns={\n",
    "    \"External Transaction Code\":        \"TRANCD\",\n",
    "    \"External Transaction Description\": \"gt_desc\",\n",
    "    \"Scoring Category 1\":               \"gt_L1\",\n",
    "    \"Scoring Category 2\":               \"gt_L2\",\n",
    "    \"Scoring Category 3\":               \"gt_L3\",\n",
    "    \"Scoring Category 4\":               \"gt_L4\",\n",
    "    \"Credit / Debit\":                   \"gt_credit_debit\",\n",
    "})\n",
    "\n",
    "gt_cols = [\"TRANCD\", \"gt_desc\", \"gt_L1\", \"gt_L2\", \"gt_L3\", \"gt_L4\", \"gt_credit_debit\"]\n",
    "df_gt = df_gt[gt_cols].copy()\n",
    "\n",
    "# Replace remaining string 'None' / 'nan' artifacts\n",
    "df_gt.replace({\"None\": None, \"nan\": None}, inplace=True)\n",
    "\n",
    "print(f\"Ground truth ready: {len(df_gt)} rows, {df_gt['TRANCD'].nunique()} unique codes\")\n",
    "df_gt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f824c153-da08-451f-9e08-cee75381f498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Save ground truth to Unity Catalog ────────────────────────────\n",
    "try:\n",
    "    sdf_gt = spark.createDataFrame(df_gt)\n",
    "    sdf_gt.write.mode(\"overwrite\").saveAsTable(GT_TABLE)\n",
    "    print(f\"Saved {len(df_gt)} rows to {GT_TABLE}\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC write (run in Databricks).\")\n",
    "    print(f\"DataFrame ready with {len(df_gt)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "202db629-be33-4dd2-9623-fe13a9f66f8c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 10"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_raw_csv(path, label=\"\"):\n",
    "    \"\"\"\n",
    "    Robust CSV loader for BankPlus raw files.\n",
    "    \n",
    "    Handles unquoted commas in EFHDS1/EFHDS2 by parsing from both ends:\n",
    "      - Front 6 columns (ACCTNO → AMT) are always safe\n",
    "      - Tail columns (Account# → end) are always safe\n",
    "      - Middle = EFHDS1 + EFHDS2, may contain extra commas\n",
    "    \n",
    "    Also filters out NUL bytes (\\x00) which are invalid in CSV files.\n",
    "    \"\"\"\n",
    "    # Read file and strip NUL bytes\n",
    "    with open(path, \"rb\") as f:\n",
    "        raw_bytes = f.read()\n",
    "    \n",
    "    # Remove NUL bytes\n",
    "    cleaned_bytes = raw_bytes.replace(b'\\x00', b'')\n",
    "    \n",
    "    # Decode to string and split into lines for csv.reader\n",
    "    text = cleaned_bytes.decode('utf-8', errors='replace')\n",
    "    lines = text.splitlines()\n",
    "    \n",
    "    # Parse header\n",
    "    reader = csv.reader(lines)\n",
    "    header = next(reader)\n",
    "\n",
    "    n_expected = len(header)\n",
    "    n_front = 6                    # ACCTNO, status, TRANCD, description, TRDATE, AMT\n",
    "    n_tail  = n_expected - 6 - 2   # everything after EFHDS2 (Account#, PostingDate, etc.)\n",
    "\n",
    "    rows = []\n",
    "    n_fixed = 0\n",
    "\n",
    "    reader = csv.reader(lines[1:])  # skip header\n",
    "    for line_num, fields in enumerate(reader, start=2):\n",
    "        n = len(fields)\n",
    "\n",
    "        if n == n_expected:\n",
    "            # Normal row\n",
    "            rows.append(fields)\n",
    "\n",
    "        elif n > n_expected:\n",
    "            # Extra commas inside EFHDS1/EFHDS2 area\n",
    "            front  = fields[:n_front]                    # first 6\n",
    "            tail   = fields[n - n_tail:]                 # last N\n",
    "            middle = fields[n_front : n - n_tail]        # everything between\n",
    "\n",
    "            # EFHDS2 = last token in the middle group\n",
    "            # EFHDS1 = everything else, re-joined with commas\n",
    "            efhds1 = \",\".join(middle[:-1])\n",
    "            efhds2 = middle[-1]\n",
    "\n",
    "            rows.append(front + [efhds1, efhds2] + tail)\n",
    "            n_fixed += 1\n",
    "\n",
    "        # else: fewer fields than expected → skip (shouldn't happen)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=header)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df[\"TRANCD\"] = df[\"TRANCD\"].astype(str).str.strip()\n",
    "\n",
    "    total = len(df)\n",
    "    print(f\"{label} rows: {total:,}\")\n",
    "    print(f\"  → Rows with commas in EFHDS1 (fixed): {n_fixed:,}\")\n",
    "    print(f\"{label} unique TRANCD: {df['TRANCD'].nunique()}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231b8cbb-4052-4d72-a6f0-6321c8351451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Section B — Transaction Code Catalog\n",
    "\n",
    "Build a catalog of unique transaction codes from the raw NON_POS and POS daily files.\n",
    "For each code, capture one sample description and the total transaction volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fc59ca3-0391-4715-87a0-c3a6871880dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_non_pos = load_raw_csv(RAW_NON_POS_PATH, label=\"NON_POS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c8b0885-21be-43fa-bf53-fc8fb113c208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(df_non_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5cd2e3a-af37-4cf1-a771-e37d53375ec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pos = load_raw_csv(RAW_POS_PATH, label=\"POS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55246eae-b363-4939-8504-27c6902b9fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Build NON_POS catalog entries ─────────────────────────────────\n",
    "# EFHDS1 is the primary description field (the `description` column is always empty)\n",
    "non_pos_catalog = (\n",
    "    df_non_pos\n",
    "    .groupby(\"TRANCD\")\n",
    "    .agg(\n",
    "        sample_desc_1=(\"EFHDS1\", \"first\"),\n",
    "        volume=(\"TRANCD\", \"size\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "non_pos_catalog[\"source_file\"] = \"NON_POS\"\n",
    "non_pos_catalog[\"sample_desc_1\"] = non_pos_catalog[\"sample_desc_1\"].astype(str).str.strip()\n",
    "\n",
    "print(f\"NON_POS catalog: {len(non_pos_catalog)} unique codes\")\n",
    "non_pos_catalog.sort_values(\"volume\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed3d881d-b4ab-40da-a7e0-e17b9417778a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Build POS catalog entries ─────────────────────────────────────\n",
    "# POS has a populated `description` column (unlike NON_POS)\n",
    "pos_catalog = (\n",
    "    df_pos\n",
    "    .groupby(\"TRANCD\")\n",
    "    .agg(\n",
    "        sample_desc_1=(\"description\", \"first\"),\n",
    "        volume=(\"TRANCD\", \"size\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "pos_catalog[\"source_file\"] = \"POS\"\n",
    "pos_catalog[\"sample_desc_1\"] = pos_catalog[\"sample_desc_1\"].astype(str).str.strip()\n",
    "\n",
    "print(f\"POS catalog: {len(pos_catalog)} unique codes\")\n",
    "pos_catalog.sort_values(\"volume\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe3c4bb1-1f65-4d93-9868-aa90e85ab8d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Combine into a single catalog ─────────────────────────────────\n",
    "df_catalog = pd.concat([non_pos_catalog, pos_catalog], ignore_index=True)\n",
    "df_catalog[\"TRANCD\"] = df_catalog[\"TRANCD\"].astype(str)\n",
    "\n",
    "print(f\"Combined catalog: {len(df_catalog)} codes\")\n",
    "print(f\"Total transaction volume: {df_catalog['volume'].sum():,}\")\n",
    "df_catalog.sort_values(\"volume\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c60cb0e-3721-4ea5-abd4-fa61062b9feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Section C — Layer Assignment\n",
    "\n",
    "Assign each catalog code to a test layer based on how it appears in the ground truth:\n",
    "\n",
    "| Layer | Name | Rule |\n",
    "|-------|------|------|\n",
    "| 1 | Obvious | Exactly 1 unique (L1,L2,L3,L4) mapping in GT |\n",
    "| 2 | Ambiguous | 2+ distinct mappings in GT |\n",
    "| 3 | Unknown | TRANCD absent from GT entirely |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ead052a-e85f-47e0-aa3f-0e45a88edf6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count distinct mappings per TRANCD in ground truth\n",
    "gt_mapping_counts = (\n",
    "    df_gt\n",
    "    .groupby(\"TRANCD\")\n",
    "    .apply(lambda g: g[[\"gt_L1\", \"gt_L2\", \"gt_L3\", \"gt_L4\"]].drop_duplicates().shape[0])\n",
    "    .reset_index(name=\"n_mappings\")\n",
    ")\n",
    "\n",
    "multi_codes  = set(gt_mapping_counts.loc[gt_mapping_counts[\"n_mappings\"] > 1, \"TRANCD\"])\n",
    "single_codes = set(gt_mapping_counts.loc[gt_mapping_counts[\"n_mappings\"] == 1, \"TRANCD\"])\n",
    "all_gt_codes = set(df_gt[\"TRANCD\"].unique())\n",
    "\n",
    "print(f\"GT codes with 1 mapping (Layer 1):  {len(single_codes)}\")\n",
    "print(f\"GT codes with 2+ mappings (Layer 2): {len(multi_codes)}\")\n",
    "print(f\"Multi-mapping codes: {sorted(multi_codes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96bf8997-60de-467b-aa27-c9f8349df3c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def assign_layer(trancd):\n",
    "    if trancd not in all_gt_codes:\n",
    "        return 3  # Unknown\n",
    "    if trancd in multi_codes:\n",
    "        return 2  # Ambiguous\n",
    "    return 1      # Obvious\n",
    "\n",
    "\n",
    "df_catalog[\"layer\"] = df_catalog[\"TRANCD\"].apply(assign_layer)\n",
    "\n",
    "# ── Summary ───────────────────────────────────────────────────────\n",
    "layer_summary = (\n",
    "    df_catalog\n",
    "    .groupby(\"layer\")\n",
    "    .agg(codes=(\"TRANCD\", \"nunique\"), total_volume=(\"volume\", \"sum\"))\n",
    "    .reset_index()\n",
    ")\n",
    "layer_summary[\"pct_volume\"] = (\n",
    "    layer_summary[\"total_volume\"] / layer_summary[\"total_volume\"].sum() * 100\n",
    ").round(1)\n",
    "\n",
    "layer_names = {1: \"Obvious\", 2: \"Ambiguous\", 3: \"Unknown\"}\n",
    "layer_summary[\"name\"] = layer_summary[\"layer\"].map(layer_names)\n",
    "\n",
    "print(\"Layer assignment summary:\")\n",
    "print(layer_summary[[\"layer\", \"name\", \"codes\", \"total_volume\", \"pct_volume\"]].to_string(index=False))\n",
    "print(f\"\\nTotal codes: {len(df_catalog)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb5531e0-8e64-488d-a916-4e667ff3878f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Show codes per layer ──────────────────────────────────────────\n",
    "for layer_num, layer_name in layer_names.items():\n",
    "    layer_df = df_catalog[df_catalog[\"layer\"] == layer_num].sort_values(\"volume\", ascending=False)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Layer {layer_num} — {layer_name} ({len(layer_df)} codes)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for _, row in layer_df.iterrows():\n",
    "        print(f\"  TRANCD={row['TRANCD']:>5} | vol={row['volume']:>7,} | {row['source_file']:<7} | {str(row['sample_desc_1'])[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8cf6e14-a43a-42d5-9556-fa5dd3c829b8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 22"
    }
   },
   "outputs": [],
   "source": [
    "# ── Save catalog (with layers) to Unity Catalog ───────────────────\n",
    "try:\n",
    "    df_catalog_to_save = df_catalog.copy()\n",
    "    df_catalog_to_save[\"TRANCD\"] = df_catalog_to_save[\"TRANCD\"].astype(int)\n",
    "    \n",
    "    sdf_catalog = spark.createDataFrame(df_catalog_to_save)\n",
    "    sdf_catalog.write.mode(\"overwrite\").saveAsTable(CATALOG_TABLE)\n",
    "    print(f\"Saved {len(df_catalog)} rows to {CATALOG_TABLE}\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC write (run in Databricks).\")\n",
    "    print(f\"DataFrame ready with {len(df_catalog)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9670d39b-900d-4268-bd97-9179cd1b3340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Validation\n",
    "\n",
    "Verify that all Unity Catalog tables were created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e130f221-55b3-451a-9b11-1773154c918c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Validate UC tables ────────────────────────────────────────────\n",
    "try:\n",
    "    for table_name, expected_label in [\n",
    "        (GT_TABLE, \"ground_truth_normalized\"),\n",
    "        (CATALOG_TABLE, \"transaction_code_catalog\"),\n",
    "    ]:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {table_name}\").collect()[0][\"cnt\"]\n",
    "        print(f\"  OK  {expected_label}: {count} rows\")\n",
    "\n",
    "    # Verify the catalog has a layer column\n",
    "    catalog_cols = [f.name for f in spark.table(CATALOG_TABLE).schema.fields]\n",
    "    assert \"layer\" in catalog_cols, \"Missing 'layer' column in catalog table\"\n",
    "    print(f\"  OK  catalog has 'layer' column\")\n",
    "\n",
    "    print(\"\\nAll validations passed.\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC validation (run in Databricks).\")\n",
    "    print(\"Local DataFrames are ready:\")\n",
    "    print(f\"  df_gt:      {len(df_gt)} rows, {df_gt['TRANCD'].nunique()} unique codes\")\n",
    "    print(f\"  df_catalog: {len(df_catalog)} rows, columns: {list(df_catalog.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_prepare_data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
