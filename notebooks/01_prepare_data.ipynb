{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acdd4fa3-3c0f-4a7b-a000-d25df2829193",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 01 — Prepare Data\n",
    "\n",
    "**Objective:** Build the three foundational Unity Catalog tables that the rest of the pipeline depends on.\n",
    "\n",
    "| Section | Output Table | Description |\n",
    "|---------|-------------|-------------|\n",
    "| **A** | `ground_truth_normalized` | Cleaned, normalized Master Fee Table (431 codes) |\n",
    "| **B.0** | `client_column_mappings` | AI-generated mapping from client columns to canonical schema |\n",
    "| **B** | `transaction_code_catalog` | Unique standardized codes from raw data with descriptions and volume |\n",
    "| **C** | Layer assignment | Adds `layer` column to the catalog (Obvious / Ambiguous / Unknown) |\n",
    "\n",
    "**Runs on:** Databricks Runtime 15.4 LTS or above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec9d4529-101d-4413-a633-73b96520c469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Configuration ─────────────────────────────────────────────────\n",
    "CATALOG_NAME = \"ciq-bp_dummy-dev\"\n",
    "SCHEMA_NAME  = \"default\"\n",
    "MODEL_NAME   = \"databricks-claude-opus-4-6\"\n",
    "\n",
    "GT_PATH          = \"../data/bank-plus-data/source-of-truth/Master Fee Table(Master).csv\"\n",
    "RAW_NON_POS_PATH = \"../data/bank-plus-data/raw/CheckingIQ_NON_POS_Daily_012626_rerun.csv\"\n",
    "RAW_POS_PATH     = \"../data/bank-plus-data/raw/CheckingIQ_POS_Daily_012626_rerun.csv\"\n",
    "\n",
    "GT_TABLE       = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.ground_truth_normalized\"\n",
    "CATALOG_TABLE  = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.transaction_code_catalog\"\n",
    "MAPPINGS_TABLE = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.client_column_mappings\"\n",
    "\n",
    "# Generic canonical schema — universal column names for ALL clients.\n",
    "CANONICAL_SCHEMA = {\n",
    "    \"transaction_code\":  \"Numeric transaction code (primary key for categorization)\",\n",
    "    \"description_1\":     \"Primary transaction description text\",\n",
    "    \"description_2\":     \"Secondary description / memo line\",\n",
    "    \"amount\":            \"Transaction amount\",\n",
    "    \"transaction_date\":  \"Date the transaction occurred\",\n",
    "    \"posting_date\":      \"Date the transaction was posted\",\n",
    "    \"account_number\":    \"Account identifier\",\n",
    "    \"account_status\":    \"Account status code\",\n",
    "    \"internal_account\":  \"Internal/alternate account number\",\n",
    "    \"transaction_desc\":  \"Transaction type description (may be empty)\",\n",
    "}\n",
    "\n",
    "print(f\"Catalog:        {CATALOG_NAME}\")\n",
    "print(f\"Schema:         {SCHEMA_NAME}\")\n",
    "print(f\"GT table:       {GT_TABLE}\")\n",
    "print(f\"Catalog table:  {CATALOG_TABLE}\")\n",
    "print(f\"Mappings table: {MAPPINGS_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d5a3889-6b11-43d3-a7c5-5d0fe3dfacab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Validation: check that input files exist ─────────────────────\n",
    "import os\n",
    "\n",
    "for path, label in [\n",
    "    (GT_PATH, \"Master Fee Table\"),\n",
    "    (RAW_NON_POS_PATH, \"NON_POS raw data\"),\n",
    "    (RAW_POS_PATH, \"POS raw data\"),\n",
    "]:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"  OK  {label}: {path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Missing {label}: {path}\")\n",
    "\n",
    "print(\"\\nAll input files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf956b1-d81b-42c3-bf67-1de3820698f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Section A — Ground Truth Normalization\n",
    "\n",
    "Read the Master Fee Table, clean it, normalize casing inconsistencies, and save to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba2bc0b5-532d-4835-8566-0caeee478e08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_gt_raw = pd.read_csv(GT_PATH, encoding=\"latin-1\")\n",
    "\n",
    "# Strip whitespace from column names\n",
    "df_gt_raw.columns = [c.strip() for c in df_gt_raw.columns]\n",
    "\n",
    "print(f\"Raw rows loaded: {len(df_gt_raw)}\")\n",
    "print(f\"Columns: {list(df_gt_raw.columns)}\")\n",
    "df_gt_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a015948b-6bde-4b0c-bcff-a53dc37fc49d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gt = df_gt_raw.copy()\n",
    "\n",
    "# Strip whitespace from all string cells\n",
    "for col in df_gt.columns:\n",
    "    if df_gt[col].dtype == object:\n",
    "        df_gt[col] = df_gt[col].astype(str).str.strip()\n",
    "\n",
    "# Drop rows where External Transaction Code is empty or non-numeric\n",
    "# These are section headers (e.g. \"ATM activities\"), generic descriptions\n",
    "# (no transaction_code), and trailing blank rows.\n",
    "df_gt = df_gt[\n",
    "    df_gt[\"External Transaction Code\"].notna()\n",
    "    & (df_gt[\"External Transaction Code\"].astype(str).str.strip() != \"\")\n",
    "    & (df_gt[\"External Transaction Code\"].astype(str).str.strip() != \"nan\")\n",
    "].copy()\n",
    "\n",
    "df_gt[\"External Transaction Code\"] = df_gt[\"External Transaction Code\"].astype(str).str.strip()\n",
    "\n",
    "# Drop header-leak rows (the header row repeated mid-file at the Fee Items boundary)\n",
    "df_gt = df_gt[\n",
    "    df_gt[\"Scoring Category 1\"].astype(str).str.strip() != \"Scoring Category 1\"\n",
    "].copy()\n",
    "\n",
    "# Keep only rows where the transaction_code is a valid integer\n",
    "df_gt = df_gt[\n",
    "    df_gt[\"External Transaction Code\"].str.match(r\"^\\d+$\")\n",
    "].copy()\n",
    "\n",
    "print(f\"After cleaning: {len(df_gt)} rows, {df_gt['External Transaction Code'].nunique()} unique codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d6ae179-2dab-4bf9-80b9-56b8f133a199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Normalization maps ────────────────────────────────────────────\n",
    "# Fix casing inconsistencies found in the spreadsheet.\n",
    "\n",
    "L1_NORM = {\n",
    "    \"Fee Item\":  \"Fee item\",\n",
    "    \"Fee item\":  \"Fee item\",\n",
    "    \"Non-fee item\": \"Non-fee item\",\n",
    "}\n",
    "\n",
    "L2_NORM = {\n",
    "    \"NSF /OD\":            \"NSF/OD\",\n",
    "    \"NSF/OD\":             \"NSF/OD\",\n",
    "    \"Money Movement\":     \"Money movement\",\n",
    "    \"Money movement\":     \"Money movement\",\n",
    "    \"Account Operations\": \"Account operations\",\n",
    "    \"Account operations\": \"Account operations\",\n",
    "    \"All others\":         \"All others\",\n",
    "    \"Service Charges\":    \"Service Charges\",\n",
    "    \"Interchange\":        \"Interchange\",\n",
    "    \"Miscellaneous\":      \"Miscellaneous\",\n",
    "    \"Unclassified\":       \"Unclassified\",\n",
    "}\n",
    "\n",
    "L3_NORM = {\n",
    "    \"N/A\": None,\n",
    "    \"nan\": None,\n",
    "    \"Money Movement\":     \"Money movement\",\n",
    "    \"Account Operations\": \"Account operations\",\n",
    "}\n",
    "\n",
    "L4_NORM = {\n",
    "    \"N/A\": None,\n",
    "    \"nan\": None,\n",
    "}\n",
    "\n",
    "\n",
    "def _apply_map(series, norm_map):\n",
    "    \"\"\"Map values through a normalization dict, keeping unmapped values as-is.\"\"\"\n",
    "    mapped = series.map(norm_map)\n",
    "    # Where the map returned a value (including explicit None), use it.\n",
    "    # Where the key was not in the map, keep the original.\n",
    "    has_mapping = series.isin(norm_map.keys())\n",
    "    return mapped.where(has_mapping, series)\n",
    "\n",
    "\n",
    "df_gt[\"Scoring Category 1\"] = _apply_map(df_gt[\"Scoring Category 1\"], L1_NORM)\n",
    "df_gt[\"Scoring Category 2\"] = _apply_map(df_gt[\"Scoring Category 2\"], L2_NORM)\n",
    "df_gt[\"Scoring Category 3\"] = _apply_map(df_gt[\"Scoring Category 3\"], L3_NORM)\n",
    "df_gt[\"Scoring Category 4\"] = _apply_map(df_gt[\"Scoring Category 4\"], L4_NORM)\n",
    "\n",
    "# Drop rows where L1 ended up as None (shouldn't happen after cleaning, but safety)\n",
    "df_gt = df_gt[df_gt[\"Scoring Category 1\"].notna()].copy()\n",
    "\n",
    "print(\"After normalization:\")\n",
    "print(f\"  L1 values: {sorted(df_gt['Scoring Category 1'].dropna().unique())}\")\n",
    "print(f\"  L2 values: {sorted(df_gt['Scoring Category 2'].dropna().unique())}\")\n",
    "print(f\"  L3 values: {sorted(df_gt['Scoring Category 3'].dropna().unique())}\")\n",
    "l4_vals = df_gt['Scoring Category 4'].dropna().unique()\n",
    "print(f\"  L4 values: {sorted([v for v in l4_vals if v is not None])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d526d9-0a32-4d8f-96f6-3a4160373da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Rename to canonical column names ──────────────────────────────\n",
    "df_gt = df_gt.rename(columns={\n",
    "    \"External Transaction Code\":        \"transaction_code\",\n",
    "    \"External Transaction Description\": \"gt_desc\",\n",
    "    \"Scoring Category 1\":               \"gt_L1\",\n",
    "    \"Scoring Category 2\":               \"gt_L2\",\n",
    "    \"Scoring Category 3\":               \"gt_L3\",\n",
    "    \"Scoring Category 4\":               \"gt_L4\",\n",
    "    \"Credit / Debit\":                   \"gt_credit_debit\",\n",
    "})\n",
    "\n",
    "gt_cols = [\"transaction_code\", \"gt_desc\", \"gt_L1\", \"gt_L2\", \"gt_L3\", \"gt_L4\", \"gt_credit_debit\"]\n",
    "df_gt = df_gt[gt_cols].copy()\n",
    "\n",
    "# Replace remaining string 'None' / 'nan' artifacts\n",
    "df_gt.replace({\"None\": None, \"nan\": None}, inplace=True)\n",
    "\n",
    "print(f\"Ground truth ready: {len(df_gt)} rows, {df_gt['transaction_code'].nunique()} unique codes\")\n",
    "df_gt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f824c153-da08-451f-9e08-cee75381f498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Save ground truth to Unity Catalog ────────────────────────────\n",
    "try:\n",
    "    sdf_gt = spark.createDataFrame(df_gt)\n",
    "    sdf_gt.write.mode(\"overwrite\").saveAsTable(GT_TABLE)\n",
    "    print(f\"Saved {len(df_gt)} rows to {GT_TABLE}\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC write (run in Databricks).\")\n",
    "    print(f\"DataFrame ready with {len(df_gt)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "202db629-be33-4dd2-9623-fe13a9f66f8c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 10"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_raw_csv(path, label=\"\"):\n",
    "    \"\"\"\n",
    "    Robust CSV loader for BankPlus raw files.\n",
    "    \n",
    "    Handles unquoted commas in EFHDS1/EFHDS2 by parsing from both ends:\n",
    "      - Front 6 columns (ACCTNO → AMT) are always safe\n",
    "      - Tail columns (Account# → end) are always safe\n",
    "      - Middle = EFHDS1 + EFHDS2, may contain extra commas\n",
    "    \n",
    "    Also filters out NUL bytes (\\x00) which are invalid in CSV files.\n",
    "    \"\"\"\n",
    "    # Read file and strip NUL bytes\n",
    "    with open(path, \"rb\") as f:\n",
    "        raw_bytes = f.read()\n",
    "    \n",
    "    # Remove NUL bytes\n",
    "    cleaned_bytes = raw_bytes.replace(b'\\x00', b'')\n",
    "    \n",
    "    # Decode to string and split into lines for csv.reader\n",
    "    text = cleaned_bytes.decode('utf-8', errors='replace')\n",
    "    lines = text.splitlines()\n",
    "    \n",
    "    # Parse header\n",
    "    reader = csv.reader(lines)\n",
    "    header = next(reader)\n",
    "\n",
    "    n_expected = len(header)\n",
    "    n_front = 6                    # ACCTNO, status, TRANCD, description, TRDATE, AMT\n",
    "    n_tail  = n_expected - 6 - 2   # everything after EFHDS2 (Account#, PostingDate, etc.)\n",
    "\n",
    "    rows = []\n",
    "    n_fixed = 0\n",
    "\n",
    "    reader = csv.reader(lines[1:])  # skip header\n",
    "    for line_num, fields in enumerate(reader, start=2):\n",
    "        n = len(fields)\n",
    "\n",
    "        if n == n_expected:\n",
    "            # Normal row\n",
    "            rows.append(fields)\n",
    "\n",
    "        elif n > n_expected:\n",
    "            # Extra commas inside EFHDS1/EFHDS2 area\n",
    "            front  = fields[:n_front]                    # first 6\n",
    "            tail   = fields[n - n_tail:]                 # last N\n",
    "            middle = fields[n_front : n - n_tail]        # everything between\n",
    "\n",
    "            # EFHDS2 = last token in the middle group\n",
    "            # EFHDS1 = everything else, re-joined with commas\n",
    "            efhds1 = \",\".join(middle[:-1])\n",
    "            efhds2 = middle[-1]\n",
    "\n",
    "            rows.append(front + [efhds1, efhds2] + tail)\n",
    "            n_fixed += 1\n",
    "\n",
    "        # else: fewer fields than expected → skip (shouldn't happen)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=header)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # Apply standardized column mapping if available\n",
    "    if 'CLIENT_RENAME_MAP' in globals():\n",
    "        df = df.rename(columns=CLIENT_RENAME_MAP)\n",
    "        trancd_col = \"transaction_code\"\n",
    "    else:\n",
    "        trancd_col = \"TRANCD\"\n",
    "\n",
    "    df[trancd_col] = df[trancd_col].astype(str).str.strip()\n",
    "\n",
    "    total = len(df)\n",
    "    print(f\"{label} rows: {total:,}\")\n",
    "    print(f\"  → Rows with commas in EFHDS1 (fixed): {n_fixed:,}\")\n",
    "    print(f\"{label} unique codes: {df[trancd_col].nunique()}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231b8cbb-4052-4d72-a6f0-6321c8351451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Section B.0 — AI Column Mapping\n",
    "\n",
    "Identify the mapping from client-specific column names to the generic canonical schema\n",
    "using `ai_query()`. This allows the pipeline to handle different core banking systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0.1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "def get_column_profiles(path, n_rows=20):\n",
    "    \"\"\"Load a sample and infer basic column types for the AI prompt.\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "    \n",
    "    header = [c.strip() for c in header]\n",
    "    df_sample = pd.read_csv(path, nrows=n_rows, dtype=str, encoding='utf-8', on_bad_lines='skip')\n",
    "    df_sample.columns = [c.strip() for c in df_sample.columns]\n",
    "\n",
    "    profiles = []\n",
    "    for col in df_sample.columns:\n",
    "        vals = df_sample[col].dropna().astype(str).str.strip()\n",
    "        vals = vals[vals != \"\"].head(5).tolist()\n",
    "        dtype = \"text\"\n",
    "        if all(v.replace(\".\", \"\").replace(\"-\", \"\").isdigit() for v in vals if v):\n",
    "            if any(\".\" in v for v in vals): dtype = \"decimal numbers\"\n",
    "            elif all(len(v) <= 3 for v in vals): dtype = \"small integers\"\n",
    "            else: dtype = \"integers\"\n",
    "        profiles.append(f\"  - Column: \\\"{col}\\\"\\n    Data type: {dtype}\\n    Sample values: {vals}\")\n",
    "    return \"\\n\".join(profiles), list(df_sample.columns)\n",
    "\n",
    "print(\"Mapping column profiles from NON_POS data...\")\n",
    "profiles_text, client_cols = get_column_profiles(RAW_NON_POS_PATH)\n",
    "\n",
    "canonical_schema_text = \"\\n\".join(f\"  - {col}: {desc}\" for col, desc in CANONICAL_SCHEMA.items())\n",
    "\n",
    "mapping_prompt = f\"\"\"You are a banking data expert. Map client columns to our canonical schema.\n",
    "### Canonical Schema\\n{canonical_schema_text}\\n\n",
    "### Client Data Profile\\n{profiles_text}\\n\n",
    "### Instructions\\n1. Map each client column to ONE canonical column or null.\\n2. Return ONLY a JSON object: {{client_col: canonical_col}}.\n",
    "\"\"\"\n",
    "print(\"Prompt prepared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0.2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "escaped_prompt = mapping_prompt.replace(\"'\", \"''\")\n",
    "def sanitize(c): return re.sub(r'[^a-zA-Z0-9_.-]', '_', c)\n",
    "san_to_orig = {sanitize(c): c for c in client_cols}\n",
    "props = {sanitize(c): {\"type\": [\"string\", \"null\"]} for c in client_cols}\n",
    "schema = json.dumps({\"type\": \"json_schema\", \"json_schema\": {\"name\": \"mapping\", \"schema\": {\"type\": \"object\", \"properties\": props}, \"strict\": True}})\n",
    "\n",
    "query = f\"SELECT ai_query('{MODEL_NAME}', '{escaped_prompt}', responseFormat => '{schema}') as res\"\n",
    "try:\n",
    "    print(\"Calling ai_query...\")\n",
    "    res_raw = spark.sql(query).collect()[0][\"res\"]\n",
    "    ai_mapping_san = json.loads(res_raw)\n",
    "    CLIENT_RENAME_MAP = {san_to_orig[k]: v for k, v in ai_mapping_san.items() if v}\n",
    "    print(f\"Mapping derived for {len(CLIENT_RENAME_MAP)} columns.\")\n",
    "except NameError:\n",
    "    print(\"Spark not found — using hardcoded Bank Plus mapping.\")\n",
    "    CLIENT_RENAME_MAP = {\"TRANCD\": \"transaction_code\", \"EFHDS1\": \"description_1\", \"EFHDS2\": \"description_2\", \"AMT\": \"amount\", \"TRDATE\": \"transaction_date\", \"PostingDate\": \"posting_date\", \"ACCTNO\": \"account_number\", \"status\": \"account_status\", \"Account#\": \"internal_account\", \"description\": \"transaction_desc\"}\n",
    "\n",
    "print(\"Final Rename Map:\")\n",
    "for k, v in CLIENT_RENAME_MAP.items(): print(f\"  {k:<15} -> {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0.3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "mapping_df = pd.DataFrame([{\"client_column\": k, \"canonical_column\": v, \"run_timestamp\": datetime.utcnow().isoformat()} for k, v in CLIENT_RENAME_MAP.items()])\n",
    "try:\n",
    "    spark.createDataFrame(mapping_df).write.mode(\"overwrite\").saveAsTable(MAPPINGS_TABLE)\n",
    "    print(f\"Saved mapping to {MAPPINGS_TABLE}\")\n",
    "except NameError: print(\"Skipping UC write.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231b8cbb-4052-4d72-a6f0-6321c8351451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Section B — Transaction Code Catalog\n",
    "\n",
    "Build a catalog of unique transaction codes from the raw NON_POS and POS daily files.\n",
    "For each code, capture one sample description and the total transaction volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fc59ca3-0391-4715-87a0-c3a6871880dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_non_pos = load_raw_csv(RAW_NON_POS_PATH, label=\"NON_POS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c8b0885-21be-43fa-bf53-fc8fb113c208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(df_non_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5cd2e3a-af37-4cf1-a771-e37d53375ec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pos = load_raw_csv(RAW_POS_PATH, label=\"POS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55246eae-b363-4939-8504-27c6902b9fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Build NON_POS catalog entries ─────────────────────────────────\n",
    "# description_1 is the primary description field (mapped from EFHDS1 for Bank Plus)\n",
    "non_pos_catalog = (\n",
    "    df_non_pos\n",
    "    .groupby(\"transaction_code\")\n",
    "    .agg(\n",
    "        description_1=(\"description_1\", \"first\"),\n",
    "        volume=(\"transaction_code\", \"size\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "non_pos_catalog[\"source_file\"] = \"NON_POS\"\n",
    "non_pos_catalog[\"description_1\"] = non_pos_catalog[\"description_1\"].astype(str).str.strip()\n",
    "\n",
    "print(f\"NON_POS catalog: {len(non_pos_catalog)} unique codes\")\n",
    "non_pos_catalog.sort_values(\"volume\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed3d881d-b4ab-40da-a7e0-e17b9417778a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Build POS catalog entries ─────────────────────────────────────\n",
    "# transaction_desc is the populated description column for POS\n",
    "pos_catalog = (\n",
    "    df_pos\n",
    "    .groupby(\"transaction_code\")\n",
    "    .agg(\n",
    "        description_1=(\"transaction_desc\", \"first\"),\n",
    "        volume=(\"transaction_code\", \"size\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "pos_catalog[\"source_file\"] = \"POS\"\n",
    "pos_catalog[\"description_1\"] = pos_catalog[\"description_1\"].astype(str).str.strip()\n",
    "\n",
    "print(f\"POS catalog: {len(pos_catalog)} unique codes\")\n",
    "pos_catalog.sort_values(\"volume\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe3c4bb1-1f65-4d93-9868-aa90e85ab8d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Combine into a single catalog ─────────────────────────────────\n",
    "df_catalog = pd.concat([non_pos_catalog, pos_catalog], ignore_index=True)\n",
    "df_catalog[\"transaction_code\"] = df_catalog[\"transaction_code\"].astype(str)\n",
    "\n",
    "print(f\"Combined catalog: {len(df_catalog)} codes\")\n",
    "print(f\"Total transaction volume: {df_catalog['volume'].sum():,}\")\n",
    "df_catalog.sort_values(\"volume\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c60cb0e-3721-4ea5-abd4-fa61062b9feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Section C — Layer Assignment\n",
    "\n",
    "Assign each catalog code to a test layer based on how it appears in the ground truth:\n",
    "\n",
    "| Layer | Name | Rule |\n",
    "|-------|------|------|\n",
    "| 1 | Obvious | Exactly 1 unique (L1,L2,L3,L4) mapping in GT |\n",
    "| 2 | Ambiguous | 2+ distinct mappings in GT |\n",
    "| 3 | Unknown | transaction_code absent from GT entirely |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ead052a-e85f-47e0-aa3f-0e45a88edf6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count distinct mappings per transaction_code in ground truth\n",
    "gt_mapping_counts = (\n",
    "    df_gt\n",
    "    .groupby(\"transaction_code\")\n",
    "    .apply(lambda g: g[[\"gt_L1\", \"gt_L2\", \"gt_L3\", \"gt_L4\"]].drop_duplicates().shape[0])\n",
    "    .reset_index(name=\"n_mappings\")\n",
    ")\n",
    "\n",
    "multi_codes  = set(gt_mapping_counts.loc[gt_mapping_counts[\"n_mappings\"] > 1, \"transaction_code\"])\n",
    "single_codes = set(gt_mapping_counts.loc[gt_mapping_counts[\"n_mappings\"] == 1, \"transaction_code\"])\n",
    "all_gt_codes = set(df_gt[\"transaction_code\"].unique())\n",
    "\n",
    "print(f\"GT codes with 1 mapping (Layer 1):  {len(single_codes)}\")\n",
    "print(f\"GT codes with 2+ mappings (Layer 2): {len(multi_codes)}\")\n",
    "print(f\"Multi-mapping codes: {sorted(multi_codes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96bf8997-60de-467b-aa27-c9f8349df3c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def assign_layer(trancd):\n",
    "    if trancd not in all_gt_codes:\n",
    "        return 3  # Unknown\n",
    "    if trancd in multi_codes:\n",
    "        return 2  # Ambiguous\n",
    "    return 1      # Obvious\n",
    "\n",
    "\n",
    "df_catalog[\"layer\"] = df_catalog[\"transaction_code\"].apply(assign_layer)\n",
    "\n",
    "# ── Summary ───────────────────────────────────────────────────────\n",
    "layer_summary = (\n",
    "    df_catalog\n",
    "    .groupby(\"layer\")\n",
    "    .agg(codes=(\"transaction_code\", \"nunique\"), total_volume=(\"volume\", \"sum\"))\n",
    "    .reset_index()\n",
    ")\n",
    "layer_summary[\"pct_volume\"] = (\n",
    "    layer_summary[\"total_volume\"] / layer_summary[\"total_volume\"].sum() * 100\n",
    ").round(1)\n",
    "\n",
    "layer_names = {1: \"Obvious\", 2: \"Ambiguous\", 3: \"Unknown\"}\n",
    "layer_summary[\"name\"] = layer_summary[\"layer\"].map(layer_names)\n",
    "\n",
    "print(\"Layer assignment summary:\")\n",
    "print(layer_summary[[\"layer\", \"name\", \"codes\", \"total_volume\", \"pct_volume\"]].to_string(index=False))\n",
    "print(f\"\\nTotal codes: {len(df_catalog)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb5531e0-8e64-488d-a916-4e667ff3878f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Show codes per layer ──────────────────────────────────────────\n",
    "for layer_num, layer_name in layer_names.items():\n",
    "    layer_df = df_catalog[df_catalog[\"layer\"] == layer_num].sort_values(\"volume\", ascending=False)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Layer {layer_num} — {layer_name} ({len(layer_df)} codes)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for _, row in layer_df.iterrows():\n",
    "        print(f\"  transaction_code={row['transaction_code']:>5} | vol={row['volume']:>7,} | {row['source_file']:<7} | {str(row['description_1'])[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8cf6e14-a43a-42d5-9556-fa5dd3c829b8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 22"
    }
   },
   "outputs": [],
   "source": [
    "# ── Save catalog (with layers) to Unity Catalog ───────────────────\n",
    "try:\n",
    "    df_catalog_to_save = df_catalog.copy()\n",
    "    df_catalog_to_save[\"transaction_code\"] = df_catalog_to_save[\"transaction_code\"].astype(int)\n",
    "    \n",
    "    sdf_catalog = spark.createDataFrame(df_catalog_to_save)\n",
    "    sdf_catalog.write.mode(\"overwrite\").saveAsTable(CATALOG_TABLE)\n",
    "    print(f\"Saved {len(df_catalog)} rows to {CATALOG_TABLE}\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC write (run in Databricks).\")\n",
    "    print(f\"DataFrame ready with {len(df_catalog)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9670d39b-900d-4268-bd97-9179cd1b3340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Validation\n",
    "\n",
    "Verify that all Unity Catalog tables were created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e130f221-55b3-451a-9b11-1773154c918c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Validate UC tables ────────────────────────────────────────────\n",
    "try:\n",
    "    for table_name, expected_label in [\n",
    "        (GT_TABLE, \"ground_truth_normalized\"),\n",
    "        (CATALOG_TABLE, \"transaction_code_catalog\"),\n",
    "    ]:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {table_name}\").collect()[0][\"cnt\"]\n",
    "        print(f\"  OK  {expected_label}: {count} rows\")\n",
    "\n",
    "    # Verify the catalog has a layer column\n",
    "    catalog_cols = [f.name for f in spark.table(CATALOG_TABLE).schema.fields]\n",
    "    assert \"layer\" in catalog_cols, \"Missing 'layer' column in catalog table\"\n",
    "    print(f\"  OK  catalog has 'layer' column\")\n",
    "\n",
    "    print(\"\\nAll validations passed.\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC validation (run in Databricks).\")\n",
    "    print(\"Local DataFrames are ready:\")\n",
    "    print(f\"  df_gt:      {len(df_gt)} rows, {df_gt['transaction_code'].nunique()} unique codes\")\n",
    "    print(f\"  df_catalog: {len(df_catalog)} rows, columns: {list(df_catalog.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_prepare_data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
