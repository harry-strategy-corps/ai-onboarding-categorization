{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 — Prepare Data\n",
    "\n",
    "**Objective:** Build the three foundational Unity Catalog tables that the rest of the pipeline depends on.\n",
    "\n",
    "| Section | Output Table | Description |\n",
    "|---------|-------------|-------------|\n",
    "| **A** | `ground_truth_normalized` | Cleaned, normalized Master Fee Table (431 codes) |\n",
    "| **B** | `transaction_code_catalog` | Unique TRANCD codes from raw data with sample descriptions and volume |\n",
    "| **C** | Layer assignment | Adds `layer` column to the catalog (Obvious / Ambiguous / Unknown) |\n",
    "\n",
    "**Runs on:** Databricks Runtime 15.4 LTS or above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Configuration ─────────────────────────────────────────────────\n",
    "CATALOG_NAME = \"ciq-bp_dummy-dev\"\n",
    "SCHEMA_NAME  = \"default\"\n",
    "\n",
    "GT_PATH          = \"../data/bank-plus-data/source-of-truth/Master Fee Table(Master).csv\"\n",
    "RAW_NON_POS_PATH = \"../data/bank-plus-data/raw/CheckingIQ_NON_POS_Daily_012626_rerun.csv\"\n",
    "RAW_POS_PATH     = \"../data/bank-plus-data/raw/CheckingIQ_POS_Daily_012626_rerun.csv\"\n",
    "\n",
    "GT_TABLE      = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.ground_truth_normalized\"\n",
    "CATALOG_TABLE = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.transaction_code_catalog\"\n",
    "\n",
    "print(f\"Catalog:        {CATALOG_NAME}\")\n",
    "print(f\"Schema:         {SCHEMA_NAME}\")\n",
    "print(f\"GT table:       {GT_TABLE}\")\n",
    "print(f\"Catalog table:  {CATALOG_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Validation: check that input files exist ─────────────────────\n",
    "import os\n",
    "\n",
    "for path, label in [\n",
    "    (GT_PATH, \"Master Fee Table\"),\n",
    "    (RAW_NON_POS_PATH, \"NON_POS raw data\"),\n",
    "    (RAW_POS_PATH, \"POS raw data\"),\n",
    "]:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"  OK  {label}: {path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Missing {label}: {path}\")\n",
    "\n",
    "print(\"\\nAll input files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section A — Ground Truth Normalization\n",
    "\n",
    "Read the Master Fee Table, clean it, normalize casing inconsistencies, and save to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_gt_raw = pd.read_csv(GT_PATH, encoding=\"latin-1\")\n",
    "\n",
    "# Strip whitespace from column names\n",
    "df_gt_raw.columns = [c.strip() for c in df_gt_raw.columns]\n",
    "\n",
    "print(f\"Raw rows loaded: {len(df_gt_raw)}\")\n",
    "print(f\"Columns: {list(df_gt_raw.columns)}\")\n",
    "df_gt_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt = df_gt_raw.copy()\n",
    "\n",
    "# Strip whitespace from all string cells\n",
    "for col in df_gt.columns:\n",
    "    if df_gt[col].dtype == object:\n",
    "        df_gt[col] = df_gt[col].astype(str).str.strip()\n",
    "\n",
    "# Drop rows where External Transaction Code is empty or non-numeric\n",
    "# These are section headers (e.g. \"ATM activities\"), generic descriptions\n",
    "# (no TRANCD), and trailing blank rows.\n",
    "df_gt = df_gt[\n",
    "    df_gt[\"External Transaction Code\"].notna()\n",
    "    & (df_gt[\"External Transaction Code\"].astype(str).str.strip() != \"\")\n",
    "    & (df_gt[\"External Transaction Code\"].astype(str).str.strip() != \"nan\")\n",
    "].copy()\n",
    "\n",
    "df_gt[\"External Transaction Code\"] = df_gt[\"External Transaction Code\"].astype(str).str.strip()\n",
    "\n",
    "# Drop header-leak rows (the header row repeated mid-file at the Fee Items boundary)\n",
    "df_gt = df_gt[\n",
    "    df_gt[\"Scoring Category 1\"].astype(str).str.strip() != \"Scoring Category 1\"\n",
    "].copy()\n",
    "\n",
    "# Keep only rows where the TRANCD is a valid integer\n",
    "df_gt = df_gt[\n",
    "    df_gt[\"External Transaction Code\"].str.match(r\"^\\d+$\")\n",
    "].copy()\n",
    "\n",
    "print(f\"After cleaning: {len(df_gt)} rows, {df_gt['External Transaction Code'].nunique()} unique codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Normalization maps ────────────────────────────────────────────\n",
    "# Fix casing inconsistencies found in the spreadsheet.\n",
    "\n",
    "L1_NORM = {\n",
    "    \"Fee Item\":  \"Fee item\",\n",
    "    \"Fee item\":  \"Fee item\",\n",
    "    \"Non-fee item\": \"Non-fee item\",\n",
    "}\n",
    "\n",
    "L2_NORM = {\n",
    "    \"NSF /OD\":            \"NSF/OD\",\n",
    "    \"NSF/OD\":             \"NSF/OD\",\n",
    "    \"Money Movement\":     \"Money movement\",\n",
    "    \"Money movement\":     \"Money movement\",\n",
    "    \"Account Operations\": \"Account operations\",\n",
    "    \"Account operations\": \"Account operations\",\n",
    "    \"All others\":         \"All others\",\n",
    "    \"Service Charges\":    \"Service Charges\",\n",
    "    \"Interchange\":        \"Interchange\",\n",
    "    \"Miscellaneous\":      \"Miscellaneous\",\n",
    "    \"Unclassified\":       \"Unclassified\",\n",
    "}\n",
    "\n",
    "L3_NORM = {\n",
    "    \"N/A\": None,\n",
    "    \"nan\": None,\n",
    "    \"Money Movement\":     \"Money movement\",\n",
    "    \"Account Operations\": \"Account operations\",\n",
    "}\n",
    "\n",
    "L4_NORM = {\n",
    "    \"N/A\": None,\n",
    "    \"nan\": None,\n",
    "}\n",
    "\n",
    "\n",
    "def _apply_map(series, norm_map):\n",
    "    \"\"\"Map values through a normalization dict, keeping unmapped values as-is.\"\"\"\n",
    "    mapped = series.map(norm_map)\n",
    "    # Where the map returned a value (including explicit None), use it.\n",
    "    # Where the key was not in the map, keep the original.\n",
    "    has_mapping = series.isin(norm_map.keys())\n",
    "    return mapped.where(has_mapping, series)\n",
    "\n",
    "\n",
    "df_gt[\"Scoring Category 1\"] = _apply_map(df_gt[\"Scoring Category 1\"], L1_NORM)\n",
    "df_gt[\"Scoring Category 2\"] = _apply_map(df_gt[\"Scoring Category 2\"], L2_NORM)\n",
    "df_gt[\"Scoring Category 3\"] = _apply_map(df_gt[\"Scoring Category 3\"], L3_NORM)\n",
    "df_gt[\"Scoring Category 4\"] = _apply_map(df_gt[\"Scoring Category 4\"], L4_NORM)\n",
    "\n",
    "# Drop rows where L1 ended up as None (shouldn't happen after cleaning, but safety)\n",
    "df_gt = df_gt[df_gt[\"Scoring Category 1\"].notna()].copy()\n",
    "\n",
    "print(\"After normalization:\")\n",
    "print(f\"  L1 values: {sorted(df_gt['Scoring Category 1'].dropna().unique())}\")\n",
    "print(f\"  L2 values: {sorted(df_gt['Scoring Category 2'].dropna().unique())}\")\n",
    "print(f\"  L3 values: {sorted(df_gt['Scoring Category 3'].dropna().unique())}\")\n",
    "l4_vals = df_gt['Scoring Category 4'].dropna().unique()\n",
    "print(f\"  L4 values: {sorted([v for v in l4_vals if v is not None])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Rename to canonical column names ──────────────────────────────\n",
    "df_gt = df_gt.rename(columns={\n",
    "    \"External Transaction Code\":        \"TRANCD\",\n",
    "    \"External Transaction Description\": \"gt_desc\",\n",
    "    \"Scoring Category 1\":               \"gt_L1\",\n",
    "    \"Scoring Category 2\":               \"gt_L2\",\n",
    "    \"Scoring Category 3\":               \"gt_L3\",\n",
    "    \"Scoring Category 4\":               \"gt_L4\",\n",
    "    \"Credit / Debit\":                   \"gt_credit_debit\",\n",
    "})\n",
    "\n",
    "gt_cols = [\"TRANCD\", \"gt_desc\", \"gt_L1\", \"gt_L2\", \"gt_L3\", \"gt_L4\", \"gt_credit_debit\"]\n",
    "df_gt = df_gt[gt_cols].copy()\n",
    "\n",
    "# Replace remaining string 'None' / 'nan' artifacts\n",
    "df_gt.replace({\"None\": None, \"nan\": None}, inplace=True)\n",
    "\n",
    "print(f\"Ground truth ready: {len(df_gt)} rows, {df_gt['TRANCD'].nunique()} unique codes\")\n",
    "df_gt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save ground truth to Unity Catalog ────────────────────────────\n",
    "try:\n",
    "    sdf_gt = spark.createDataFrame(df_gt)\n",
    "    sdf_gt.write.mode(\"overwrite\").saveAsTable(GT_TABLE)\n",
    "    print(f\"Saved {len(df_gt)} rows to {GT_TABLE}\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC write (run in Databricks).\")\n",
    "    print(f\"DataFrame ready with {len(df_gt)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section B — Transaction Code Catalog\n",
    "\n",
    "Build a catalog of unique transaction codes from the raw NON_POS and POS daily files.\n",
    "For each code, capture one sample description and the total transaction volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load raw NON_POS data ─────────────────────────────────────────\n",
    "df_non_pos = pd.read_csv(RAW_NON_POS_PATH, dtype={\"TRANCD\": str})\n",
    "df_non_pos.columns = [c.strip() for c in df_non_pos.columns]\n",
    "df_non_pos[\"TRANCD\"] = df_non_pos[\"TRANCD\"].astype(str).str.strip()\n",
    "\n",
    "print(f\"NON_POS rows: {len(df_non_pos):,}\")\n",
    "print(f\"NON_POS unique TRANCD: {df_non_pos['TRANCD'].nunique()}\")\n",
    "print(f\"Columns: {list(df_non_pos.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load raw POS data ─────────────────────────────────────────────\n",
    "df_pos = pd.read_csv(RAW_POS_PATH, dtype={\"TRANCD\": str})\n",
    "df_pos.columns = [c.strip() for c in df_pos.columns]\n",
    "df_pos[\"TRANCD\"] = df_pos[\"TRANCD\"].astype(str).str.strip()\n",
    "\n",
    "print(f\"POS rows: {len(df_pos):,}\")\n",
    "print(f\"POS unique TRANCD: {df_pos['TRANCD'].nunique()}\")\n",
    "print(f\"Columns: {list(df_pos.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Build NON_POS catalog entries ─────────────────────────────────\n",
    "# EFHDS1 is the primary description field (the `description` column is always empty)\n",
    "non_pos_catalog = (\n",
    "    df_non_pos\n",
    "    .groupby(\"TRANCD\")\n",
    "    .agg(\n",
    "        sample_desc_1=(\"EFHDS1\", \"first\"),\n",
    "        volume=(\"TRANCD\", \"size\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "non_pos_catalog[\"source_file\"] = \"NON_POS\"\n",
    "non_pos_catalog[\"sample_desc_1\"] = non_pos_catalog[\"sample_desc_1\"].astype(str).str.strip()\n",
    "\n",
    "print(f\"NON_POS catalog: {len(non_pos_catalog)} unique codes\")\n",
    "non_pos_catalog.sort_values(\"volume\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Build POS catalog entries ─────────────────────────────────────\n",
    "# POS has a populated `description` column (unlike NON_POS)\n",
    "pos_catalog = (\n",
    "    df_pos\n",
    "    .groupby(\"TRANCD\")\n",
    "    .agg(\n",
    "        sample_desc_1=(\"description\", \"first\"),\n",
    "        volume=(\"TRANCD\", \"size\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "pos_catalog[\"source_file\"] = \"POS\"\n",
    "pos_catalog[\"sample_desc_1\"] = pos_catalog[\"sample_desc_1\"].astype(str).str.strip()\n",
    "\n",
    "print(f\"POS catalog: {len(pos_catalog)} unique codes\")\n",
    "pos_catalog.sort_values(\"volume\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Combine into a single catalog ─────────────────────────────────\n",
    "df_catalog = pd.concat([non_pos_catalog, pos_catalog], ignore_index=True)\n",
    "df_catalog[\"TRANCD\"] = df_catalog[\"TRANCD\"].astype(str)\n",
    "\n",
    "print(f\"Combined catalog: {len(df_catalog)} codes\")\n",
    "print(f\"Total transaction volume: {df_catalog['volume'].sum():,}\")\n",
    "df_catalog.sort_values(\"volume\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section C — Layer Assignment\n",
    "\n",
    "Assign each catalog code to a test layer based on how it appears in the ground truth:\n",
    "\n",
    "| Layer | Name | Rule |\n",
    "|-------|------|------|\n",
    "| 1 | Obvious | Exactly 1 unique (L1,L2,L3,L4) mapping in GT |\n",
    "| 2 | Ambiguous | 2+ distinct mappings in GT |\n",
    "| 3 | Unknown | TRANCD absent from GT entirely |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count distinct mappings per TRANCD in ground truth\n",
    "gt_mapping_counts = (\n",
    "    df_gt\n",
    "    .groupby(\"TRANCD\")\n",
    "    .apply(lambda g: g[[\"gt_L1\", \"gt_L2\", \"gt_L3\", \"gt_L4\"]].drop_duplicates().shape[0])\n",
    "    .reset_index(name=\"n_mappings\")\n",
    ")\n",
    "\n",
    "multi_codes  = set(gt_mapping_counts.loc[gt_mapping_counts[\"n_mappings\"] > 1, \"TRANCD\"])\n",
    "single_codes = set(gt_mapping_counts.loc[gt_mapping_counts[\"n_mappings\"] == 1, \"TRANCD\"])\n",
    "all_gt_codes = set(df_gt[\"TRANCD\"].unique())\n",
    "\n",
    "print(f\"GT codes with 1 mapping (Layer 1):  {len(single_codes)}\")\n",
    "print(f\"GT codes with 2+ mappings (Layer 2): {len(multi_codes)}\")\n",
    "print(f\"Multi-mapping codes: {sorted(multi_codes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_layer(trancd):\n",
    "    if trancd not in all_gt_codes:\n",
    "        return 3  # Unknown\n",
    "    if trancd in multi_codes:\n",
    "        return 2  # Ambiguous\n",
    "    return 1      # Obvious\n",
    "\n",
    "\n",
    "df_catalog[\"layer\"] = df_catalog[\"TRANCD\"].apply(assign_layer)\n",
    "\n",
    "# ── Summary ───────────────────────────────────────────────────────\n",
    "layer_summary = (\n",
    "    df_catalog\n",
    "    .groupby(\"layer\")\n",
    "    .agg(codes=(\"TRANCD\", \"nunique\"), total_volume=(\"volume\", \"sum\"))\n",
    "    .reset_index()\n",
    ")\n",
    "layer_summary[\"pct_volume\"] = (\n",
    "    layer_summary[\"total_volume\"] / layer_summary[\"total_volume\"].sum() * 100\n",
    ").round(1)\n",
    "\n",
    "layer_names = {1: \"Obvious\", 2: \"Ambiguous\", 3: \"Unknown\"}\n",
    "layer_summary[\"name\"] = layer_summary[\"layer\"].map(layer_names)\n",
    "\n",
    "print(\"Layer assignment summary:\")\n",
    "print(layer_summary[[\"layer\", \"name\", \"codes\", \"total_volume\", \"pct_volume\"]].to_string(index=False))\n",
    "print(f\"\\nTotal codes: {len(df_catalog)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Show codes per layer ──────────────────────────────────────────\n",
    "for layer_num, layer_name in layer_names.items():\n",
    "    layer_df = df_catalog[df_catalog[\"layer\"] == layer_num].sort_values(\"volume\", ascending=False)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Layer {layer_num} — {layer_name} ({len(layer_df)} codes)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for _, row in layer_df.iterrows():\n",
    "        print(f\"  TRANCD={row['TRANCD']:>5} | vol={row['volume']:>7,} | {row['source_file']:<7} | {str(row['sample_desc_1'])[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save catalog (with layers) to Unity Catalog ───────────────────\n",
    "try:\n",
    "    sdf_catalog = spark.createDataFrame(df_catalog)\n",
    "    sdf_catalog.write.mode(\"overwrite\").saveAsTable(CATALOG_TABLE)\n",
    "    print(f\"Saved {len(df_catalog)} rows to {CATALOG_TABLE}\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC write (run in Databricks).\")\n",
    "    print(f\"DataFrame ready with {len(df_catalog)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Validation\n",
    "\n",
    "Verify that all Unity Catalog tables were created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Validate UC tables ────────────────────────────────────────────\n",
    "try:\n",
    "    for table_name, expected_label in [\n",
    "        (GT_TABLE, \"ground_truth_normalized\"),\n",
    "        (CATALOG_TABLE, \"transaction_code_catalog\"),\n",
    "    ]:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {table_name}\").collect()[0][\"cnt\"]\n",
    "        print(f\"  OK  {expected_label}: {count} rows\")\n",
    "\n",
    "    # Verify the catalog has a layer column\n",
    "    catalog_cols = [f.name for f in spark.table(CATALOG_TABLE).schema.fields]\n",
    "    assert \"layer\" in catalog_cols, \"Missing 'layer' column in catalog table\"\n",
    "    print(f\"  OK  catalog has 'layer' column\")\n",
    "\n",
    "    print(\"\\nAll validations passed.\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC validation (run in Databricks).\")\n",
    "    print(\"Local DataFrames are ready:\")\n",
    "    print(f\"  df_gt:      {len(df_gt)} rows, {df_gt['TRANCD'].nunique()} unique codes\")\n",
    "    print(f\"  df_catalog: {len(df_catalog)} rows, columns: {list(df_catalog.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
