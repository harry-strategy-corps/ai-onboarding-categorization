{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6948a4fc-5e30-4c99-a0b5-c89ca7f48111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 03 — Categorize Transactions\n",
    "\n",
    "**Objective:** Classify every transaction code in the catalog into the StrategyCorp\n",
    "taxonomy using `ai_query()` with `responseFormat` for structured JSON output.\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "1. Read the `transaction_code_catalog` from Unity Catalog (produced by `01_prepare_data`).\n",
    "2. Load the taxonomy markdown as prompt context.\n",
    "3. Batch codes (15–20 per prompt) and call `ai_query()` with a JSON schema response format.\n",
    "4. Track token estimates and cost per batch.\n",
    "5. Save all results to a single `classification_results` UC table with `layer`, `prompt_version`, and cost metadata.\n",
    "\n",
    "**Runs on:** Databricks Runtime 15.4 LTS or above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f55e08-7d9a-4c12-86db-71ec686a2efa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Configuration ─────────────────────────────────────────────────\n",
    "CATALOG_NAME    = \"ciq-bp_dummy-dev\"\n",
    "SCHEMA_NAME     = \"default\"\n",
    "MODEL_NAME      = \"databricks-claude-opus-4-6\"\n",
    "PROMPT_VERSION  = \"v1.0\"\n",
    "BATCH_SIZE      = 15\n",
    "TAXONOMY_PATH   = \"../data/taxonomy/transaction_categorization_taxonomy.md\"\n",
    "\n",
    "CATALOG_TABLE  = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.transaction_code_catalog\"\n",
    "RESULTS_TABLE  = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.classification_results\"\n",
    "\n",
    "# Token pricing (Databricks Foundation Model Serving — approximate $/1K tokens)\n",
    "PRICING = {\n",
    "    \"databricks-claude-opus-4-6\": {\"input\": 0.015, \"output\": 0.075},\n",
    "    \"databricks-meta-llama-3-1-70b-instruct\": {\"input\": 0.001, \"output\": 0.001},\n",
    "}\n",
    "\n",
    "print(f\"Model:          {MODEL_NAME}\")\n",
    "print(f\"Prompt version: {PROMPT_VERSION}\")\n",
    "print(f\"Batch size:     {BATCH_SIZE}\")\n",
    "print(f\"Catalog table:  {CATALOG_TABLE}\")\n",
    "print(f\"Results table:  {RESULTS_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6234fc74-5cc8-45b0-9ab9-7b6987de2300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Step 1 — Validate upstream tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d9780c-dda7-42cf-a702-43bb32321ce7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    catalog_sdf = spark.table(CATALOG_TABLE)\n",
    "    catalog_cols = [f.name for f in catalog_sdf.schema.fields]\n",
    "    assert \"layer\" in catalog_cols, \"Missing 'layer' column in catalog — run 01_prepare_data first\"\n",
    "    assert \"transaction_code\" in catalog_cols, \"Missing 'transaction_code' column in catalog\"\n",
    "    assert \"description_1\" in catalog_cols, \"Missing 'description_1' column in catalog\"\n",
    "\n",
    "    df_catalog = catalog_sdf.toPandas()\n",
    "    df_catalog[\"transaction_code\"] = df_catalog[\"transaction_code\"].astype(str)\n",
    "\n",
    "    print(f\"Loaded {len(df_catalog)} codes from {CATALOG_TABLE}\")\n",
    "    print(f\"Columns: {list(df_catalog.columns)}\")\n",
    "    print(f\"\\nLayer distribution:\")\n",
    "    for layer in sorted(df_catalog[\"layer\"].unique()):\n",
    "        n = len(df_catalog[df_catalog[\"layer\"] == layer])\n",
    "        print(f\"  Layer {layer}: {n} codes\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — loading catalog from local notebook 01 output.\")\n",
    "    print(\"Run this notebook in Databricks for full functionality.\")\n",
    "    raise SystemExit(\"Requires Databricks environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "982a6a8b-feac-4832-92d8-a4350078a890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Step 2 — Load taxonomy and build system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "726c8285-e2ae-42c8-b0f1-5123a123e534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(TAXONOMY_PATH):\n",
    "    raise FileNotFoundError(f\"Taxonomy file not found: {TAXONOMY_PATH}\")\n",
    "\n",
    "with open(TAXONOMY_PATH, \"r\") as f:\n",
    "    taxonomy_md = f.read()\n",
    "\n",
    "print(f\"Taxonomy loaded: {len(taxonomy_md)} chars (~{len(taxonomy_md)//4} tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f012792-f849-427d-a7f1-ce6d57b96316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = f\"\"\"You are a transaction categorization engine for a US bank.\n",
    "Given a list of transaction codes (transaction_code) with descriptions, classify each into the\n",
    "StrategyCorp taxonomy below.\n",
    "\n",
    "{taxonomy_md}\n",
    "\n",
    "### Rules:\n",
    "1. First determine Block A (Non-fee item) or Block B (Fee item). Fee items typically\n",
    "   contain: \"fee\", \"charge\", \"surcharge\", \"penalty\", \"service charge\", \"reversal\".\n",
    "2. Refunds/Reversals of fees: Must be Block A > Money movement > Deposits.\n",
    "3. Classify through Level 2 > Level 3 > Level 4. Use EXACT strings from the taxonomy.\n",
    "4. Use \"Unclassified\" if no mapping fits. Do not guess.\n",
    "5. `include_in_scoring` (Block A only): true for NSF/OD and Money movement; false for\n",
    "   Account operations, Misc, Unclassified. For Block B, set to false.\n",
    "6. `credit_debit`: \"Credit\" for money into the account, \"Debit\" for money out.\n",
    "   If unclear, use \"Debit\".\n",
    "7. `confidence`: 0.0 to 1.0 — your certainty in the classification.\n",
    "\n",
    "### Few-Shot Examples:\n",
    "\n",
    "Input: transaction_code=183, DESC=\"ACH Debit - SERMONS\"\n",
    "Output: {{{{\n",
    "  \"transaction_code\": \"183\",\n",
    "  \"category_1\": \"Non-fee item\",\n",
    "  \"category_2\": \"Money movement\",\n",
    "  \"category_3\": \"ACH\",\n",
    "  \"category_4\": null,\n",
    "  \"include_in_scoring\": true,\n",
    "  \"credit_debit\": \"Debit\",\n",
    "  \"confidence\": 0.99\n",
    "}}}}\n",
    "\n",
    "Input: transaction_code=299, DESC=\"ATM Service Charge\"\n",
    "Output: {{{{\n",
    "  \"transaction_code\": \"299\",\n",
    "  \"category_1\": \"Fee item\",\n",
    "  \"category_2\": \"All others\",\n",
    "  \"category_3\": \"Money movement\",\n",
    "  \"category_4\": \"ATM\",\n",
    "  \"include_in_scoring\": false,\n",
    "  \"credit_debit\": \"Debit\",\n",
    "  \"confidence\": 0.98\n",
    "}}}}\n",
    "\n",
    "Input: transaction_code=141, DESC=\"Transfer from DDA\"\n",
    "Output: {{{{\n",
    "  \"transaction_code\": \"141\",\n",
    "  \"category_1\": \"Non-fee item\",\n",
    "  \"category_2\": \"Money movement\",\n",
    "  \"category_3\": \"Transfers & Payments\",\n",
    "  \"category_4\": null,\n",
    "  \"include_in_scoring\": true,\n",
    "  \"credit_debit\": \"Credit\",\n",
    "  \"confidence\": 0.95\n",
    "}}}}\n",
    "\"\"\"\n",
    "\n",
    "print(f\"System prompt: {len(SYSTEM_PROMPT)} chars (~{len(SYSTEM_PROMPT)//4} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5df21f01-c3a1-4c4b-bc0b-6b81d1223976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Step 3 — Define responseFormat JSON schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b54c6fef-570f-4bc7-899e-4de51cfd7f27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "RESPONSE_SCHEMA = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"transaction_classifications\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"classifications\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"transaction_code\":    {\"type\": \"string\"},\n",
    "                            \"category_1\":          {\"type\": \"string\"},\n",
    "                            \"category_2\":          {\"type\": \"string\"},\n",
    "                            \"category_3\":          {\"type\": [\"string\", \"null\"]},\n",
    "                            \"category_4\":          {\"type\": [\"string\", \"null\"]},\n",
    "                            \"include_in_scoring\":  {\"type\": \"boolean\"},\n",
    "                            \"credit_debit\":        {\"type\": \"string\"},\n",
    "                            \"confidence\":          {\"type\": \"number\"}\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"transaction_code\", \"category_1\", \"category_2\",\n",
    "                            \"category_3\", \"category_4\",\n",
    "                            \"include_in_scoring\", \"credit_debit\", \"confidence\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"classifications\"]\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "response_schema_str = json.dumps(RESPONSE_SCHEMA)\n",
    "print(f\"Response schema ready ({len(response_schema_str)} chars)\")\n",
    "print(json.dumps(RESPONSE_SCHEMA, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b4c55f9-3bff-4c84-92ce-4ee4488c01b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Step 4 — Batch classification engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd6be622-54bf-48c5-a495-1f1739b1c523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_user_prompt(batch_df):\n",
    "    \"\"\"Build the user prompt listing transaction codes for a single batch.\"\"\"\n",
    "    lines = []\n",
    "    for _, row in batch_df.iterrows():\n",
    "        desc = str(row[\"description_1\"]).strip()\n",
    "        if desc in (\"nan\", \"\", \"None\"):\n",
    "            desc = \"(no description)\"\n",
    "        lines.append(f'transaction_code={row[\"transaction_code\"]}, DESC=\"{desc}\"')\n",
    "\n",
    "    codes_text = \"\\n\".join(f\"  - {line}\" for line in lines)\n",
    "    return f\"\"\"Classify these {len(batch_df)} transaction codes:\\n{codes_text}\\n\\nReturn a JSON object with a \\\"classifications\\\" array containing one entry per code.\"\"\"\n",
    "\n",
    "\n",
    "def estimate_tokens(text):\n",
    "    \"\"\"Rough token estimate: ~4 chars per token.\"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "\n",
    "def estimate_cost(tokens_in, tokens_out, model_name):\n",
    "    \"\"\"Calculate estimated cost in USD.\"\"\"\n",
    "    prices = PRICING.get(model_name, {\"input\": 0.0, \"output\": 0.0})\n",
    "    return (tokens_in * prices[\"input\"] + tokens_out * prices[\"output\"]) / 1000\n",
    "\n",
    "\n",
    "# Quick test\n",
    "test_batch = df_catalog.head(3)\n",
    "print(\"Sample user prompt:\")\n",
    "print(build_user_prompt(test_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec49bdb9-2efb-4e34-af53-feb177fbc84b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def classify_batch(batch_df, batch_num, layer):\n",
    "    \"\"\"\n",
    "    Classify a batch of transaction codes via ai_query().\n",
    "    Returns a list of result dicts with metadata.\n",
    "    \"\"\"\n",
    "    user_prompt = build_user_prompt(batch_df)\n",
    "    full_prompt = SYSTEM_PROMPT + \"\\n\" + user_prompt\n",
    "    escaped = full_prompt.replace(\"'\", \"''\")\n",
    "    escaped_schema = response_schema_str.replace(\"'\", \"''\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT ai_query(\n",
    "        '{MODEL_NAME}',\n",
    "        '{escaped}',\n",
    "        responseFormat => '{escaped_schema}'\n",
    "    ) as result\n",
    "    \"\"\"\n",
    "\n",
    "    tokens_in = estimate_tokens(full_prompt)\n",
    "\n",
    "    print(f\"  Batch {batch_num} | Layer {layer} | {len(batch_df)} codes | ~{tokens_in} tokens in\")\n",
    "\n",
    "    result_raw = spark.sql(query).collect()[0][\"result\"]\n",
    "    tokens_out = estimate_tokens(result_raw)\n",
    "    cost = estimate_cost(tokens_in, tokens_out, MODEL_NAME)\n",
    "\n",
    "    parsed = json.loads(result_raw)\n",
    "    classifications = parsed.get(\"classifications\", [])\n",
    "\n",
    "    run_ts = datetime.utcnow().isoformat()\n",
    "\n",
    "    # Build a lookup from the batch for metadata\n",
    "    batch_lookup = batch_df.set_index(\"transaction_code\").to_dict(orient=\"index\")\n",
    "\n",
    "    results = []\n",
    "    for cls in classifications:\n",
    "        tc = str(cls[\"transaction_code\"])\n",
    "        meta = batch_lookup.get(tc, {})\n",
    "        results.append({\n",
    "            \"transaction_code\":    tc,\n",
    "            \"description_1\":      meta.get(\"description_1\", \"\"),\n",
    "            \"volume\":             meta.get(\"volume\", 0),\n",
    "            \"source_file\":        meta.get(\"source_file\", \"\"),\n",
    "            \"layer\":              layer,\n",
    "            \"category_1\":         cls.get(\"category_1\"),\n",
    "            \"category_2\":         cls.get(\"category_2\"),\n",
    "            \"category_3\":         cls.get(\"category_3\"),\n",
    "            \"category_4\":         cls.get(\"category_4\"),\n",
    "            \"include_in_scoring\": cls.get(\"include_in_scoring\"),\n",
    "            \"credit_debit\":       cls.get(\"credit_debit\"),\n",
    "            \"confidence\":         cls.get(\"confidence\"),\n",
    "            \"prompt_version\":     PROMPT_VERSION,\n",
    "            \"model_name\":         MODEL_NAME,\n",
    "            \"run_timestamp\":      run_ts,\n",
    "            \"tokens_in\":          tokens_in,\n",
    "            \"tokens_out\":         tokens_out,\n",
    "            \"estimated_cost\":     cost,\n",
    "            \"llm_raw\":            result_raw,\n",
    "        })\n",
    "\n",
    "    print(f\"    → Parsed {len(results)} classifications | ~{tokens_out} tokens out | ${cost:.4f}\")\n",
    "    return results\n",
    "\n",
    "print(\"classify_batch() defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af461c37-bb41-4e4a-b6a7-a4a6351d4cdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Step 5 — Run classification by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a0a6d18-8404-45c0-b616-303fb83b3a98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "all_results = []\n",
    "layer_names = {1: \"Obvious\", 2: \"Ambiguous\", 3: \"Unknown\"}\n",
    "\n",
    "for layer_num in sorted(df_catalog[\"layer\"].unique()):\n",
    "    layer_df = df_catalog[df_catalog[\"layer\"] == layer_num].reset_index(drop=True)\n",
    "    layer_name = layer_names.get(layer_num, f\"Layer {layer_num}\")\n",
    "    n_batches = math.ceil(len(layer_df) / BATCH_SIZE)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Layer {layer_num} — {layer_name} | {len(layer_df)} codes | {n_batches} batches\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        batch = layer_df.iloc[i * BATCH_SIZE : (i + 1) * BATCH_SIZE]\n",
    "        try:\n",
    "            batch_results = classify_batch(batch, i + 1, layer_num)\n",
    "            all_results.extend(batch_results)\n",
    "        except Exception as e:\n",
    "            print(f\"    ERROR in batch {i+1}: {e}\")\n",
    "            # Record failed codes so we know what was missed\n",
    "            for _, row in batch.iterrows():\n",
    "                all_results.append({\n",
    "                    \"transaction_code\":    str(row[\"transaction_code\"]),\n",
    "                    \"description_1\":      row.get(\"description_1\", \"\"),\n",
    "                    \"volume\":             row.get(\"volume\", 0),\n",
    "                    \"source_file\":        row.get(\"source_file\", \"\"),\n",
    "                    \"layer\":              layer_num,\n",
    "                    \"category_1\":         \"ERROR\",\n",
    "                    \"category_2\":         str(e)[:200],\n",
    "                    \"category_3\":         None,\n",
    "                    \"category_4\":         None,\n",
    "                    \"include_in_scoring\": False,\n",
    "                    \"credit_debit\":       None,\n",
    "                    \"confidence\":         0.0,\n",
    "                    \"prompt_version\":     PROMPT_VERSION,\n",
    "                    \"model_name\":         MODEL_NAME,\n",
    "                    \"run_timestamp\":      datetime.utcnow().isoformat(),\n",
    "                    \"tokens_in\":          0,\n",
    "                    \"tokens_out\":         0,\n",
    "                    \"estimated_cost\":     0.0,\n",
    "                    \"llm_raw\":            str(e),\n",
    "                })\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TOTAL: {len(all_results)} classifications collected\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f496b68-bb7d-4164-b538-ffe712bf87a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Step 6 — Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50affe6-c37b-42ed-818c-a3e2d9e9a0ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "# ── Summary by layer ──────────────────────────────────────────────\n",
    "print(f\"Total results: {len(df_results)}\")\n",
    "print(f\"Columns: {list(df_results.columns)}\\n\")\n",
    "\n",
    "for layer_num in sorted(df_results[\"layer\"].unique()):\n",
    "    layer_df = df_results[df_results[\"layer\"] == layer_num]\n",
    "    layer_name = layer_names.get(layer_num, f\"Layer {layer_num}\")\n",
    "    errors = len(layer_df[layer_df[\"category_1\"] == \"ERROR\"])\n",
    "\n",
    "    print(f\"Layer {layer_num} ({layer_name}):\")\n",
    "    print(f\"  Codes:      {len(layer_df)}\")\n",
    "    print(f\"  Errors:     {errors}\")\n",
    "    print(f\"  Tokens in:  {layer_df['tokens_in'].sum():,}\")\n",
    "    print(f\"  Tokens out: {layer_df['tokens_out'].sum():,}\")\n",
    "    print(f\"  Est. cost:  ${layer_df['estimated_cost'].sum():.4f}\")\n",
    "    print()\n",
    "\n",
    "total_cost = df_results[\"estimated_cost\"].sum()\n",
    "print(f\"Total estimated cost: ${total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9caad66-8c29-4a3c-94b4-18ff445dc456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Category distribution ─────────────────────────────────────────\n",
    "valid = df_results[df_results[\"category_1\"] != \"ERROR\"]\n",
    "\n",
    "print(\"Category 1 (L1) distribution:\")\n",
    "print(valid[\"category_1\"].value_counts().to_string())\n",
    "\n",
    "print(\"\\nCategory 2 (L2) distribution:\")\n",
    "print(valid[\"category_2\"].value_counts().to_string())\n",
    "\n",
    "print(\"\\nConfidence statistics:\")\n",
    "print(valid[\"confidence\"].describe().to_string())\n",
    "\n",
    "# Show low-confidence classifications\n",
    "low_conf = valid[valid[\"confidence\"] < 0.8]\n",
    "if len(low_conf) > 0:\n",
    "    print(f\"\\nLow-confidence ({len(low_conf)} codes < 0.8):\")\n",
    "    for _, row in low_conf.iterrows():\n",
    "        print(f\"  transaction_code={row['transaction_code']:>5} | conf={row['confidence']:.2f} | {row['category_1']} > {row['category_2']} > {row['category_3']}\")\n",
    "else:\n",
    "    print(\"\\nNo low-confidence classifications (all >= 0.8).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8115346e-07da-416d-801a-f340ae0f812c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Step 7 — Save results to Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6146e7f3-592f-434c-8df5-aad3892d39ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert types for Spark compatibility\n",
    "df_to_save = df_results.copy()\n",
    "df_to_save[\"transaction_code\"] = df_to_save[\"transaction_code\"].astype(int)\n",
    "df_to_save[\"volume\"] = df_to_save[\"volume\"].astype(int)\n",
    "df_to_save[\"tokens_in\"] = df_to_save[\"tokens_in\"].astype(int)\n",
    "df_to_save[\"tokens_out\"] = df_to_save[\"tokens_out\"].astype(int)\n",
    "df_to_save[\"confidence\"] = df_to_save[\"confidence\"].astype(float)\n",
    "df_to_save[\"estimated_cost\"] = df_to_save[\"estimated_cost\"].astype(float)\n",
    "\n",
    "try:\n",
    "    sdf_results = spark.createDataFrame(df_to_save)\n",
    "    sdf_results.write.mode(\"overwrite\").saveAsTable(RESULTS_TABLE)\n",
    "    print(f\"Saved {len(df_to_save)} rows to {RESULTS_TABLE}\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping UC write.\")\n",
    "    print(f\"DataFrame ready with {len(df_to_save)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6ab9153-fd11-4318-95c7-58fc42343d28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Step 8 — Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96e003d1-6263-4955-871c-77b487e3e6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {RESULTS_TABLE}\").collect()[0][\"cnt\"]\n",
    "    print(f\"  OK  {RESULTS_TABLE}: {count} rows\")\n",
    "\n",
    "    result_cols = [f.name for f in spark.table(RESULTS_TABLE).schema.fields]\n",
    "    required = [\"transaction_code\", \"layer\", \"category_1\", \"category_2\", \"prompt_version\",\n",
    "                \"model_name\", \"tokens_in\", \"tokens_out\", \"estimated_cost\"]\n",
    "    missing = [c for c in required if c not in result_cols]\n",
    "    assert not missing, f\"Missing columns: {missing}\"\n",
    "    print(f\"  OK  All required columns present\")\n",
    "\n",
    "    # Check for errors\n",
    "    err_count = spark.sql(\n",
    "        f\"SELECT COUNT(*) as cnt FROM {RESULTS_TABLE} WHERE category_1 = 'ERROR'\"\n",
    "    ).collect()[0][\"cnt\"]\n",
    "    if err_count > 0:\n",
    "        print(f\"  WARN  {err_count} codes had classification errors\")\n",
    "    else:\n",
    "        print(f\"  OK  No classification errors\")\n",
    "\n",
    "    # Check layer coverage\n",
    "    layer_counts = spark.sql(\n",
    "        f\"SELECT layer, COUNT(*) as cnt FROM {RESULTS_TABLE} GROUP BY layer ORDER BY layer\"\n",
    "    ).toPandas()\n",
    "    print(f\"\\n  Layer coverage:\")\n",
    "    for _, row in layer_counts.iterrows():\n",
    "        name = layer_names.get(row[\"layer\"], \"?\")\n",
    "        print(f\"    Layer {row['layer']} ({name}): {row['cnt']} codes\")\n",
    "\n",
    "    print(\"\\nAll validations passed.\")\n",
    "except NameError:\n",
    "    print(\"Spark session not found — skipping validation (run in Databricks).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01d9b05b-656e-4c9d-aab5-36b13ee0189a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## (Optional) ai_classify & ai_extract comparison tests\n",
    "\n",
    "These cells test Databricks task-specific AI functions as alternatives to `ai_query()`.\n",
    "They are kept for reference and comparison, not part of the main pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7c69798-6ecd-41e7-910a-f2c1788ee4ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test ai_classify: Level 1 (Block A vs Block B)\n",
    "classify_l1_query = f\"\"\"\n",
    "SELECT\n",
    "  transaction_code,\n",
    "  description_1 as description,\n",
    "  ai_classify(description_1, ARRAY('Non-fee item', 'Fee item')) as l1_prediction\n",
    "FROM {CATALOG_TABLE}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    print(\"ai_classify — Level 1 (Block assignment):\")\n",
    "    display(spark.sql(classify_l1_query))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcf8a37f-a9f2-4a7f-ba4d-ec4fa6e55e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test ai_classify: Level 2 categories\n",
    "l2_labels = [\"Money movement\", \"Account operations\", \"NSF/OD\", \"Misc\", \"Service Charges\", \"Interchange\"]\n",
    "labels_sql = \", \".join([f\"'{l}'\" for l in l2_labels])\n",
    "\n",
    "classify_l2_query = f\"\"\"\n",
    "SELECT\n",
    "  transaction_code,\n",
    "  description_1 as description,\n",
    "  ai_classify(description_1, ARRAY({labels_sql})) as l2_prediction\n",
    "FROM {CATALOG_TABLE}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    print(\"ai_classify — Level 2:\")\n",
    "    display(spark.sql(classify_l2_query))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a8c35ce-9b1c-4832-9f80-4f93b863e297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test ai_extract: attribute detection\n",
    "extract_query = f\"\"\"\n",
    "SELECT\n",
    "  transaction_code,\n",
    "  description_1 as description,\n",
    "  ai_extract(description_1, ARRAY('transaction_method', 'is_reversal', 'is_fee', 'is_refund')) as extracted_info\n",
    "FROM {CATALOG_TABLE}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    print(\"ai_extract — Attribute detection:\")\n",
    "    display(spark.sql(extract_query))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_categorize_transactions",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
